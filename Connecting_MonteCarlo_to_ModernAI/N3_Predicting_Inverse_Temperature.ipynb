{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/daleas_module/Connecting_MonteCarlo_to_ModernAI/N3_Predicting_Inverse_Temperature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLh-j9ve6CLi"
      },
      "source": [
        "# Notebook 3: Predicting the Inverse Temperature of a Lattice\n",
        "Ashley Dale\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, you will learn the following concepts:\n",
        "\n",
        "- The difference between linear/non-linear regression and Deep Neural Networks\n",
        "- The Universal Approximation Theorem\n",
        "- How to implement a Fully-connected Deep Neural Network (FC-DNN) for regression of 1D data\n",
        "- How to implement a Convolutional Neural Network (CNN) for regression of 2D data\n",
        "\n",
        "> **Note**: This notebook can take 5+ hours to compute.  It is recommended that you start it early, leave it running in the background, then return to it after some time to complete the analysis.  Don't leave everything to the last minute!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nprZegeW8Un9"
      },
      "source": [
        "## Setup Python Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETaZvbJQ5LsQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numba import jit\n",
        "from tqdm import tqdm, trange\n",
        "import copy\n",
        "\n",
        "import timeit\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "from ipywidgets import interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6KwwTQs8bGy"
      },
      "source": [
        "# Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOjY281e8_Jv"
      },
      "source": [
        "## Problem Statement\n",
        "\n",
        "Consider a situation where we have magnetic domain images taken of a ferromagnetic system when it is close to the transition temperature $T_C$, but we aren't sure what the thermodynamic temperature actually is.  (This is not the case for our simulated data, but it would be the case for most experimental data.) We would like a model that predicts the temperature of the input lattice.\n",
        "\n",
        "**The easy way:** Train a regression model using the net-magnetism of the system as the input variable, and the temperature as the output variable.  The net-magnetism of the system is a *latent feature*: it is something we can calculate from the raw data to make training the prediction model easier.\n",
        "\n",
        "**The hard way:** Train a regression model using the lattice directly.  The benefit of doing it this way is that we keep the coupling information defined by the lattice site adjacencies, as well as the magnetic domain shapes, structures, and distribution.  The bad thing is that it requires a larger model and more effort to get it to work successfully.\n",
        "\n",
        "We are going to try both, compare them, and see what gives the best result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5u6Ar_OHzI3"
      },
      "source": [
        "## Regression Model for 1D Data\n",
        "\n",
        "If we use the net-magnetism $M$ and inverse temperature $\\beta$ as our dataset features, each lattice state can be represented as just two numbers, or a row vector of shape $1 \\times 2$.  We can represent this as $l_i = [\\beta_i, M_i]$, where $l_i$ is the $i^{th}$ lattice.\n",
        "\n",
        "[<img src=https://mathworld.wolfram.com/images/interactive/TanhReal.gif alt=\"Picture of $tanh(x)$.  Source: WolframAlpha\" align=\"left\" width=300>](https://mathworld.wolfram.com/images/interactive/TanhReal.gif)\n",
        "\n",
        "It would be tempting to try and use a linear model, except we know that the analytical solution should be non-linear and something resembling hyperbolic tangent (as shown to the left; [source](https://mathworld.wolfram.com/HyperbolicTangent.html)).  A better approach is to use our domain knowledge to pick a model that we know will be able to learn the $tanh(x)$ function directly and well.  Because $tanh(x)$ has two horizontal asymptotes and polynomial expressions do not, any polynomial-based model such as [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR) is expected to behave poorly.  This is why we will explore a multi-layer perceptron (MLP) model, also known as deep neural network (DNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkKvrqw4Y5cX"
      },
      "source": [
        "### When to Use a DNN\n",
        "\n",
        "DNN's are [difficult to get right](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607), [hard to explain](https://arxiv.org/abs/2004.14545), [can require a tremendous amount of energy to train](https://www.idgconnect.com/article/3602888/reducing-energy-use-in-neural-networks.html) in addition to a [tremendous amount of data](https://machinelearningmastery.com/much-training-data-required-machine-learning/), and [struggle to quantify uncertainty in predictions](https://imerit.net/blog/a-comprehensive-introduction-to-uncertainty-in-machine-learning-all-una/).  This is bad news for scientists seeking rigorous results with small datasets.\n",
        "\n",
        "Therefore, in general, a [linear](https://www.analyticsvidhya.com/blog/2021/10/everything-you-need-to-know-about-linear-regression/) or [non-linear](https://www.investopedia.com/terms/n/nonlinear-regression.asp) regression approach to modeling should be the first approach to generating a model.  Linear and non-linear approaches may also return an analytical expression with constants and variables which can be connected backwards to your experiment.  [This resource](https://www.ibm.com/topics/linear-regression) has a good discussion of what assumptions are required for linear regression to successfully model data; most of these assumptions also hold true for non-linear regression.\n",
        "\n",
        "However, there are many, many problems for which DNNs definitely are the best solution.  This can include probabilistic and stochastic systems and data for which no analytical expression can be presumed to exist.\n",
        "\n",
        "So. When should you use a DNN to model your data?\n",
        "\n",
        "**When nothing else works better for your data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM1GIhTGkmGt"
      },
      "source": [
        "#### The Universal Approximation Theorem\n",
        "\n",
        "Why does a DNN work when other linear or non-linear results don't?\n",
        "\n",
        "---\n",
        "*A deep neural network that is wide enough and/or deep enough can be used to approximate any measureable function $f(x)$ for an input $x$ perfectly.*\n",
        "---\n",
        "\n",
        "This is the Universal Approximation Theorem, and it was [first published in 1989](https://www.sciencedirect.com/science/article/abs/pii/0893608089900208), long before computers capable of calculating such models existed.  It tells us a path forward exists, but says nothing about how to get there.  It also doesn't say anything about why the theorem might be true.\n",
        "\n",
        "However, the results of the theorem are extremely powerful, and justify why choosing a DNN to model our data is a valid decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9XJNWajsgvG"
      },
      "source": [
        "## Regression Model for 2D Data\n",
        "\n",
        "If we use the full 2D lattice array and inverse temperature ð›½ as our dataset features, we want a model that takes advantage of the spatial relationships that are preserved in the array.  This way, we are training the model not only with the net-magnetization information produced by the simulation, but also with the data features such as magnetic domain shape, size, and distribution.  These are features which are definitely present in the data, but difficult to encode numerically into a vector.\n",
        "\n",
        "These difficult features arising from structural information can be learned using a [*Convolutional Neural Network*](https://www.ibm.com/topics/convolutional-neural-networks) or CNN.  CNNs get very complicated very quickly, but here are some things you should know:\n",
        "\n",
        "- CNN's are a special kind of DNN, and are typically used for image processing\n",
        "- The heart of a CNN is [2D convolution](http://www.songho.ca/dsp/convolution/convolution2d_example.html).\n",
        "- Because [2D convolution can be expressed as matrix multiplication](https://www.baeldung.com/cs/convolution-matrix-multiplication), this makes CNNs a very good choice for GPUs.\n",
        "- By repeating 2D convolution on the input image, along with other operations, we can condense the input image to a single number that represents the label.  For this notebook, that number will be the inverse temperature $\\beta$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0ybbJQB1j7h"
      },
      "source": [
        "## More Notebooks about DNNs and CNNs\n",
        "\n",
        "Any of the following notebooks can help deepen your understanding of why we are going to do what we do next.  There are also many good internet resources, some of which are linked in the \"Additional Readings\" section at the end of the notebook.\n",
        "\n",
        "- [Intro to Deep Neural Networks](https://github.com/GDS-Education-Community-of-Practice/DSECOP/tree/main/Intro_to_Deep_Learning)\n",
        "- [NN Basics](https://github.com/GDS-Education-Community-of-Practice/DSECOP/blob/main/Learning_the_Schrodinger_Equation/00_NN_Basics.ipynb)\n",
        "- [Intro to Machine Learning Workflow with Linear Regression](https://github.com/GDS-Education-Community-of-Practice/DSECOP/tree/main/Machine_Learning_Workflow)\n",
        "- [What is a neural network?](https://github.com/GDS-Education-Community-of-Practice/DSECOP/blob/main/Solving_Differential_Equations_with_NNs/02_neural_networks.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yptbqn168zrj"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In the previous notebook, we used statistical mechanics and the Ising Hamiltonian to model and simulate a ferromagnetic system of atoms.  Now, we are trying to model and simulate the same physics, but without the benefit of having the model know what the physics is.\n",
        "\n",
        "We are also going to try and run the simulation \"backwards\".  In the previous notebook, a value for the inverse temperature was chosen first, and then we simulated a lattice based on the probability of that lattice for the given inverse temperature value.  Here, we are going to start with a lattice where the temperature is unknown, then work backwards to see if we can figure out what inverse temperature might have allowed this particular lattice to form.   We will try to figure it out based on all of the other lattice-temperature relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJAmGFSmOoIB"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "The first code blocks in the `Generate data for usage` section can be executed directly to generate the training and test data for the coding exercises later.\n",
        "\n",
        "The programming exercises here are meant to help you gain intuition about how the different kinds of models behave, so you will use pre-defined functions and models when they are available.  If you would like to try coding some models from scratch, there are many tutorials on the internet that are beyond the scope of this notebook.\n",
        "\n",
        "**Programming Exercise 1** walks you through implementing a *Support Vector Machine - Regression* or SVR.\n",
        "\n",
        "**Programming Exercise 2** walks you through implementing a *Fully-Connected Deep Neural Network* or FC-DNN.\n",
        "\n",
        "**Programming Exercise 3** walks you through implementing a *2D Convolutional Neural Network* or CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmiC_qfZOqmm"
      },
      "source": [
        "## Generate data for usage\n",
        "\n",
        "Before training the models, we need need data to train them with.  Executing the following code cell will create the functions needed to duplicate the simulations from the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ejdDQlrO2LD"
      },
      "outputs": [],
      "source": [
        "def initialize_lattice(L: int, c1 = 0):\n",
        "    \"\"\"\n",
        "    Function to initialize lattice.  Adds a border of zeros\n",
        "    to represent non-interacting atoms and make the neighbor\n",
        "    calculation easier\n",
        "\n",
        "    L: The square root of the number of atoms in the lattice\n",
        "    returns padded_lattice: A lattice of size (L+2, L+2)\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize the lattice using np.random.random for a lattice of size\n",
        "    # LxL\n",
        "\n",
        "    init_lattice = np.random.random(size=(L,L))\n",
        "\n",
        "    # create a lattice of zeros that has an extra row on the top and bottom,\n",
        "    # and an extra column on the left and the right\n",
        "\n",
        "    padded_lattice = np.zeros((L+2, L+2))\n",
        "\n",
        "    #mask lattice by setting values above 0.5 to 1, and everything else to -1\n",
        "    for idx in range(L):\n",
        "        for jdx in range(L):\n",
        "            if init_lattice[idx, jdx] > 0.5:\n",
        "                init_lattice[idx, jdx] = 1\n",
        "            else:\n",
        "                init_lattice[idx, jdx] = -1\n",
        "    #init_lattice[init_lattice>0.5]= 1\n",
        "    #init_lattice[init_lattice !=1]= -1\n",
        "\n",
        "    # added step to create non-interacting atoms\n",
        "    padded_lattice[1:L+1, 1:L+1] = init_lattice\n",
        "\n",
        "    return np.array(padded_lattice)\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def MCMC_step_optimized(beta: float, lattice: np.array):\n",
        "    \"\"\"\n",
        "    Function to repeat the Monte Carlo Markov Chain for this system.\n",
        "    beta: the inverse temperature value for the MCMC step\n",
        "    lattice: the system of spins that will be simulated\n",
        "    returns: an updated version of the input lattice\n",
        "    \"\"\"\n",
        "\n",
        "    # Figure out the size of the lattice\n",
        "    [rows, cols] = lattice.shape\n",
        "\n",
        "    # keep the neighbors inside the region\n",
        "    for r in range(1,rows-1):\n",
        "        for c in range(1,cols-1):\n",
        "\n",
        "            # sum over the nearest neighbors\n",
        "            sum_NN = (lattice[r-1,c]+lattice[r+1, c]+lattice[r,c+1]+lattice[r,c-1])\n",
        "\n",
        "            # calculate the energy\n",
        "            E_a = -0.5*lattice[r,c]*sum_NN\n",
        "\n",
        "            # re-calculate the energy for a spin state change\n",
        "            E_b = -1*E_a\n",
        "\n",
        "            # choose whether to keep the new state or not\n",
        "            if E_b < E_a or np.exp(-(E_b - E_a)*beta) > np.random.rand():\n",
        "                lattice[r, c] *= -1\n",
        "\n",
        "    return lattice\n",
        "\n",
        "def generate_lattice_set(sqrt_N: int, min_temp: float, max_temp: float, num_temps: int):\n",
        "    \"\"\"\n",
        "    Main function to complete a simple Metropolis-Hastings Ising model\n",
        "    experiment.\n",
        "\n",
        "    sqrt_N: integer that is the square root of the number of atoms in the system\n",
        "\n",
        "    returns [inverse_temperature, M]: where M is the net magnetization at each\n",
        "            temperature\n",
        "    \"\"\"\n",
        "\n",
        "    spin_lattice = initialize_lattice(int(sqrt_N))\n",
        "\n",
        "    inverse_temperature = np.linspace(min_temp, max_temp, num_temps)\n",
        "\n",
        "    M = np.zeros(num_temps) #empty variable to hold net magnetism\n",
        "\n",
        "    # Declare new variable to hold spin lattices here\n",
        "    lattice_at_T = np.zeros((num_temps, sqrt_N+2, sqrt_N+2))\n",
        "\n",
        "    # For each temperature\n",
        "    for i in (trange(len(inverse_temperature))):\n",
        "        beta = inverse_temperature[i]\n",
        "\n",
        "        # Repeat the MCMC step 100 times to make sure the system is stable\n",
        "        for n in range(100):\n",
        "\n",
        "            spin_lattice = MCMC_step_optimized(beta, spin_lattice)\n",
        "\n",
        "        M[i] = (np.abs(np.sum(np.sum(spin_lattice)))/(sqrt_N*sqrt_N))\n",
        "        lattice_at_T[i, :, :] = (copy.copy(spin_lattice))\n",
        "\n",
        "    return np.array(inverse_temperature), np.array(M), np.array(lattice_at_T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpcR7Lc0VhPh"
      },
      "source": [
        "Let's define some parameters for our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwgE7n6WPTQg"
      },
      "outputs": [],
      "source": [
        "sqrt_N = 120 # Consider a system of 120x120 atoms\n",
        "\n",
        "# We want to limit the temperature range to capture the phase transition between\n",
        "# ordered and disordered.  If too many lattices are simulated before or after the\n",
        "# transition, we can expect them to not show much temperature dependence\n",
        "# and this will make training the models difficult\n",
        "\n",
        "T_min = 0.8 # The lowest inverse temperature will be 0.8\n",
        "T_max = 1.0 # The highest inverse temperature will be 1.0\n",
        "\n",
        "# We want to increase the number of lattices so that\n",
        "# a sufficient number are generated *during* the phase transition.\n",
        "num_temps = 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHrIH38lVhPi"
      },
      "source": [
        "Repeat the simulation three times, to make sure there is enough data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvARTdJwVhPi"
      },
      "outputs": [],
      "source": [
        "b, m, L = generate_lattice_set(sqrt_N, T_min, T_max, num_temps)\n",
        "b2, m2, L2 = generate_lattice_set(sqrt_N, T_min, T_max, num_temps)\n",
        "b3, m3, L3 = generate_lattice_set(sqrt_N, T_min, T_max, num_temps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGbJyUU8VhPj"
      },
      "source": [
        "Combine all of the simulation results to make them easier to process in the next step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGjTVAlHVhPj"
      },
      "outputs": [],
      "source": [
        "b = np.concatenate((b, b2, b3))\n",
        "m = np.concatenate((m, m2, m3))\n",
        "L = np.concatenate((L, L2, L3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRFMoO_r8iyF"
      },
      "source": [
        "### Format Data into Training and Testing splits\n",
        "\n",
        "Before the data can be used to train a model, we want to divide it into *testing* data and *training* data.  We can do this by randomly sampling the generated data using a uniform distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhyZns73P1m9"
      },
      "outputs": [],
      "source": [
        "# partition into train and test\n",
        "partition_mask = np.random.uniform(size=(len(b)))\n",
        "train = np.where(partition_mask > 0.3)\n",
        "test = np.where(partition_mask <= 0.3)\n",
        "\n",
        "# create labeled data partitions for reuse later\n",
        "X_train = m[train[0]]\n",
        "y_train = b[train[0]]\n",
        "\n",
        "X_test = m[test[0]]\n",
        "y_test = b[test[0]]\n",
        "\n",
        "# These are the np.arrays that hold the lattices.  They have\n",
        "# the same y_train and y_test labels as above\n",
        "X_lattice_train = L[None, train[0]]\n",
        "X_lattice_test = L[None, test[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfF6fvHj-TbY"
      },
      "source": [
        "It is always a good idea to double check that the training and test data agree with eachother.  A plot is a fast way to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNlomdhYQUFO"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1)\n",
        "ax.scatter(y_train, X_train, s=1, label=\"Train\")\n",
        "ax.scatter(y_test, X_test, s=1, label=\"Test\")\n",
        "ax.set_xlabel(r\"$\\beta$\")\n",
        "ax.set_ylabel(\"M\")\n",
        "ax.set_title(\"Sanity Check: Train and Test data matches\")\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmx1eIbhYn2Y"
      },
      "outputs": [],
      "source": [
        "def plot_mag_domains(x):\n",
        "    plt.figure(2)\n",
        "    plt.imshow(L[x])\n",
        "    plt.title(r\"$\\beta$ = \"+str(b[x]))\n",
        "    plt.show()\n",
        "\n",
        "interactive_plot = interactive(plot_mag_domains, x=(0,len(b)-1))\n",
        "output = interactive_plot.children[-1]\n",
        "output.layout.height = '500px'\n",
        "interactive_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3WJAfiyTRwp"
      },
      "source": [
        "## Programming Exercise 1: SVR Implementation\n",
        "\n",
        "Earlier, we hypothesized that SVR will not do a good job of modeling our data because [SVR uses n-dimensional polynomials](https://www.mathworks.com/help/stats/understanding-support-vector-machine-regression.html) to create its model, and we already know our data looks similar to hyperbolic tangent.  However, it is an easy thing to double check, and we will do this check now.\n",
        "\n",
        "### 1. Build the Model\n",
        "\n",
        "Look carefully at the SVR model object in the code cell below, and note that it has [four hyperparameters](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html):\n",
        "\n",
        "- `kernel` says what kind of math modeling will be used.  The options are `linear`, `poly`, `rbf`, and `sigmoid`.  For some insight in how these different kernels behave, [click here](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py).\n",
        "- `C` is the regularization constant that tries to keep the model parameters small.  Large parameters make it difficult to balance terms, and also can require a lot of memory. **C > 0** always.\n",
        "- `gamma` scales the kernel parameters.  If your data has a particular distribution (e.g. between -0.01 and 0.01), then you want to scale your model parameters so that it will output data within the same range.\n",
        "- `epsilon` determines how much model error is acceptable.  If you think about the example of fitting your data with a line, the `epsilon` term says that data points which are `epsilon` distance away from the line won't be used to calculate the model error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7zRJ29STNAC"
      },
      "outputs": [],
      "source": [
        "svr = SVR(kernel=\"sigmoid\", C=1E-5, gamma=\"auto\", epsilon=1E-12,verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqrN0cvYAjuc"
      },
      "source": [
        "### 2. Train the Model\n",
        "\n",
        "Now that the model is defined, we want to fit the model on the net-magnetization and inverse temperature training data, then use the model to predict the inverse temperature for the net-magnetization testing data.  Uncomment the lines below and execute the code cell to do this.\n",
        "\n",
        "> Note: the `X_train[:, None]` and `X_test[:, None]` have the `None` in the array because the `svr` model object is expecting input data with at least two dimensions.  The `None` argument defines the second dimension as `None` so that our data has the correct shape expected by the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G8iFh-eA-iO"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n",
        "svr.fit(X_train[:, None], y_train)\n",
        "print(\"\\nTime to completion for SVR: \"+str(timeit.default_timer() - start))\n",
        "print(\"\\nNumber of Iterations: \"+str(svr.n_iter_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrtmT_S9VhPm"
      },
      "source": [
        "### 3. Evaluate the Model\n",
        "\n",
        "Once the model is trained, we can use it to predict inverse temperature $\\beta$ values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjzHJClhVhPm"
      },
      "outputs": [],
      "source": [
        "y_predict = svr.predict(X_test[:, None])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3cP24wvBR5T"
      },
      "source": [
        "#### Make a Plot\n",
        "Visualize the difference between the true inverse temperature in `y_test` and the predicted inverse temperature in `y_predict`.\n",
        "- Create a plot that shows `y_test` on the x-axis and `X_test` on the y-axis.  Add appropriate axis labels and legend.\n",
        "- On the same plot, add `y_predict` on the x-axis and `X_test` on the y-axis as a second data series.\n",
        "\n",
        "Based on this figure, do you think the model did a good job predicting the true temperature?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FfHYxgzVQhi"
      },
      "outputs": [],
      "source": [
        "# Add plotting code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBKSG0JOCKCm"
      },
      "source": [
        "Calculate the [mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) for `y_test` and `y_pred` by calling the `mean_squared_error` function. (Click the link for documentation on how to use the function; no need to write one from scratch.)  A `mse` value of zero means that there is perfect agreement between the predicted inverse temperatures and the actual inverse temperatures.  `mse` values larger than zero need to be interpreted correctly, because anything larger than zero is \"bad\".  Figuring out \"how bad\" is \"bad\" depends on your data distribution and how many samples you have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoBDZf_qClYW"
      },
      "outputs": [],
      "source": [
        "mse = mean_squared_error(y_test, y_predict)\n",
        "print(\"MSE: \"+str(mse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIIEwycMDZM0"
      },
      "source": [
        "### 4. Retrain the Model\n",
        "\n",
        "The most important hyperpameter for `SVR` is the kernel, but the other hyperparameters have impact as well.\n",
        "\n",
        "Rerun steps 1-3 and try five different combinations of kernels with other hyperparameter values to see if you can reduce the `mse` value.  Fill out the table below with your results.  What was your best combination?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IONzCwZaVhPo"
      },
      "source": [
        "#### Fill out this table\n",
        "\n",
        "The first row is completed for you.  Add four more rows.\n",
        "\n",
        "|Kernel Type|C|$\\Gamma$|$\\epsilon$|MSE|Time (s)|\n",
        "|---|---|---|---|---|---|\n",
        "|sigmoid|1E-5|auto|1E-12|0.01069|4.971|\n",
        "|||||||\n",
        "|||||||\n",
        "|||||||\n",
        "|||||||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4pZUtYXFiLD"
      },
      "source": [
        "## Programming Exercise 2: Fully Connected Deep Neural Network\n",
        "\n",
        "Next, we will try to use a Fully Connected Deep Neural Network or *FC-NN* (see example on the right: [source](https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Colored_neural_network.svg)) to predict the inverse temperature for our data.\n",
        "\n",
        "[<img src=https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg alt=\"Picture of feed forward neural network.  Source: Wikipedia\" align=\"right\" width=300>](https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Colored_neural_network.svg)\n",
        "\n",
        "In a FC-DNN, only `dense` layers are used.  A `dense` layer contains a set of [*artificial neurons*](https://en.wikipedia.org/wiki/Artificial_neuron), where each neuron has two main parts:\n",
        "1. A linear operation of the form $y=mx+b$, where\n",
        "   - $x$ is the neuron input\n",
        "   - $y$ is the neuron output\n",
        "   - $m$ is the neuron *weight*\n",
        "   - $b$ is the neuron *bias*\n",
        "\n",
        "   The weights and biases are what the model learns during training.  This means that a `dense` layer can **almost** be thought of as a set of linear functions that emphasize the most important parts of the data.  *Almost.*\n",
        "\n",
        "2. An activation function that decides whether to pass all, some, or none of the layer output values to the next layer.  This is where the FC-DNN becomes non-linear.  It typically mixes the outputs of the neurons in complicated, non-intuitive ways.\n",
        "\n",
        "These dense layers are then stacked in a *sequence*, with each neuron connected to every neuron in the previous layer **and** every neuron in the following layer, making it *fully connected*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF1V6rprVhPo"
      },
      "source": [
        "### 1. Build the Model\n",
        "\n",
        "The Universal Approximation Theorem is very important here, as there is no rule or mathematical result that states how many neurons and in what order they should be connected.  Therefore, the most important hyperparameters for a `FC-DNN` model are:\n",
        "\n",
        "- `hidden_layer_sizes`: if you pass an argument `hidden_layer_sizes=[5,2,3]` then your FC-DNN will have three hidden layers with five, two, and three neurons respectively for a total of five layers in the model when including the input and output layers.\n",
        "- `max_iter`: how long to train the network.  Too long, and the model will over-fit to the data.  Too short, and the model won't learn enough to generalize to the test data.\n",
        "- `random_state`: makes sure that you can re-initialize the model weights the same way for repeatability.\n",
        "- `batch_size`: How many data points the model sees at one time.  Too few, and the model will never learn a comprehensive pattern.  Too many, and the model will struggle to capture details.\n",
        "\n",
        "In the model below, the model is initialized with `[15, 30, 20, 50, 20, 30, 15]`, for a total of approximately 360 parameters (180 neurons, each with its own weight and bias)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpMGK_KdVhPq"
      },
      "outputs": [],
      "source": [
        "layer_neurons = [15, 30, 20, 50, 20, 30, 15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN-yKP48VhPq"
      },
      "outputs": [],
      "source": [
        "regr = MLPRegressor(random_state=1,\n",
        "                    max_iter=2000,\n",
        "                    hidden_layer_sizes=layer_neurons,\n",
        "                    batch_size=15,\n",
        "                   verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmpMcMugVhPr"
      },
      "source": [
        "### 2. Train the Model\n",
        "\n",
        "Once the model is defined, it can be trained.  Remember to time the execution!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxWCRpBsH5vi"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n",
        "regr.fit(X_train[:, None], y_train)\n",
        "print(\"\\nTime to completion for FC-NN: \"+str(timeit.default_timer() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBpFIZFFVhPr"
      },
      "source": [
        "### 3. Evaluate the Model\n",
        "\n",
        "Now that the model is trained, we can use it to predict some inverse temperature values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k-pmpi1JNIv"
      },
      "outputs": [],
      "source": [
        "y_pred_dnn = regr.predict(X_test[:, None])\n",
        "mse = mean_squared_error(y_test, y_pred_dnn)\n",
        "print(\"MSE: \"+str(mse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqcCpvalVhPr"
      },
      "source": [
        "#### Make a Plot\n",
        "Visualize the difference between the true inverse temperature in `y_test` and the predicted inverse temperature in `y_pred_dnn`.\n",
        "- Create a plot that shows `y_test` on the x-axis and `X_test` on the y-axis.  Add appropriate axis labels and legend.\n",
        "- On the same plot, add `y_pred_dnn` on the x-axis and `X_test` on the y-axis as a second data series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOuthOJcJYzH"
      },
      "outputs": [],
      "source": [
        "# Add plotting code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwHuLsIxVhPr"
      },
      "source": [
        "### 4. Retrain the Model\n",
        "\n",
        "Now try to optimize the model for speed (less training time), number of neurons (less memory), and performance (reduce MSE).  Re-run steps 1-3 with five different combinations, and fill out the table below. Here are some suggestions to get you started:\n",
        "- Try reducing the number of hidden layers from 7 to a different number\n",
        "- Try reducing the number of neurons in each layer\n",
        "- Try changing the batch size to more/fewer.  If you go too large, you might get an out-of-memory (OOM) error; just reduce the batch size further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEEE1qRYVhPt"
      },
      "source": [
        "#### Fill out this table\n",
        "\n",
        "The first row is completed for you.  Add four more rows.  What is your best combination?\n",
        "\n",
        "|`layer_neurons`|Batch Size|MSE|Time (s)|\n",
        "|---|---|---|---|\n",
        "|`[15, 30, 20, 50, 20, 30, 15]`|15|0.0010047|4.226|\n",
        "|||||\n",
        "|||||\n",
        "|||||\n",
        "|||||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bJMKqDWLBU3"
      },
      "source": [
        "## Programming Exercise 3: Convolutional Neural Network\n",
        "\n",
        "Using a 2D Convolutional Neural Network to study Ising lattices is a novel and increasingly popular approach for physicists.  Here are some references that link to recently published work in this area: [1](https://www.nature.com/articles/s41598-020-69848-5), [2](https://academic.oup.com/ptep/article/2021/6/061A01/6270799), [3](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.103.033305), [4](https://arxiv.org/abs/1710.04987), [5](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.094427).\n",
        "\n",
        "> Note: Plan on several hours to complete this exercise.  Google colab notebooks are limited to 12 hours runtime, and GPU usage is limited based on availability.  If you run out of GPU-resources and compute time, you can decrease the number of epochs, but it will severely affect your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPrGDITJTqml"
      },
      "source": [
        "### 0. GPU Availability Check\n",
        "\n",
        "You will want to make sure that you have GPU usage enabled for this next excerise, as it is compute intensive.\n",
        "\n",
        "To Enable GPU usage in Google Colab, do the following:\n",
        "\n",
        "1. In the right top corner find the **RAM/Disk** icon.\n",
        "2. Click on -> Dropdown ->View resources - >change runtime -> hardware accelerator -> select GPU -> save\n",
        "\n",
        "If you are using a local machine with a GPU, discuss with your instructor about the proper procedure to enable GPU access.\n",
        "\n",
        "The following code cell checks to see if a GPU is available.\n",
        "\n",
        "> Note: If you choose to proceed without a GPU, expect the following calculation to take more than 10 hours.  If you use Google Colab for this period of time, the connection may time out and your notebook (with all calculations) may be reset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "nVaDw9_kSxDU",
        "tags": []
      },
      "outputs": [],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM4LLRSuVhPt"
      },
      "source": [
        "### 1. Build a CNN Model\n",
        "\n",
        "Unlike the `SVR` and `FC-NN` models, there is no pre-built `CNN` model that is a good fit for the Ising-Model simulation data.  Instead, we will build one ourselves from scratch using *layers* and *activation functions*.  An example architecture is shown below ([source](https://en.wikipedia.org/wiki/Convolutional_neural_network#/media/File:Typical_cnn.png)).\n",
        "\n",
        "[<img src=https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png alt=\"Picture of typical convolutional neural network.  Source: Wikipedia\" align=\"center\" width=800>](https://en.wikipedia.org/wiki/Convolutional_neural_network#/media/File:Typical_cnn.png)\n",
        "\n",
        "#### Layers\n",
        "A *Covolutional Neural Network* is typically a mix of several different kinds of layers:\n",
        "- `convolutional layer`: This layer takes a matrix of data, then filters it to emphasize the most important information in the dataset.  The filters are learned and updated to be specific to the problem.  [See here for a good in-depth tutorial.](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/)  You need the correct ratio of convolutional layers to the amount of data in your dataset.  Too few layers, and your model cannot learn all of the information it needs.  Too many layers, and your dataset may not have enough information for all of the layers to be helpful, leading to the untrained layers negatively affecting the trained layers.\n",
        "-  `pooling layer`: This layer reduces the size of the input matrix in order to emphasize big, important patterns in the input matrix.  *Max pooling* means that for an $n\\times n$ patch, only the maximum value of that patch is kept.  *Average pooling* means that the average value of the $n\\times n$ patch is kept. In both methods, small variations get filtered out.  For example, if the pooling layer has a stride of 2, the output of the layer will be 1/2 the size of the input to the layer.  No model information is stored in this layer, but you can \"lose\" too much information from previous layers if you aren't careful.  [See here for a good in-depth explanation.](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/)\n",
        "-  `batch normalization`: This layer keeps all of the matrix values centered around zero.  If the distribution of the values shifts too far from zero, it can take a longer time for the model to learn the distribution.  [See here for a good in-depth explanation.](https://www.baeldung.com/cs/batch-normalization-cnn)\n",
        "-  `flatten`: This is a layer that reshapes the matrix of data into a vector.  No model information is stored in this layer.\n",
        "-  `dense`: These are exactly the same layers used in the FC-DNN example above; the difference is that they are no longer fully connected.  For regression and classification tasks, a model is expected to return a number or set of numbers (vector), not a matrix.  This layer learns the best vector to represent the input created by layers earlier in the network.\n",
        "\n",
        "There are many types of layers available, and each has its own nuances.  [See here for a complete list of layers available in the Keras Python package.](https://keras.io/api/layers/)\n",
        "\n",
        "#### Activation Functions\n",
        "After choosing the layers, the next thing to understand is the `activation` function for each layer.  [See here for a good overview of the different activation functions.](https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253)  Two different activation functions are selected for the model below: `relu` and `sigmoid`.  The reasons for choosing these two functions are discussed in the [Model Intuition section at the end of this notebook](#model_intuition).\n",
        "\n",
        "---\n",
        "\n",
        "Obtaining this particular configuration of layers and parameterized required much testing and iterating.  However, it is [mathematically unlikely](https://www.youtube.com/watch?v=ZVVnvZdUMUk) that this is the only model configuration that can successfully learn the data.\n",
        "\n",
        "If you have the resources, see if you can create a more efficient and trainable solution at the end of this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tKGis42VhPv"
      },
      "source": [
        "The model described above can be built by executing the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogilCAHKUdjV"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(sqrt_N+2, sqrt_N+2, 1)))\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
        "model.add(layers.AveragePooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
        "model.add(layers.AveragePooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='sigmoid'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_rmFVLbVhPv"
      },
      "source": [
        "The next line prints a summary of the model.  Pay attention to the output shapes and the number of parameters; they should match the above explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5md0XS2pWxj-"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_APgWn-VhPw"
      },
      "source": [
        "### 2. Choose how to optimize the model.\n",
        "\n",
        "The model `optimizer` calculates how the model parameters are updated.  There are [many parameters and options here](https://keras.io/api/optimizers/), but the most important things to know are the following:\n",
        "- The `adam` optimizer is considered state-of-the-art.  It is based on an algorithm called [*stochastic gradient descent*](https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/) or `sgd`, which uses the same logic as the energy-minimization logic as in simulated annealing: if a change in values minimizes a loss function then that change is accepted.  However, an updated value which doesn't minimize the loss function is also accepted with some probability.\n",
        "\n",
        "- The loss function is analogous to the energy calculated by a Hamiltonian: loss and energy are both minimized by their respective systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD-VFB-TeZa-"
      },
      "outputs": [],
      "source": [
        "optimizer=tf.keras.optimizers.Adam(learning_rate=3E-8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FsDAZn1VhPw"
      },
      "source": [
        "### 3. Choose the model loss function.\n",
        "\n",
        "The choice of loss function is important, and again, there are many options to choose from.\n",
        "\n",
        "Here, the *mean squared error* (MSE) is chosen because it is easy to understand:\n",
        "\n",
        "<center>$MSE = \\frac{1}{N}\\sum_N(y - y')^2$</center>\n",
        "\n",
        "where $y$ is the true value, and $y'$ is the predicted value.  It is equivalent to the square of the Euclidean distance between the two values.\n",
        "\n",
        "Choosing MSE as the loss function also makes it easier to compare the error between the *SVR*, *FC-DNN*, and *CNN* models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUVf4a_DVhPx"
      },
      "outputs": [],
      "source": [
        "loss_function = tf.keras.losses.MeanSquaredError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1YFXn7PVhPx"
      },
      "source": [
        "### 4. Compile the model\n",
        "\n",
        "The model must be made into a static object that is trainable.  The next command does this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XenY7Hy9W3EJ"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer,\n",
        "            loss=loss_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuxSrEq1VhPx"
      },
      "source": [
        "### 5. Train the Model for 500 Epochs\n",
        "\n",
        "Because the model has almost 3 million parameters, we are going to train it for some time, then pause to check the results predicted by the model.  During this process, we need to consider the following parameters:\n",
        "\n",
        "- `epochs`: This is how many times the model weights will be updated\n",
        "- `batch_size`: This is how many data points the model will see at once\n",
        "- `shuffle`: This is whether or not the model sees the data in order, or out of order\n",
        "\n",
        "Finally, make sure to time the training so that you can keep track of your resource usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVMGPvlgZF1J",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "start1 = timeit.default_timer()\n",
        "with tf.device(device_name):\n",
        "    history = model.fit(X_lattice_train[0], y_train, epochs=500, batch_size=128,\n",
        "                        shuffle=True, validation_data=(X_lattice_test[0], y_test))\n",
        "stop1 = timeit.default_timer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEsvcqiGVhPy"
      },
      "outputs": [],
      "source": [
        "print(\"Training Part 1: \"+str((stop1 - start1)/60)+ \" minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dn7DSg9VhPy"
      },
      "source": [
        "### 6. Evaluate the Model\n",
        "\n",
        "Since the model has finished 500 epochs, we can stop and see how it is doing by having the model `predict` on the test data and then calculating the error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQpywQuXcX78"
      },
      "outputs": [],
      "source": [
        "results = model.predict(X_lattice_test[0])\n",
        "print(\"MSE: \"+str(mean_squared_error(results, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIEXQpGhVhPy"
      },
      "source": [
        "We also want to double check that it is learning the `train` data correctly, so we will have it predict temperatures for these values as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naof1oH84kno"
      },
      "outputs": [],
      "source": [
        "results_train = model.predict(X_lattice_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJpiR1zsVhPz"
      },
      "source": [
        "Finally, we want to visualize both the training loss over time, and the prediction results compared to the true values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSXK9UUNXq43"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ax[0].set_title(\"How the network is learning over time\")\n",
        "ax[0].plot(np.linspace(1, len(history.history['loss']), len(history.history['loss'])),history.history['loss'], label=\"Train\")\n",
        "ax[0].plot(np.linspace(1, len(history.history['loss']), len(history.history['loss'])),history.history['val_loss'], label=\"Validation\")\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_yscale(\"log\")\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].set_title(\"How predicted values compare to the actual values\")\n",
        "ax[1].scatter(y_test, X_test, s=1, label=\"Test\")\n",
        "ax[1].scatter(results_train, X_train, s=1, label=\"Predictions on Train\")\n",
        "ax[1].scatter(results, X_test, s=1, label=\"Predictions on Test\")\n",
        "ax[1].set_xlabel(r\"$\\beta$\")\n",
        "ax[1].set_ylabel(\"M\")\n",
        "ax[1].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CHaegNoVhP0"
      },
      "source": [
        "In the plot of the *Loss* vs *Epochs*, we see that the mean squared error for the data decreases quickly at first, and then more slowly. This is typical.  The model values begin with a random initialization, and quickly settles down to a more consistent improvement trend.  If the model is learning things correctly, then the loss value for both the `train` and `validation` data should be decreasing at about the same rate.  If there is a constant offset between them, that is fine; what is not okay is if the validation loss is increasing while the training loss is decreasing.  If this happens, the model is *overfitting* on the training data, and the model parameters or training parameters should be adjusted.\n",
        "\n",
        "The second plot uses the net-magnetization values calculated earlier so that we can get a sense of how the model is behaving.  The y-axis values are all fixed, and the x-axis values are either the *ground-truth* values used as inputs to the Metropolis-Hastings algorithm that generated the lattices, *predictions on the training data*, or *predictions on the testing data*.  What we would like to see is all three distributions perfectly aligned.  If they aren't, we can continue training the model to try and improve the prediction results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c8IWII9VhP0"
      },
      "source": [
        "### 7. (OPTIONAL) Continue Training the Model\n",
        "\n",
        "We can continue training the model for 500 epochs at a time until we see the loss decrease to an acceptable threshold.\n",
        "\n",
        "### Keep going until you see the loss quit decreasing, or you run out of time.\n",
        "\n",
        "Add more code cells as needed below.\n",
        "\n",
        "#### Epochs 501-1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6n55QM0fVhP0"
      },
      "outputs": [],
      "source": [
        "start2 = timeit.default_timer()\n",
        "with tf.device(device_name):\n",
        "    history2 = model.fit(X_lattice_train[0], y_train, initial_epoch=501, epochs=1000, batch_size=128,\n",
        "                        shuffle=True, validation_data=(X_lattice_test[0], y_test))\n",
        "stop2 = timeit.default_timer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NodZiVfBVhP1"
      },
      "outputs": [],
      "source": [
        "print(\"Training Part 2: \"+str((stop2 - start2)/60)+ \" minutes\")\n",
        "results2 = model.predict(X_lattice_test[0])\n",
        "print(\"MSE: \"+str(mean_squared_error(results, y_test)))\n",
        "results_train2 = model.predict(X_lattice_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY9VVVbeVhP1"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ax[0].set_title(\"How the network is learning over time\")\n",
        "ax[0].plot(np.linspace(1, len(history2.history['loss']), len(history2.history['loss'])),history2.history['loss'], label=\"train\")\n",
        "ax[0].plot(np.linspace(1, len(history2.history['loss']), len(history2.history['loss'])),history2.history['val_loss'], label=\"val\")\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_yscale(\"log\")\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].set_title(\"How predicted values compare to the actual values\")\n",
        "ax[1].scatter(y_test, X_test, s=1, label=\"Test\")\n",
        "ax[1].scatter(results_train2, X_train, s=1, label=\"Predictions on Train\")\n",
        "ax[1].scatter(results2, X_test, s=1, label=\"Predictions on Test\")\n",
        "ax[1].set_xlabel(r\"$\\beta$\")\n",
        "ax[1].set_ylabel(\"M\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr0wzs8TVhP1"
      },
      "source": [
        "As you generate additional checkpoints for the model, pay attention to the plots created.  How do they compare to the first ones you made?  Have the values shifted?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9lWaUlxVhP_"
      },
      "source": [
        "# Answer these questions\n",
        "Edit the cell below with your answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtRi4cZAVhQA"
      },
      "source": [
        "1. What are the six steps used to build and train a CNN?\n",
        "   > **Answer**:\n",
        "\n",
        "2. How many epochs did you train the CNN?  How long (in hours) did it take to train? What was the best `MSE` achieved?\n",
        "   > **Answer**:\n",
        "\n",
        "3. How many iterations did you train the FC-NN?  How long (in hours) did it take to train?  What was the best `MSE` achieved?\n",
        "   > **Answer**:\n",
        "\n",
        "4. How many iterations did you train the SVR?  How long (in hours) did it take to train?  What was the best `MSE` achieved? (Hint: Look for the `#iter` variable reported for number of iterations.)\n",
        "   > **Answer**:\n",
        "\n",
        "5. Which of the three model types (SVR, FC-NN, or CNN) was easiest to use and why?  Which model had the best results (how do you know)?  Which model used the most resources (how do you know)?  Which one do you think best solved the problem of reliably predicting the inverse temperature and why?\n",
        "   > **Answer**:\n",
        "\n",
        "6. A physicist has experimental velocity vs time data of a feather undergoing free-fall in a vacuum, [as in this video](https://www.youtube.com/watch?v=AV-qyDnZx0A).  Which type of model should the physicist choose: linear or non-linear?  Why?  Give a kinematics equation to support your answer.\n",
        "   > **Answer**:\n",
        "\n",
        "7. A physicist has experimental 2D X-ray Diffraction (XRD) data for a range of lattice spacings $d$ that [follow Bragg's law: $n\\lambda = 2dsin(\\theta)$](https://www.doitpoms.ac.uk/tlplib/xray-diffraction/bragg.php).  Which of the three model types (SVR, FC-NN, CNN) should the physicist choose?  Why?  [Click here](https://www.doitpoms.ac.uk/tlplib/diffraction/diffraction3.php) and [here](https://www.researchgate.net/figure/Two-dimensional-2D-X-ray-diffraction-patterns-of-a-Cs-b-Rb5Cs-and-c-GA5Cs-based_fig3_331256089) for an examples of 2D data.\n",
        "   > **Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpbuSnpfNR8U"
      },
      "source": [
        "# Additional Readings\n",
        "\n",
        "1. [Overview of different Non-linear Regression Techniques](https://towardsdatascience.com/3-techniques-for-building-a-machine-learning-regression-model-from-a-multivariate-nonlinear-dataset-88b25fc24ad5)\n",
        "2. [When to use what kind of machine learning model](https://machinelearningmastery.com/when-to-use-mlp-cnn-and-rnn-neural-networks/)\n",
        "3. [Understanding the Universal Approximation Theorem](https://towardsai.net/p/deep-learning/understanding-the-universal-approximation-theorem)\n",
        "4. [Tensorflow Tutorial on CNN](https://www.tensorflow.org/tutorials/images/cnn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"model_intuition\"></a>\n",
        "## Intuition for the CNN Model\n",
        "\n",
        "The CNN model provided has the following sequence:\n",
        "\n",
        ">0. **Input**: accepts an input matrix of size [$\\sqrt{N}+2$, $\\sqrt{N}+2$, $1$] because this is the size we defined for our Ising lattices.  There is a $1$ in the size definition because even though the lattice is not 3D, the python function still requires a defined value along the third dimension.\n",
        "\n",
        "In this notebook, the lattice size is $122 \\times 122 \\times 1$.\n",
        "\n",
        ">1. **Conv2D**: first pass at learning the spatial relationships and values of the lattice\n",
        ">2. **Conv2D**: second pass at learning the spatial relationships and values of the lattice\n",
        "\n",
        "We want the model to have a good opportunity to figure out the most important details about the data before we reduce the lattice size.  If the data is very noisy, or has a lot of very small domains that distinguish it from lattices at similar temperatures, the model should have information at this scale.  A `kernel_size=(3,3)` argument means that a matrix of size $3 \\times 3$ is convolved across the matrix.  Depending the size of the magnetic domains, a $3 \\times 3$ kernel size may learn relevant information, or the model may benefit from a larger kernel of $5 \\times 5$. Larger than $5 \\times 5$ is typically not used (even for very large matrices), and $4 \\times 4$ is not used because it is even.\n",
        "\n",
        "Like the *nearest-neighbor* calculation in the previous notebook, which required the lattice to have some padding or skip calculating values for the atoms on the edges of the lattice, the convolution operation requires special handling at the edges.  The `padding=\"same\"` argument puts zeros around the edges to keep the matrix size constant; otherwise it decreases by 2 along each dimension every time.\n",
        "\n",
        "Finally, the `Conv2D` layer does not output a single lattice: it outputs `filters=32`  *feature maps* of the input lattice, where each feature map is a version of the input that emphasizes different information.  For example, one filter might emphasize only edges of domains in the input lattice, another filter might emphasize only the smallest domains, and yet another might emphasize only the largest domains.\n",
        "\n",
        ">3. **MaxPool2D**: Reduces the lattice size by 2 in each dimension, so we can expect the output to be [$0.5(\\sqrt{N}+2)$, $0.5(\\sqrt{N}+2)$, 1).\n",
        "\n",
        "As the data moves deeper into the network, it is condensed.  (This layer can be thought of in terms of the [Renormalization Group flow](https://ieeexplore.ieee.org/abstract/document/9110872) of the lattice.)  Since only one-fourth of the of the lattice remains, and the initial size was $120 \\times 120$ before padding, the lattice is now $60 \\times 60$.  Larger domains have shrunk, and smaller domains have disappeared or become specks.  However, because of the number of `filters` in the `Conv2D` layers is still large, we can think of this layer as sorting lattice patterns into different parts of a large $M \\times M \\times P$ *feature vector*.\n",
        "\n",
        ">4. **Conv2D**: Now the network is learning \"medium\" sized patterns, and the number of feature maps created has increased to $filters=64$.\n",
        ">5. **MaxPool2D**: Reduces the lattice again, so that it is now one-sixteenth of its original size.\n",
        "\n",
        "Now, the lattice size is $30 \\times 30$.  Another convolutional layer is added to the model to learn smaller patterns, followed by a `BatchNormalization()` layer.\n",
        "\n",
        ">6. **Conv2D**: Same parameters as Layer #4, only now learning even more finely grained features.\n",
        ">7. **BatchNormalization**: This centers the data around zero and gives a maximum value of 1.\n",
        ">8. **Flatten**: Reshapes all of the matrices values into one long vector\n",
        "\n",
        "The `BatchNormalization()` layer is important, because up until this point, the matrix values passed by the `relu` activation function have been allowed to spread between $[0, \\infty]$.  We need the value returned for the inverse temperature $\\beta$ to be $\\beta \\in [0, 1]$, and the normalization makes sure the values are centered around zero with a standard deviation of 1 before the data moves into the fully connected layers.\n",
        "\n",
        "Finally, since the `dense` layers require a vector input, the output matrices of the convolutional layers is reshaped by the `flatten` layer.\n",
        "\n",
        ">9. **Dense**: This layer takes all of the values created at this point and combines them into $64$ numbers.  A `sigmoid` activation function is used to keep the values between [0, 1].\n",
        "\n",
        "Moving directly from the full set of matrix values to a single number requires the model to condense all of the information very quickly, without much opportunity to shift the data distribution correctly.  Adding Layer 9 before the final output helps correct for this.\n",
        "\n",
        ">10. **Dense**: This layer returns a single number, the predicted value for $\\beta$.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "rG2BWp2_-RaD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2pZaTgK-QrC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}