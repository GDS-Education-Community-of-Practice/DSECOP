{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_BjtTXzBGTM"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/Solving_Differential_Equations_with_NNs/02_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Notebook 2: What is a Neural Network?\n",
    "\n",
    "This notebook will briefly go through the aspects of neural networks that will important for this application.  For a more general overview of neural networks, please see the set of lectures and exercises located [here](https://github.com/GDS-Education-Community-of-Practice/DSECOP/tree/IntroDeepLearn/IntroDeepLearning/lectures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYdPXwjEA5tD"
   },
   "source": [
    "## What is a neural network?\n",
    "* Neural networks can be defined as computational systems that can learn to perform tasks by considering examples, generally without being programmed with any task-specific rules. Another way to phrase this is that a neural network is a computational system that learns to match a given input to the correct output. They are a broad category of machine learning algorithms that include popular algorithms such as convolutional neural networks, recurrent neural networks, and deep learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGonPJUyA5tG"
   },
   "source": [
    "## Neural Network Terminology\n",
    "\n",
    "![NN](https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Solving_Differential_Equations_with_NNs/nn.png)\n",
    "\n",
    "* Neuron (Node): the simplest unit of a neural network, takes in an input and produces an output.  Neurons are represent by circles in the above diagram.\n",
    "* Layer: a collection of neurons that act together.  Layers are represented by vertical stacks of neurons in the above diagram.\n",
    "* Input Layer: the first layer in a neural network, performs no manipulations to the data.  The layer to the very left in the above diagram is the input layer.\n",
    "* Output Layer: the last layer of a neural network.  The layer to the very left in the above diagram is the output layer.\n",
    "* Hidden Layer: any other layer of a neural network, in between the input and output layers.  All other layers besides the input and output layers in the above diagram are hidden layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGRwZisAA5tH"
   },
   "source": [
    "## Fully Connected Feedforward Neural Network (Fully Connected FFNN)\n",
    "\n",
    "![NN](https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Solving_Differential_Equations_with_NNs/nn.png)\n",
    "\n",
    "Though there are many types of neural network, using just the phrase \"neural network\" typically refers to a type of neural network known as a fully connected feedforward neural network (FFNN).  This type of network can also be known as a multilayer perceptron (MLP) if it has at least one hidden layer.\n",
    "\n",
    "Information in an FFNN moves only forward (left to right in the above diagram).  Additionally each neuron is connected to every neuron in the next layer and there are no connection between neurons in the same layer.  This means that the input to a layer in the neural network is simply the output from the previous layer.  As we will see in a moment, each neuron receives a weighted sum of the outputs of all neurons in the previous layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYNZ38p6A5tJ"
   },
   "source": [
    "## Mathematics of  a Neuron\n",
    "\n",
    "Each neuron is a mathematical function involving a column from a weight matrix, a scalar from a bias vector, and an activation function.  We can represent the mathematical form of the i-th neuron as:\n",
    "\n",
    "\n",
    "$$\\hat{y}_i = f(\\sum_{j=1}^M w_{ij}x_j + b_i),$$\n",
    "\n",
    "\n",
    "where x is the input to the neural network, w is the weight matrix which scales the input of the neuron, b is the bias vector that makes sure the output of the neuron is non-zero, and f is known as the activation function, which adds nonlinearity to the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg69XMBVA5tK"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "There are many common function to use as activation functions for neural networks, but three of the most commons ones are listed here.\n",
    "\n",
    "**Sigmoid**\n",
    "\n",
    "\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "\n",
    "**Hyperbolic Tangent**\n",
    "\n",
    "\n",
    "\n",
    "$$f(x) = tanh(x) = \\frac{e^{2x} - 1}{e^{2x} + 1}$$\n",
    "\n",
    "**Relu (Rectified Linear Unit)**\n",
    "\n",
    "\n",
    "\n",
    "$$f(x) = max(0, x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGW3t46NA5tL"
   },
   "source": [
    "## A Note on Hyperparameters\n",
    "\n",
    "The number of hidden layers, number of neurons per layer, activation function, and many other features are called hyperparameters of a neural network, meaning that their values must be chosen by the user before the network is run. Changing the value of a hyperparameter can drastically change the results of the neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_SDms1XA5tp"
   },
   "source": [
    "## Mathematics of Neural Networks: The First Hidden Layer\n",
    "\n",
    "For each neuron, i, in the first hidden layer of a neural network, we can represent its mathematical form as:\n",
    "\n",
    "\n",
    "$$\\hat{y}_i^1 = f^1(\\sum_{j=1}^M w_{ij}^1x_j + b_i^1).$$\n",
    "\n",
    "\n",
    "\n",
    "We can also write out the mathematical form for the entire first hidden layer as:\n",
    "\n",
    "\n",
    "$$\\hat{y}_1 = f^1(W_1\\textbf{x} + \\textbf{b}_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnWfc8YcA5uU"
   },
   "source": [
    "## Mathematics of Neural Networks: The Second Hidden Layer\n",
    "\n",
    "Similarly for the second hidden layer, we can represent the mathematical form of each neuron as:\n",
    "\n",
    "$$y_i^2 = f^2(\\sum_{j=1}^N w_{ij}^2y_j^1 + b_i^2).$$\n",
    "\n",
    "Note here that the weights matriz is no longer multiplied by the inputs to the neural network (x), but rather to the output of the first hidden layer.  This is because the input to the first hidden layer is the input ot the neural network but the input to the second hidden layer is the output of the first hidden layer.  Therefore we can expand the above equation to be a bit more clear:\n",
    "\n",
    "$$y_i^2 = f^2(\\sum_{j=1}^N w_{ij}^2f^1(\\sum_{k=1}^M w_{kj}^1x_k + b_j^1) + b_i^2).$$\n",
    "\n",
    "\n",
    "We can also write a mathematical form for the entire second hidden layer as:\n",
    "\n",
    "$$\\hat{y}_2 = f^2(W_2\\hat{y}_1 + \\textbf{b}_2),$$\n",
    "\n",
    "or more explicitly as\n",
    "\n",
    "$$\\hat{y}_2 = f^2(W_2f^1(W_1\\textbf(x) + \\textbf{b}_1) + \\textbf{b}_2).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxgZ4mriA5uy"
   },
   "source": [
    "## Mathematics of Neural Networks: The l-th Hidden Layer\n",
    "\n",
    "Finally, we can use the pattern we have developed to write down the equation for the mathematical output for a neuron on the l-th hidden layer of the neural network.  For the i-th neuron on the l-th layer we can describe it mathematically as:\n",
    "\n",
    "\n",
    "$$y_i^l = f^l(\\sum_{j=1}^{N_l} w_{ij}^ly_j^{l-1} + b_i^l),$$\n",
    "\n",
    "and more explicitly as\n",
    "\n",
    "$$y_i^l = f^l(\\sum_{j=1}^{N_l} w_{ij}^lf^{l-1}(\\sum_{k=1}^{N_{l-1}} w_{kj}^{l-2}y_k^{l-1} + b_j^{l-1}) + b_i^l),$$\n",
    "\n",
    "and finally all the way expanded as\n",
    "\n",
    "$$y_i^l = f^l(\\sum_{j=1}^{N_l} w_{ij}^l f^{l-1}(\\sum_{k=1}^{N_{l-1}} w_{jk}^{l-1}( \\cdot \\cdot \\cdot f^1(\\sum_{n=1}^M w_{mn}^1x_n + b_m^1) \\cdot \\cdot \\cdot ) + b_k^{l-1}) + b_j^l).$$\n",
    "\n",
    "\n",
    "We can also write a mathematical expression for the output of the entire l-th layer as:\n",
    "\n",
    "$$\\hat{y}_l = f^l (W_l \\hat{y}_{l-1} + \\textbf{b}_l),$$\n",
    "\n",
    "which can be expanded to\n",
    "\n",
    "$$\\hat{y}_l = f^l(W_lf^{l-1}(W_{l-1}\\hat{y}_{l-2} + \\textbf{b}_{l-1}) + \\textbf{b}_l),$$\n",
    "\n",
    "and finally to\n",
    "\n",
    "$$\\hat{y}_l = f^l(W_lf^{l-1}(W_{l-1}(\\cdot \\cdot \\cdot f^1(W_1\\textbf{x} + \\textbf{b}_1) \\cdot \\cdot \\cdot) + \\textbf{b}_{l-1}) + \\textbf{b}_l).$$\n",
    "\n",
    "\n",
    "It's a complicated expression and can grow to be very large but it is a set equation that describes the output of a neural network with l-1 hidden layers and an output layer. So it is also possible to rephrase the definition of a neural network to be an analytical function that maps a set of inputs to a set of outputs using a set of optimized parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_NKvIYSA5vC"
   },
   "source": [
    "## Neural Network Loss Function\n",
    "\n",
    "A loss function is used to determine how much the output from a neural network differs from the true/expected result.  There is not a set loss function that is used with neural networks, but two common loss functions are the mean-squared error loss function and the mean absolute error function.  The mean-squared error loss function (MSE) can be defined as:\n",
    "\n",
    "$$J_{MSE}(W) = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y}_i)^2,$$\n",
    "\n",
    "where y is the true data set, $\\hat{y}$ is the neural network prediction, N is the number of data points in the set, and W are the weights of the neural network.  The loss function depends on the weights of the neural network because changing the weights of the neural network changes its output.  \n",
    "\n",
    "The mean-absolute error loss function (MAE) has a similar form:\n",
    "\n",
    "$$J_{MAE}(W) = \\frac{1}{N}\\sum_{i=1}^N |y_i - \\hat{y}_i|..$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTMinj7QA5vH"
   },
   "source": [
    "## Finding The Optimized Weights and Biases\n",
    "\n",
    "A major part of working with neural networks is a process known as training where the weights of the neural network are optimized such that the cost function is minimized.  This training process has two phases: the forward pass and the backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VnG4AWQA5vJ"
   },
   "source": [
    "## Forward Pass\n",
    "\n",
    "![NN](https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Solving_Differential_Equations_with_NNs/bp1.png)\n",
    "\n",
    "The forward pass occurs when data is sent through the neural network (from left to right on the above graph) to produce a predicted output.  This predicted output is then fed into the loss function with the true data set to generate the loss value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7CQpwLjA5vL"
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "![NN](https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Solving_Differential_Equations_with_NNs/bp2.png)\n",
    "\n",
    "After the forward pass comes backpropagation, where the error from the loss function is backpropagated through the layers of the neural networks and its weights are adjusted layer by layer so that the next forward pass will result in a reduced loss value.  A simple way to optimize the weights of a neural network during backpropagation is through an optimization technique known as gradient descent.  The weights of the neural network are simply adjusted by the derivative of the loss function with respect to the weights, scaled by a hyperparameter known as the learning rate:\n",
    "\n",
    "$$W = W - r_{l}\\frac{\\partial J(W)}{\\partial W}.$$\n",
    "\n",
    "The learning rate (r$_l$) is a number typically much less than 1 and it is also a hyperparameter, so its value must be set before the neural network is run.\n",
    "\n",
    "The process of training a neural network involves many different iterations of forward pass followed by backpropagation.  Typically a training process will continue until a certain number of training iterations has been reached or the difference in the current loss value compared to the value from the previous iteration is below a certain threshold.  However, neural networks should not be trained for an overly long time because this will lead to something called overfitting where the neural network learns to match the data set it is trained with very well (so will show a small loss value) but loses all generality when given new data (so it will perform poorly when given the new data set).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZztdU69O-nEi"
   },
   "source": [
    "**EXERCISE 1**: Take a moment and summarize what you have learned about neural networks in the text box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBrSO1G1-nZF"
   },
   "source": [
    "Delete this text and type your response here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vqA3RXtBuEY"
   },
   "source": [
    "## Creating a Neural Network from Scratch Using JAX\n",
    "\n",
    "JAX is an automatic differentiation library in Python that can find the derivative of any chunk of code it is given.  If you are interested you can read more about the library [here](https://github.com/google/jax).\n",
    "\n",
    "The below section of the notebook will create a neural network entirely from scratch and analyze its performance. We will be using the gradient feature of the JAX library to implement a gradient descent optimization of our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4lC9FhCCpWE"
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "# Math for the ceiling function\n",
    "from math import ceil\n",
    "# Matplotlib for graphing capabilities\n",
    "from matplotlib import pyplot as plt\n",
    "# Numpy for arrays\n",
    "import numpy as np\n",
    "# Modules from the JAX library for creating neural networks\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "from jax import random as npr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i67TbrTQlVCk"
   },
   "source": [
    "### Generate the Data Set\n",
    "\n",
    "Let's keep things simple and generate a data points from a Gaussian curve.  We will have our x data be evenly space between -10 and 10 and our y data be the corresponding points on a Gaussian curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUuugR9kCp3L"
   },
   "outputs": [],
   "source": [
    "# Let's create a data set that is just a basic Gaussian curve\n",
    "X = np.linspace(-10,10,250)\n",
    "y = np.exp(-X**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzdz0Mg8lX5H"
   },
   "source": [
    "### Perform a Train-Test Split\n",
    "\n",
    "In machine learning problems it is common to split your data into two sets.  The first set, which usually contains 70%-80% of the data, is called the training set.  This set of data is used to train the machine learning algorithm (i.e. set the weights such that the cost function is minimized).  The second data set, which is much smaller, is called the test set.  This is used to test the accuracy of the machine learning algorithm on data that it has not yet seen.\n",
    "\n",
    "Below we use the common training-test split functionality from the library Scikit-Learn.  We will be using 80% of our total data set as the training set with the remaining 20% being the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TbbahdRlPIX"
   },
   "outputs": [],
   "source": [
    "# We will split the data set into two pieces, a training data set that contains\n",
    "# 80% of the total data and a test set that contains the other 20%\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_size = 0.8\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnu0xUo3li8O"
   },
   "source": [
    "### Define the Neural Network\n",
    "\n",
    "Now we will define a neural network using the equations for neural networks defined above.  First we will define the sigmoid function as our activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ty4x-OXNDjDv"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "        Calculates the value of the sigmoid function for \n",
    "        a given input of x\n",
    "    \"\"\"\n",
    "    return 1. / (1. + jnp.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1w51Ui0xbif"
   },
   "source": [
    "Now we will define our neural network.  Here we will be using an architecture with two hidden layers, each using the sigmoid activation function, and an output layer which does not have an activation function.  Note how the input to the second hidden layer is the output from the first hidden layer and the input to the output layer is the output from the second hidden layer.  We will be setting the number of neurons per layer later when we are training the neural network.  The code we have defined below does not require a specific number of neurons per hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anEAM0LJJgee"
   },
   "outputs": [],
   "source": [
    "def neural_network(W, x):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            W (a list of length 2): the weights of the neural network\n",
    "            x (a float): the input value of the neural network\n",
    "        Returns:\n",
    "            Unnamed (a float): The output of the neural network\n",
    "        Defines a neural network with one hidden layer.  The number of neurons in\n",
    "        the hidden layer is the length of W[0]. The activation function is the \n",
    "        sigmoid function on the hidden layer an none on the output layer.\n",
    "    \"\"\"\n",
    "    # Calculate the output for the neurons in the hidden layers\n",
    "    hidden_layer1 = jnp.tanh(jnp.dot(x,W[0]))\n",
    "    hidden_layer2 = jnp.tanh(jnp.dot(hidden_layer1, W[1]))\n",
    "    # Calculate the result for the output neuron\n",
    "    return jnp.tanh(jnp.dot(hidden_layer2, W[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cQWo3MulmGc"
   },
   "source": [
    "### Define the Loss Function\n",
    "\n",
    "Now we need to define our loss function.  For simplicity we will be using the mean-squared error loss function, which is a very common loss function for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sq2AIpStDIsH"
   },
   "outputs": [],
   "source": [
    "def loss_function(W, x, y):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            W (a list): the weights of the neural network\n",
    "            t (a 1D NumPy array): the times to calculate the predicted position at\n",
    "        Returns:\n",
    "            loss_sum (a float): The total loss over all times\n",
    "        The loss function for the neural network to solve for position given \n",
    "        a function for acceleration.\n",
    "    \"\"\"\n",
    "    # Define a variable to hold the total loss\n",
    "    loss_sum = 0.\n",
    "    # Loop through each individual time\n",
    "    for i in range(len(x)):\n",
    "        # Get the output of the neural network with the given set of weights\n",
    "        nn = neural_network(W, x[i])[0][0]\n",
    "        err_sqr = (nn-y[i])**2\n",
    "        # Update the loss sum\n",
    "        loss_sum += err_sqr\n",
    "    loss_sum /= len(x)\n",
    "    # Return the loss sum    \n",
    "    return loss_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJsuIjroT-Rb"
   },
   "source": [
    "### Train the Neural Network\n",
    "\n",
    "Finally we need to train our neural network.  We will start by randomly initializing the weights of our neural network (with 25 neurons per hidden layer).  We then define the parameters for the learning rate, the number of training iterations, and the threshold for stopping the training.  Next, we perform gradient descent to update the weights of the neural network over for the set number of training iterations, or until the loss function value converges to some set threshold.\n",
    "\n",
    "**WARNING**: This cell will take a long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63780,
     "status": "ok",
     "timestamp": 1655154417810,
     "user": {
      "displayName": "Julie Butler Hartley",
      "userId": "16397466870240358107"
     },
     "user_tz": -120
    },
    "id": "mPlInu6xDhNb",
    "outputId": "1363187d-93db-4f1c-96ac-be133e5118d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Iteration: 1\n",
      "Loss: 0.9679325\n",
      "\n",
      "Training Iteration: 2\n",
      "Loss: 0.87496966\n",
      "\n",
      "Training Iteration: 3\n",
      "Loss: 0.42899132\n",
      "\n",
      "Training Iteration: 4\n",
      "Loss: 0.56665987\n",
      "\n",
      "Training Iteration: 5\n",
      "Loss: 0.28957328\n",
      "\n",
      "Training Iteration: 6\n",
      "Loss: 0.37441632\n",
      "\n",
      "Training Iteration: 7\n",
      "Loss: 0.38987857\n",
      "\n",
      "Training Iteration: 8\n",
      "Loss: 0.40654492\n",
      "\n",
      "Training Iteration: 9\n",
      "Loss: 0.2975046\n",
      "\n",
      "Training Iteration: 10\n",
      "Loss: 0.33554095\n",
      "\n",
      "Training Iteration: 11\n",
      "Loss: 0.28734046\n",
      "\n",
      "Training Iteration: 12\n",
      "Loss: 0.31077173\n",
      "\n",
      "Training Iteration: 13\n",
      "Loss: 0.2668793\n",
      "\n",
      "Training Iteration: 14\n",
      "Loss: 0.28293598\n",
      "\n",
      "Training Iteration: 15\n",
      "Loss: 0.25128502\n",
      "\n",
      "Training Iteration: 16\n",
      "Loss: 0.26022694\n",
      "\n",
      "Training Iteration: 17\n",
      "Loss: 0.23660965\n",
      "\n",
      "Training Iteration: 18\n",
      "Loss: 0.2390252\n",
      "\n",
      "Training Iteration: 19\n",
      "Loss: 0.22199869\n",
      "\n",
      "Training Iteration: 20\n",
      "Loss: 0.21908057\n",
      "\n",
      "Training Iteration: 21\n",
      "Loss: 0.20718534\n",
      "\n",
      "Training Iteration: 22\n",
      "Loss: 0.20055237\n",
      "\n",
      "Training Iteration: 23\n",
      "Loss: 0.19233082\n",
      "\n",
      "Training Iteration: 24\n",
      "Loss: 0.18365496\n",
      "\n",
      "Training Iteration: 25\n",
      "Loss: 0.17786014\n",
      "\n",
      "Training Iteration: 26\n",
      "Loss: 0.16854252\n",
      "\n",
      "Training Iteration: 27\n",
      "Loss: 0.1642623\n",
      "\n",
      "Training Iteration: 28\n",
      "Loss: 0.15527935\n",
      "\n",
      "Training Iteration: 29\n",
      "Loss: 0.1519319\n",
      "\n",
      "Training Iteration: 30\n",
      "Loss: 0.1438376\n",
      "\n",
      "Training Iteration: 31\n",
      "Loss: 0.1410901\n",
      "\n",
      "Training Iteration: 32\n",
      "Loss: 0.13411222\n",
      "\n",
      "Training Iteration: 33\n",
      "Loss: 0.13178869\n",
      "\n",
      "Training Iteration: 34\n",
      "Loss: 0.1259442\n",
      "\n",
      "Training Iteration: 35\n",
      "Loss: 0.12395257\n",
      "\n",
      "Training Iteration: 36\n",
      "Loss: 0.119144745\n",
      "\n",
      "Training Iteration: 37\n",
      "Loss: 0.11743273\n",
      "\n",
      "Training Iteration: 38\n",
      "Loss: 0.113516614\n",
      "\n",
      "Training Iteration: 39\n",
      "Loss: 0.11204696\n",
      "\n",
      "Training Iteration: 40\n",
      "Loss: 0.10886968\n",
      "\n",
      "Training Iteration: 41\n",
      "Loss: 0.1076121\n",
      "\n",
      "Training Iteration: 42\n",
      "Loss: 0.10503288\n",
      "\n",
      "Training Iteration: 43\n",
      "Loss: 0.103959225\n",
      "\n",
      "Training Iteration: 44\n",
      "Loss: 0.101857044\n",
      "\n",
      "Training Iteration: 45\n",
      "Loss: 0.1009418\n",
      "\n",
      "Training Iteration: 46\n",
      "Loss: 0.09921744\n",
      "\n",
      "Training Iteration: 47\n",
      "Loss: 0.09843698\n",
      "\n",
      "Training Iteration: 48\n",
      "Loss: 0.097010955\n",
      "\n",
      "Training Iteration: 49\n",
      "Loss: 0.09634497\n",
      "\n",
      "Training Iteration: 50\n",
      "Loss: 0.09515467\n",
      "\n",
      "Training Iteration: 51\n",
      "Loss: 0.09458538\n",
      "\n",
      "Training Iteration: 52\n",
      "Loss: 0.09358216\n",
      "\n",
      "Training Iteration: 53\n",
      "Loss: 0.09309526\n",
      "\n",
      "Training Iteration: 54\n",
      "Loss: 0.09224184\n",
      "\n",
      "Training Iteration: 55\n",
      "Loss: 0.09182528\n",
      "\n",
      "Training Iteration: 56\n",
      "Loss: 0.091092415\n",
      "\n",
      "Training Iteration: 57\n",
      "Loss: 0.09073689\n",
      "\n",
      "Training Iteration: 58\n",
      "Loss: 0.09010229\n",
      "\n",
      "Training Iteration: 59\n",
      "Loss: 0.08980025\n",
      "\n",
      "Training Iteration: 60\n",
      "Loss: 0.08924679\n",
      "\n",
      "Training Iteration: 61\n",
      "Loss: 0.08899272\n",
      "\n",
      "Training Iteration: 62\n",
      "Loss: 0.08850703\n",
      "\n",
      "Training Iteration: 63\n",
      "Loss: 0.088296615\n",
      "\n",
      "Training Iteration: 64\n",
      "Loss: 0.087868385\n",
      "\n",
      "Training Iteration: 65\n",
      "Loss: 0.08769861\n",
      "\n",
      "Training Iteration: 66\n",
      "Loss: 0.08731979\n",
      "\n",
      "Training Iteration: 67\n",
      "Loss: 0.08718859\n",
      "\n",
      "Training Iteration: 68\n",
      "Loss: 0.08685253\n",
      "\n",
      "Training Iteration: 69\n",
      "Loss: 0.086758174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the key for the random number generator\n",
    "key = npr.PRNGKey(0)\n",
    "# Set the number of neurons in the hidden layer\n",
    "number_hidden_neurons = 25\n",
    "# Initialize the weights of the neural network with random numbers\n",
    "W = [npr.normal(key,(1, number_hidden_neurons)), \n",
    "     npr.normal(key,(number_hidden_neurons,number_hidden_neurons)), \n",
    "     npr.normal(key,(number_hidden_neurons, 1))]\n",
    "\n",
    "# Set the learning rate and the number of training iterations for the network\n",
    "learning_rate = 0.01\n",
    "num_training_iterations = 100\n",
    "threshold = 0.0001\n",
    "previous_loss = 0\n",
    "\n",
    "# Train the neural network for the specified number of iterations\n",
    "# Update the weights using the learning rates\n",
    "for i in range(num_training_iterations):\n",
    "    print(\"Training Iteration:\", i+1)\n",
    "    current_loss = loss_function(W,X_train,y_train)\n",
    "    print(\"Loss:\", current_loss)\n",
    "    print()\n",
    "    # If the current loss is within a set threshold of the previous loss, stop\n",
    "    # the training\n",
    "    if np.abs(current_loss-previous_loss) < threshold:\n",
    "        break;\n",
    "    # Calculate the gradient of the loss function and then use that gradient to\n",
    "    # update the weights of the neural network using the learning rate and the \n",
    "    # gradient descent optimization method\n",
    "    loss_grad =  grad(loss_function)(W, X_train, y_train)\n",
    "    W[0] = W[0] - learning_rate * loss_grad[0]\n",
    "    W[1] = W[1] - learning_rate * loss_grad[1]\n",
    "    W[2] = W[2] - learning_rate * loss_grad[2]\n",
    "    previous_loss = current_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x3W_Ksnlvc5"
   },
   "source": [
    "### Analyze the Results\n",
    "\n",
    "Now we need to analyze the performance of our neural network using the test data set that was reserved earlier.  First we need to generate the neural network predictions for the y component of the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pV5isOXjEe_C"
   },
   "outputs": [],
   "source": [
    "y_nn = [neural_network(W, xi)[0][0] for xi in X_test] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdSk3F8U1PvL"
   },
   "source": [
    "First let's analyze the results graphically by plotting the predicted test data set and the true test data set on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1655154418758,
     "user": {
      "displayName": "Julie Butler Hartley",
      "userId": "16397466870240358107"
     },
     "user_tz": -120
    },
    "id": "mODZn7OTFPxu",
    "outputId": "fea37d06-40c7-4296-84f1-d1b8ea01d39f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test, y_nn, s=50, color=\"blue\",label=\"NN Prediction\")\n",
    "plt.scatter(X_test, y_test, s=50, color=\"red\", label=\"True Data\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel(\"X\",fontsize=14)\n",
    "plt.ylabel(\"y\",fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfQyLgC21u85"
   },
   "source": [
    "Next let's analyze the error numerically using the root mean-squared error (RMSE) function, which is simply the square root of the mean-squared error.  The RMSE gives the average error on each data point (instead of the squared average error) so it is a met more of a clear metric for error analysis.  First, let's define a function to calculate the RMSE between two data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1e0YM82Eu0B"
   },
   "outputs": [],
   "source": [
    "def rmse(A,B):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            A,B (NumPy arrays)\n",
    "        Returns:\n",
    "            Unnamed (a float): the RMSE error between A and B\n",
    "        Calculates the RMSE error between A and B.\n",
    "    \"\"\"\n",
    "    assert len(A)==len(B),\"The data sets must be the same length to calcualte\\\n",
    "        the RMSE.\"\n",
    "    return np.sqrt(np.average((A-B)**2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqL-fMnZ2K7a"
   },
   "source": [
    "Now let's print the RMSE between the true test data set and the neural network prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1655154418769,
     "user": {
      "displayName": "Julie Butler Hartley",
      "userId": "16397466870240358107"
     },
     "user_tz": -120
    },
    "id": "ejW4oDj1Eyp3",
    "outputId": "a7989afe-96a2-4aef-f3ed-2b37e2c2d6d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE between true test set and neural network result: 0.4083000246981841\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE between true test set and neural network result:\", rmse(y_nn,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXG9RQ8j-4d2"
   },
   "source": [
    "**EXERCISE 2**: Are you satisfied with the performance of this neural network?  If not, brainstorm some ways in which its performance may be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkEhe0yZ-4rs"
   },
   "source": [
    "Delete this text and type your response here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4N-BKVEJHayF"
   },
   "source": [
    "## Neural Networks with a Popular Python Library\n",
    "\n",
    "When using a neural network for most cases, instead of creating one by hand you will likely use a neural network implementation from a popular Python library.  These have several advantages including being able to easily use large networks (which would be hard to create by hand), the use of more advanced optimizers for training, and more optimized implementations to significantly decrease runtime (and likely more accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fyXHqDLHpYp"
   },
   "source": [
    "### Keras\n",
    "\n",
    "One of the most popular machine learning library in Python is known as Keras, which is actually a wrapper for another machine learning library known as Tensorflow.  See the [Keras website](https://keras.io/) for more information.   Keras is very commonly used for training neural networks because it makes building and training neural networks simply and intuitive. \n",
    "\n",
    "Neural networks are build in keras by adding layers with the specified parameters to a model and then compiling the model with a loss function and an optimizer.  Here we use the same architecture as we have been using (two hidden layers with 25 neurons each and a hyperbolic tangent activation function).  The main difference here is that instead of using the gradient descent algorithm the optimize the weights of the neural network we will instead be using a much more powerful optimizer known as the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 10368,
     "status": "ok",
     "timestamp": 1655154429608,
     "user": {
      "displayName": "Julie Butler Hartley",
      "userId": "16397466870240358107"
     },
     "user_tz": -120
    },
    "id": "KDhBDvuxHnC3",
    "outputId": "d5fda35e-0e75-424c-dcd6-d194b3db87d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1078\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0645\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0427\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0409\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0352\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0326\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0304\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0283\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0264\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0249\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0230\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0216\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0204\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0193\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0182\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0171\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0165\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0155\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0151\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0142\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0134\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0127\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0122\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0118\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0112\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0108\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0103\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0096\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0093\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0090\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0085\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0082\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0076\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0068\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0066\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0063\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0061\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0059\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0057\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0054\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0052\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0050\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0046\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0045\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0043\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0040\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0036\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0035\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0032\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0030\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0030\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0027\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0025\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0022\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0020\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0020\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0020\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0019\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0019\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0018\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0018\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0018\n",
      "Epoch 82/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0017\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0017\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0016\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0016\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0016\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0016\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0010\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0010\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 9.8343e-04\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 9.9275e-04\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 9.7353e-04\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 9.6189e-04\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 9.3219e-04\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 9.3055e-04\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 9.2136e-04\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 8.8719e-04\n",
      "Epoch 117/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.5708e-04\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.6396e-04\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 8.2447e-04\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.3418e-04\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.1305e-04\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.7774e-04\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.7009e-04\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.4960e-04\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.4154e-04\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.5855e-04\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.1417e-04\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.0284e-04\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.0224e-04\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.7733e-04\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 6.6874e-04\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.6444e-04\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.5241e-04\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.4083e-04\n",
      "Epoch 135/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.3749e-04\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.6859e-04\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.3209e-04\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.0695e-04\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.0195e-04\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.9255e-04\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.8917e-04\n",
      "Epoch 142/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.8393e-04\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.6860e-04\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.6240e-04\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.7392e-04\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.7676e-04\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.7071e-04\n",
      "Epoch 148/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.3583e-04\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.2616e-04\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.2702e-04\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.3313e-04\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.0830e-04\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.0240e-04\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.2469e-04\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.9143e-04\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.8827e-04\n",
      "Epoch 157/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.7646e-04\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.6802e-04\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.6930e-04\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.8631e-04\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.5814e-04\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.6103e-04\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.6353e-04\n",
      "Epoch 164/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.6680e-04\n",
      "Epoch 165/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.5697e-04\n",
      "Epoch 166/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.6745e-04\n",
      "Epoch 167/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.3146e-04\n",
      "Epoch 168/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.2322e-04\n",
      "Epoch 169/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.3428e-04\n",
      "Epoch 170/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.1617e-04\n",
      "Epoch 171/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.1878e-04\n",
      "Epoch 172/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.0123e-04\n",
      "Epoch 173/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.0234e-04\n",
      "Epoch 174/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.9102e-04\n",
      "Epoch 175/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.8357e-04\n",
      "Epoch 176/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.7910e-04\n",
      "Epoch 177/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.7660e-04\n",
      "Epoch 178/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.6767e-04\n",
      "Epoch 179/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.6114e-04\n",
      "Epoch 180/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.6740e-04\n",
      "Epoch 181/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7303e-04\n",
      "Epoch 182/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.5549e-04\n",
      "Epoch 183/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.4674e-04\n",
      "Epoch 184/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.4938e-04\n",
      "Epoch 185/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.4346e-04\n",
      "Epoch 186/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.3951e-04\n",
      "Epoch 187/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.3777e-04\n",
      "Epoch 188/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.3488e-04\n",
      "Epoch 189/200\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.3452e-04\n",
      "Epoch 190/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.2418e-04\n",
      "Epoch 191/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.2094e-04\n",
      "Epoch 192/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.2427e-04\n",
      "Epoch 193/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 3.2259e-04\n",
      "Epoch 194/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.1608e-04\n",
      "Epoch 195/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.1022e-04\n",
      "Epoch 196/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.1024e-04\n",
      "Epoch 197/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.1451e-04\n",
      "Epoch 198/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.0206e-04\n",
      "Epoch 199/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.9827e-04\n",
      "Epoch 200/200\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.9468e-04\n",
      "0.38217810638619504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f5e7cc28410>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# Create the model \n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(units = 25, activation = 'tanh', input_dim=1))\n",
    "model.add(keras.layers.Dense(units = 25, activation = 'tanh'))\n",
    "model.add(keras.layers.Dense(units = 1, activation = 'tanh'))\n",
    "model.compile(loss='mse', optimizer=\"adam\")\n",
    "\n",
    "model.fit(X_train,y_train,epochs=200)\n",
    "y_nn = model.predict(X_test)\n",
    "print(rmse(y_nn, y_test))\n",
    "plt.scatter(X_test, y_nn)\n",
    "plt.scatter(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1P-ziJe_KD-"
   },
   "source": [
    "**EXERCISE 3**: Of the three implementations of neural networks explored here (from scratch, Scikit-Learn, and Keras), which implementation do you think is best?  Consider not just the accuracy of the neural network but also its runtime and ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7iLVA0z_KjU"
   },
   "source": [
    "Delete this text and type your response here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUfpS2Vp_K3T"
   },
   "source": [
    "## Practice What You Have Learned\n",
    "\n",
    "Go back to Exercise 2 where you brainstormed ways to improve the performance of the neural network.  Choose three of these ideas that you think are the most promising and implement them one at a time below the cell.  Copy and paste as much code from above as needed.  Record how your changes effect the error when predicting the test data set.  Were you able to get the error significantly lower than the notebook's result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hn0pdnDTHs2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOvMK4DSfDMHuhnrTkgBFKR",
   "collapsed_sections": [],
   "name": "02_neural_networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
