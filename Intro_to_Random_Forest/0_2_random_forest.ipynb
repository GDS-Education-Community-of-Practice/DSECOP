{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524a9c0e",
   "metadata": {},
   "source": [
    "# Using Random Forest for Classifying Exoplanets\n",
    "\n",
    "Author: Fatemeh (Fatima) Bagheri\n",
    "\n",
    "Date Created: November 24, 2022, Last Modified: December 19, 2022\n",
    "\n",
    "Notebook 2/2 of the DSECOP Module: An Introduction to the Random Forest algorithm.\n",
    "\n",
    "\n",
    "We want to classify exoplanets based on their types. There are two major types of exoplanets: Rocky planets and Jovian (Jupiter-like) planets. Rocky planets are similar to Earth; they are made of rocks, not massive, and located at a distance smaller than the *snow-line* ([https://en.wikipedia.org/wiki/Frost_line_(astrophysics)]) of their host stars. On the other hand, Jovian planets are massive, gaseous planets that could be found at any distance, so close to the host star (hot-Jupiters) or further away, like Jupiter in our solar system.\n",
    "\n",
    "Basically, our problem is a classification problem; if the exoplanet is a rocky planet, we label it with 0; if it's a Jovian but not Hot-Jupiter, its label is 1, and Hot_Jupiters' labels are 2. We implement a classification model with a `Random Forest classifier` in this lecture using Python's `Scikit-Learn`.\n",
    "\n",
    "The steps in this procedure are:\n",
    "\n",
    "* importing necessary libraries, \n",
    "* reading the data, \n",
    "* splitting the data into training and test sets,\n",
    "* defining hyperparameters (for more information, look at the Introduction to Deep Learning module) of the model and training a Random Forest Classifier model,\n",
    "* and last, evaluating the model.\n",
    "\n",
    "So, let's start! First, import some needed libraries,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b9d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c7376",
   "metadata": {},
   "source": [
    "Now, read the data, and you can also explore their parameters and simple statistics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e97144",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"RandomForest_data.csv\")\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79371523",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e412d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f30dbed",
   "metadata": {},
   "source": [
    "We only need some of the information in the data file to classify the exoplanets based on their types. Therefore, we should work just with the parameters that could be related to the exoplanets types:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3648078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file: NASAExoplanet_header.txt to understand the header of the data\n",
    "f = open('RandomForestData_header.txt', 'r')\n",
    "lines = f.readlines()\n",
    "\n",
    "header = {}\n",
    "for i, l in enumerate(lines):\n",
    "    parts = l.split('\\t')\n",
    "\n",
    "    header[parts[2].rstrip('\\n')] = parts[1].rstrip(' ')\n",
    "\n",
    "# define the paramters that we need, use the file: NASAExoplanet_header to find which columns of data you need\n",
    "parameters = ['Planet Mass or Mass*sin(i) [Earth Mass]', \n",
    "              'Stellar Metallicity [dex]', 'Stellar Surface Gravity [log10(cm/s**2)]'\n",
    "              ,'Stellar Mass [Solar mass]',\n",
    "              'Equilibrium Temperature [K]', 'Stellar Effective Temperature [K]', \n",
    "              'Orbital Period [days]', 'Stellar Radius [Solar Radius]','label']\n",
    "\n",
    "\n",
    "# store needed data\n",
    "data = pd.DataFrame()\n",
    "for p in parameters:\n",
    "    data[p] = dataset[header[p]].astype('float32')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7031b2",
   "metadata": {},
   "source": [
    "Now, we define the label of the exoplanets as what we want to predict or outputs (y) and the parameters of the exoplanets as the inputs (x):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c3b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['label']\n",
    "X = data.drop(['label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d1cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f225a3c0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Based on this data, we should define our training and test sets by splitting data with the ratio of 80/20, with the help of `Scikit-Learn's train_test_split()`: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80301a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a23b4",
   "metadata": {},
   "source": [
    "\n",
    "Now it's time to import the `RandomForestClassifier` class and create the model. As mentioned earlier, we should set a few hyperparameters to define our model. One is the number of decision trees in our random forest model; the other is the depth of each decision tree. Thus, let's create a model with 5 decision trees (`n_estimators=5`) with a depth of 3 (`max_depth=3`): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c31ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "our_model = RandomForestClassifier(n_estimators=5, max_depth=3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb9bdb",
   "metadata": {},
   "source": [
    "Our model should learn the relationship between the parameters in X and labels in y from the training set. To do this, we can **fit** the model with training set by calling `fit()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70275dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7927b5c",
   "metadata": {},
   "source": [
    "To see what exactly is happening in our model, we can look at the structure of decision trees. By calling the `tree module` from `cikit-Learn`, we can visualize the trees:\n",
    "This can be done by using the tree module built into `Scikit-Learn`, and then looping through each of the estimators in the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "features = X.columns.values                     # The name of each column\n",
    "classes = ['0', '1', '2']                            # Labels\n",
    "\n",
    "for estimator in our_model.estimators_:\n",
    "    print(estimator)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    tree.plot_tree(estimator, feature_names=features, class_names=classes, fontsize=8, filled=True, rounded=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960a6c18",
   "metadata": {},
   "source": [
    "We can now use our trained model to the predict the label of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aecf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = our_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f59a3",
   "metadata": {},
   "source": [
    "\n",
    "At this point, we can evaluate the performance of our model. There are several ways to do that. For instance, we can calculate the **accuracy** of the model. The accuracy of the model is the percentage of the number of correct predictions of our model.\n",
    "\n",
    "The other way is calculating the **confusion matrix**. A confusion matrix is s a specific table layout that allows visualization of the performance of an algorithm. If we want to know the number of **true positive** or **false positive** as well as **true negative** and **false negative**, we can use a confusion matrix. To understand the meaning of the positive/negative true/false concepts, let's define them in our project's context. We labeled the exoplanets in our dataset: if the exoplanet is a rocky planet, we label it with 0, or if it's a Jovian, its label is 1. so, let's say label 1 is positive, and label 0 is negative. Therefore, a positive true means we **correctly** predicted Jupiter-like type (label 1, which is positive) for a sample (or sometimes it's called an *instance* in ML project). The ones predicted as positives but weren't positives are called false positives. \n",
    "\n",
    "We can now compare the predicted labels against the real labels to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f085d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predicted)\n",
    "print(classification_report(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b1cf5f",
   "metadata": {},
   "source": [
    "## Variable Importances\n",
    "\n",
    "When we use data science methods in scientific research, we often need to learn the physical model behind the problem. In fact, most of the time, that is the reason we use data analyzing methods! Since you do not know the physical model, you want to know the role of each parameter in the analysis. To quantify the relative importance of the parameters in our model, we can use `feature_importances_` in `Skicit-learn` to see the role of a variable in the prediction quantitatively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b91604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = list(our_model.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(features, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c547a918",
   "metadata": {},
   "source": [
    "## Homework:\n",
    "\n",
    "Define true negatives and false negatives in the concept of our project.\n",
    "\n",
    "# Project \n",
    "\n",
    "Generate a new model with more trees and see how it affects the results. You can remove those variables that have no importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ea50d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
