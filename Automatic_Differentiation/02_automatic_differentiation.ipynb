{"cells":[{"cell_type":"markdown","id":"f91a9e5d","metadata":{"id":"f91a9e5d"},"source":["<a href=\"https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/Automatic_Differentiation/02_automatic_differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","id":"8f899cec-0dbb-45e3-97d4-22fcef355215","metadata":{"id":"8f899cec-0dbb-45e3-97d4-22fcef355215"},"source":["# Automatic differentiation for parameter estimation\n","\n","## Automatic differentiation"]},{"cell_type":"code","execution_count":null,"id":"4f8c1f7a","metadata":{"id":"4f8c1f7a"},"outputs":[],"source":["import numpy as np\n","import scipy.integrate as si\n","import matplotlib.pyplot as plt\n","from matplotlib.animation import FuncAnimation\n","from IPython.display import HTML"]},{"cell_type":"markdown","id":"ec3a7d09-4cc1-428a-adec-50f64678c111","metadata":{"id":"ec3a7d09-4cc1-428a-adec-50f64678c111"},"source":["Previously, we use a simple derivative approximation via “finite\n","differences” as was shown in **Equation 6**. This is known to be\n","an approximate method in that it requires choosing a value for $h$.\n","\n","Many modern machine learning methods instead use alternatives to this\n","approximation by exactly computing the derivatives of the computer\n","operations as they are done. This is called “forward mode automatic\n","differentiation”.\n","\n","To better understand how this works, consider a “dual number” made up of\n","two parts, a value $u$ and a derivative $v$. The number can be written\n","simply as $(u,v)$.\n","\n","**Examples:**\n","\n","-   We could write a variable $x$ as $(x,1)$ since the derivative\n","    $dx/dx = 1$\n","-   We could write a function $\\sin(x)$ as $(\\sin(x),\\cos(x))$ since the\n","    derivative $d/dx\\sin(x) = \\cos(x)$\n","-   We could write a function $\\sin(\\cos(x))$ as\n","    $(\\sin(\\cos(x)),-\\cos(\\cos(x))\\sin(x))$ since the derivative\n","    $d/dx\\sin(\\cos(x)) = -\\cos(\\cos(x))\\sin(x)$ by the chain rule\n","\n","To avoid big nesting like the last example, we can redefine common\n","functions to take dual numbers as input and yield dual numbers as\n","output.\n","\n","**Examples:**\n","\n","-   The function $f(x) = x^3$ can be redefined as\n","    $f(u,v) = (u^3, 3u^2v)$ since the derivative\n","    $df/dx = 3x^2\\frac{dx}{dx}$ by the chain rule\n","-   The function $f(x) = \\sin(x)$ can be redefined as\n","    $f(u,v) = (\\sin(u), \\cos(u)v)$ since the derivative\n","    $df/dx = \\cos(x)\\frac{dx}{dx}$ by the chain rule\n","\n","*Notice that derivatives always include the chain rule in case the value\n","$u$ is actually a function of $x$.*\n","\n","This is all a little bit funny, really just rewriting common\n","mathematical notation in an alternative format. However, if we can write\n","programs with this dual number logic, then we will be both computing\n","values and their derivatives at the same time! Additionally, the\n","derivatives computed will not be approximations, but the exact\n","derivatives of the expression (up to the rounding ability of the\n","computer).\n","\n","### Simple examples\n","\n","In order to write code with this “dual number” format, we can create a\n","Python “class” for dual numbers. For those not familiar, “classes” in\n","programming are just a structure that can store data and functions. So\n","for example, our class will store the data `value` and `derivative` as\n","the elements of the dual number. It will also store functions for common\n","operations like adding or subtracting dual numbers. In fact, the only\n","core operations of dual numbers are:\n","\n","1.  Addition: $(u,v) + (r,s) = (u+r, v+s)$\n","2.  Subtraction: $(u,v) - (r,s) = (u-r, v-s)$\n","3.  Multiplication: $(u,v) \\times (r,s) = (ur, us + rv)$ (by the product\n","    rule of calculus)\n","4.  Division: $(u,v) / (r,s) = (u / r, (rv - us) / r^2)$ (by the\n","    quotient rule)\n","\n","We can repeat these operations for a dual number operated with a regular\n","number:\n","\n","1.  Addition: $(u,v) + r = (u+r, v)$\n","2.  Subtraction: $(u,v) - r = (u-r, v)$\n","3.  Multiplication: $(u,v) \\times r = (ur, v)$\n","4.  Division: $(u,v) / r = (u / r, v)$\n","5.  Exponent: $(u,v)^r = (u^r, ru^{(r-1)}v)$\n","\n","*Note that we didn’t have an exponential of dual numbers with dual\n","numbers because that is messy.*\n","\n","We can also add some common mathematical functions:\n","\n","1.  Exponential: $e^{(u,v)} = (e^u, e^uv)$\n","2.  Logarithm: $\\log(u,v) = (\\log(u), v/u)$\n","3.  Sine: $\\sin(u,v) = (\\sin(u), \\cos(u)v)$\n","4.  Cosine: $\\cos(u,v) = (\\cos(u), -\\sin(u)v)$\n","\n","To get comfortable with a Python dual number class, let’s begin by\n","creating a class that stores a value and a derivative:"]},{"cell_type":"code","execution_count":null,"id":"76ab1b0a","metadata":{"id":"76ab1b0a"},"outputs":[],"source":["class Dual:\n","    def __init__(self, value, derivative):\n","        self.value = value\n","        self.derivative = derivative"]},{"cell_type":"markdown","id":"e4a1af7f-16c6-4550-9b3e-3dd61a8760a5","metadata":{"id":"e4a1af7f-16c6-4550-9b3e-3dd61a8760a5"},"source":["Notice that this stores the values in its `self`.\n","\n","We could make one of these numbers by simply writing:"]},{"cell_type":"code","execution_count":null,"id":"51c69d92","metadata":{"id":"51c69d92"},"outputs":[],"source":["dual = Dual(1, 0)"]},{"cell_type":"markdown","id":"6e90896e-b48f-4a4e-ae3e-d9a94c58d598","metadata":{"id":"6e90896e-b48f-4a4e-ae3e-d9a94c58d598"},"source":["We can then add an addition function `__add__` that gets called whenever\n","we have `Dual + Dual`:"]},{"cell_type":"code","execution_count":null,"id":"516854e4","metadata":{"id":"516854e4"},"outputs":[],"source":["class Dual:\n","    def __init__(self, value, derivative):\n","        self.value = value\n","        self.derivative = derivative\n","\n","    def __add__(self, other):\n","        value = self.value + other.value\n","        derivative = self.derivative + other.derivative\n","        return Dual(value, derivative)"]},{"cell_type":"markdown","id":"8807bde4-b326-4171-a9b0-496fc2d56fc5","metadata":{"id":"8807bde4-b326-4171-a9b0-496fc2d56fc5"},"source":["We can try this out by running:"]},{"cell_type":"code","execution_count":null,"id":"65a4a772","metadata":{"id":"65a4a772"},"outputs":[],"source":["dual1 = Dual(1, 0)\n","dual2 = Dual(2, 0)\n","dual1 + dual2"]},{"cell_type":"markdown","id":"96b7d1ac-6a6c-41ce-9ba5-aa4ab67c365b","metadata":{"id":"96b7d1ac-6a6c-41ce-9ba5-aa4ab67c365b"},"source":["It looks like we need a way to print our dual numbers. We can add that\n","in with the `__repr__` method:"]},{"cell_type":"code","execution_count":null,"id":"992c9496","metadata":{"id":"992c9496"},"outputs":[],"source":["class Dual:\n","    def __init__(self, value, derivative):\n","        self.value = value\n","        self.derivative = derivative\n","\n","    def __add__(self, other):\n","        value = self.value + other.value\n","        derivative = self.derivative + other.derivative\n","        return Dual(value, derivative)\n","\n","    def __repr__(self):\n","        return \"u = {}, du/dx = {}\".format(self.value, self.derivative)"]},{"cell_type":"markdown","id":"b9c6bbac-1dad-4abd-bd05-77442e32ca83","metadata":{"id":"b9c6bbac-1dad-4abd-bd05-77442e32ca83"},"source":["Now,"]},{"cell_type":"code","execution_count":null,"id":"f7c444fc","metadata":{"id":"f7c444fc"},"outputs":[],"source":["print(dual1 + dual2)"]},{"cell_type":"markdown","id":"13414a1c-a411-4892-bfed-77f6f8a18e79","metadata":{"id":"13414a1c-a411-4892-bfed-77f6f8a18e79"},"source":["Unfortunately, our `Dual` class does not yet allow for use to add dual\n","numbers with regular numbers. To add this functionality to our `__add__`\n","function, we need to check to see if the inputted number is a dual or\n","not:"]},{"cell_type":"code","execution_count":null,"id":"7727b9a0","metadata":{"id":"7727b9a0"},"outputs":[],"source":["class Dual:\n","    def __init__(self, value, derivative):\n","        self.value = value\n","        self.derivative = derivative\n","\n","    def __add__(self, other):\n","        if isinstance(other, Dual):\n","            value = self.value + other.value\n","            derivative = self.derivative + other.derivative\n","            return Dual(value, derivative)\n","        else:\n","            return Dual(self.value+other, self.derivative)\n","\n","    def __repr__(self):\n","        return \"u = {}, du/dx = {}\".format(self.value, self.derivative)"]},{"cell_type":"markdown","id":"2de87e02-9a3f-4345-84f3-81ef82a89447","metadata":{"id":"2de87e02-9a3f-4345-84f3-81ef82a89447"},"source":["We can check it with:"]},{"cell_type":"code","execution_count":null,"id":"d9a83e3a","metadata":{"id":"d9a83e3a"},"outputs":[],"source":["dual1 = Dual(1, 0)\n","number1 = 2\n","print(dual1 + number1)"]},{"cell_type":"markdown","id":"a4af42f5-5688-4af1-93e4-26c672dda89e","metadata":{"id":"a4af42f5-5688-4af1-93e4-26c672dda89e"},"source":["Unfortunately, our `__add__` function doesn’t work for\n","`number1 + dual1`. To add this functionality, we need to add a\n","`__radd__` function:"]},{"cell_type":"code","execution_count":null,"id":"d58f53c8","metadata":{"id":"d58f53c8"},"outputs":[],"source":["class Dual:\n","    def __init__(self, value, derivative):\n","        self.value = value\n","        self.derivative = derivative\n","\n","    def __add__(self, other):\n","        if isinstance(other, Dual):\n","            value = self.value + other.value\n","            derivative = self.derivative + other.derivative\n","            return Dual(value, derivative)\n","        else:\n","            return Dual(self.value+other, self.derivative)\n","\n","    def __radd__(self, other):\n","        return self + other\n","\n","    def __repr__(self):\n","        return \"u = {}, du/dx = {}\".format(self.value, self.derivative)"]},{"cell_type":"markdown","id":"6e9bf32f-5fac-41eb-9091-d8de6d20a346","metadata":{"id":"6e9bf32f-5fac-41eb-9091-d8de6d20a346"},"source":["Now,"]},{"cell_type":"code","execution_count":null,"id":"8ff232ca","metadata":{"id":"8ff232ca"},"outputs":[],"source":["dual1 = Dual(1, 0)\n","number1 = 2\n","print(number1 + dual1)"]},{"cell_type":"markdown","id":"d277aa3e-d597-4c48-82b5-4d03c54b8255","metadata":{"id":"d277aa3e-d597-4c48-82b5-4d03c54b8255"},"source":["Great! We now have a dual number class that can do addition!\n","\n","Given that example, we can add the other functions 2 - 12 from earlier\n","in the section to our class. These can be written as follows:"]},{"cell_type":"code","execution_count":null,"id":"0d592450","metadata":{"id":"0d592450"},"outputs":[],"source":["class Dual:\n","    # When creating a dual number, require a value and derivative\n","    def __init__(self,value,derivative):\n","        self.value = value\n","        self.derivative = derivative\n","\n","    # Operations with other dual numbers\n","    def __add__(self, other):\n","        if isinstance(other, Dual):\n","            # 1. dual + dual\n","            value = self.value + other.value\n","            derivative = self.derivative + other.derivative\n","            return Dual(value, derivative)\n","        else:\n","            # 5. dual + number\n","            return Dual(self.value + other, self.derivative)\n","\n","    def __sub__(self, other):\n","        if isinstance(other, Dual):\n","            # 2. dual - dual\n","            value = self.value - other.value\n","            derivative = self.derivative - other.derivative\n","            return Dual(value, derivative)\n","        else:\n","            # 6. dual - number\n","           return Dual(self.value - other, self.derivative)\n","\n","    def __mul__(self, other):\n","        if isinstance(other, Dual):\n","            # 3. dual * dual\n","            value = self.value * other.value\n","            derivative = self.value * other.derivative + other.value * self.derivative\n","            return Dual(value, derivative)\n","        else:\n","            # 7. dual * number\n","            return Dual(self.value * other, self.derivative * other)\n","\n","    def __truediv__(self, other):\n","        if isinstance(other, Dual):\n","            # 4. dual / dual\n","            value = self.value / other.value\n","            derivative = (other.value * self.derivative - self.value * other.derivative) / other.value**2\n","            return Dual(value, derivative)\n","        else:\n","            # 8. dual / number\n","            return Dual(self.value / other, self.derivative / other)\n","\n","    def __pow__(self, n):\n","        # 9. dual**number\n","        value = self.value ** n\n","        derivative = self.derivative * n * self.value ** (n-1)\n","        return Dual(value, derivative)\n","\n","    # In case the operations are called backwards, ie number + dual\n","    def __radd__(self, other):\n","        return self + other\n","\n","    def __rsub__(self, other):\n","        return -self + other\n","\n","    def __rmul__(self, other):\n","        return self * other\n","\n","    def __rtruediv__(self, other):\n","        return other / self.value\n","\n","    # For negating the number\n","    def __neg__(self):\n","        return self * -1\n","\n","    # For printing the number\n","    def __repr__(self):\n","        return \"y = {}, dy/dx = {}\".format(self.value, self.derivative)"]},{"cell_type":"markdown","id":"b0ceb128-395d-4c29-b735-bdc52531e38a","metadata":{"id":"b0ceb128-395d-4c29-b735-bdc52531e38a"},"source":["----\n","#### Exercise 1\n","\n","The `Dual` class currently implements some common operations such as `*,+,-,/` but it lacks special functions such as `exp,log,sin,cos`. Redefine the `Dual` class with including all the previously implemented functions but also include `exp,log,sin,cos` functions.\n","\n","**Solution:**"]},{"cell_type":"code","execution_count":null,"id":"644dd145-8bab-4430-81df-d319bfd5b2fa","metadata":{"id":"644dd145-8bab-4430-81df-d319bfd5b2fa"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"0e7bc75b-a5ae-4f0b-8339-9c2692fc4334","metadata":{"id":"0e7bc75b-a5ae-4f0b-8339-9c2692fc4334"},"source":["----\n","\n","Now let’s see how this class can be used to compute some simple\n","derivatives:\n","\n","Consider the function $f(x) = x^2 + 2x - 3$ with derivative\n","$f'(x) = 2x + 2$. With this info, $f(1) = 0$ and $f'(1) = 4$:"]},{"cell_type":"code","execution_count":null,"id":"622f00b3","metadata":{"id":"622f00b3"},"outputs":[],"source":["# f(x) = x^2 + 2x - 3\n","# df/dx = 2x + 2\n","# f(1) = 0, df/dx(1) = 4\n","x = Dual(1,1)\n","y = x**2 + 2*x - 3\n","print(y)"]},{"cell_type":"markdown","id":"7cc7f544-accc-4d2d-9e5e-0be3296d72a5","metadata":{"id":"7cc7f544-accc-4d2d-9e5e-0be3296d72a5"},"source":["Wow! Just by defining the first number as a `Dual`, we could write our\n","regular Python code and automatically get the derivative!\n","\n","Let’s try this with a couple other examples:"]},{"cell_type":"code","execution_count":null,"id":"21ae7b81","metadata":{"id":"21ae7b81"},"outputs":[],"source":["# f(x) = x^7 - 12x\n","# df/dx = 7x^6 - 12\n","# f(1) = -11, df/dx(1) = -5\n","x = Dual(1,1)\n","y = x**7 - 12*x\n","print(y)"]},{"cell_type":"code","execution_count":null,"id":"7d619ff4","metadata":{"id":"7d619ff4"},"outputs":[],"source":["# f(x) = sin(x)^2\n","# df/dx = 2sin(x)*cos(x)\n","# f(pi/4) = 1/2, df/dx(pi/4) = 1\n","x = Dual(np.pi/4,1)\n","y = np.sin(x)**2\n","print(y)"]},{"cell_type":"markdown","id":"98f10741-5877-4ac4-8284-fd5a42ccfd4a","metadata":{"id":"98f10741-5877-4ac4-8284-fd5a42ccfd4a"},"source":["How about something that is messy enough that we really don’t want to\n","work it out?"]},{"cell_type":"code","execution_count":null,"id":"53b8f907","metadata":{"id":"53b8f907"},"outputs":[],"source":["# f(x) = exp(sin(x^2 - (3-cos(x^7 + 10exp(x)))^2))\n","# df/dx = terribly messy\n","# f(1) = 0.50898, df/dx(1) = 8.62062\n","x = Dual(1,1)\n","y = np.exp(np.sin(x**2 - (3 - np.cos(x**7 + 10*np.exp(x)))**2))\n","print(y)"]},{"cell_type":"markdown","id":"1060f63f-20cf-4f08-aa2d-f6cda5a104a3","metadata":{"id":"1060f63f-20cf-4f08-aa2d-f6cda5a104a3"},"source":["Very automatic.\n","\n","## Parameter estimation with automatic differentiation\n","\n","Recall the previous notebook that illustrated a simple method for\n","estimating parameters of the equation for an RLC circuit using finite\n","differences and gradient descent. The equation is:\n","\n","<span id=\"eq-rlc-diffeq\">$$ L Q''(t) + RQ'(t) + \\frac{Q(t)}{C} = E(t)  \\qquad(1)$$</span>\n","\n","To review, it used the following code to simulate the system:"]},{"cell_type":"code","execution_count":null,"id":"ced77421","metadata":{"id":"ced77421"},"outputs":[],"source":["t0 = 0                           # Starting time\n","tf = 200                         # Final time\n","R = 1                            # Parameters\n","L = 10\n","C = 2\n","E = 60\n","ps = (R,L,C,E)\n","y0_0 = np.zeros(2)               # Starting conditions, Q(t0) = D(t0) = 0\n","times = np.linspace(t0, tf, 200) # Times to collect simulation at\n","\n","# dy/dt = f(t,y,R,L,C,E)\n","# Assuming y[0] = D(t) and y[1] = Q(t)\n","def f(t,y,R,L,C,E):\n","    return np.array([-R*y[0]/L - y[1]/(L*C) + E/L, y[0]])\n","\n","approx_solution = si.solve_ivp(f, (t0, tf), y0_0, args=ps, t_eval=times)"]},{"cell_type":"markdown","id":"1fc8b4c2-ca0b-4c3d-9823-9d97b3be4458","metadata":{"id":"1fc8b4c2-ca0b-4c3d-9823-9d97b3be4458"},"source":["It then used the following to estimate the parameters:"]},{"cell_type":"code","execution_count":null,"id":"2afe369f","metadata":{"id":"2afe369f"},"outputs":[],"source":["# First define how we calculate Q over time with simulation\n","def Q(R,L):\n","    return si.solve_ivp(f, (t0, tf), y0_0, args=(R,L,C,E), t_eval=times).y[1,:]\n","\n","def loss(R,L,Qhat):\n","    Q_RL = Q(R,L)\n","    result = 0\n","    N = len(Q_RL)\n","    for i in range(len(Q_RL)):\n","        result += (Q_RL[i] - Qhat[i])**2\n","    return result / N\n","\n","def dQ(R,L,h):\n","    Q_RL = Q(R,L)\n","    dQdR = (Q(R+h,L) - Q_RL) / h\n","    dQdL = (Q(R,L+h) - Q_RL) / h\n","    return np.array([dQdR,dQdL])\n","\n","def dloss(Qhat,R,L,h):\n","    Q_RL = Q(R,L)\n","    dQ_RL = dQ(R,L,h)\n","    N = len(Q_RL)\n","    return 2*np.sum((Q_RL - Qhat) * dQ_RL, axis=1) / N\n","\n","def gradient_descent(p0, df, alpha=0.1, max_iter=100):\n","    pstar = p0\n","    all_pstars = []\n","    for n in range(max_iter):\n","        pstar = pstar - alpha*df(pstar)\n","        all_pstars.append(pstar)\n","    return pstar, np.array(all_pstars)\n","\n","def plot_RL_descent(descent_RLs):\n","    # Map out the landscape\n","    max_R = max(abs(descent_RLs[:,0].max()+.5),5); min_R = min(abs(descent_RLs[:,0].min()-.5), .5)\n","    max_L = max(abs(descent_RLs[:,1].max()+.5),10.5); min_L = min(abs(descent_RLs[:,1].min()-.5), .5)\n","    Rs = np.linspace(min_R, max_R, 30)\n","    Ls = np.linspace(min_L, max_L, 30)\n","    Qs = np.zeros((30,30))\n","    for i in range(30):\n","        for j in range(30):\n","            Qs[j,i] = loss(Rs[i],Ls[j],data)\n","\n","    fig = plt.figure()\n","    plt.contourf(Rs,Ls,Qs,levels=30),\n","    plots = [\n","        plt.scatter(1,10,c='orange',s=100,zorder=2,label=\"Correct\"),\n","        plt.scatter(descent_RLs[0][0],descent_RLs[0][1],c='r',s=100,zorder=3,label=\"Gradient descent\"),\n","    ]\n","    plt.legend()\n","    plt.colorbar(plots[0], label=\"Loss\")\n","    plt.xlabel(\"$R$\"); plt.ylabel(\"$L$\")\n","    def anim_func(i):\n","        plots[1].set_offsets([descent_RLs[i][0],descent_RLs[i][1]])\n","        return plots\n","\n","    anim = FuncAnimation(fig, anim_func, frames=range(len(descent_RLs)), interval=100, blit=True)\n","    plt.close()\n","    return HTML(anim.to_jshtml())\n","\n","# Initial guess\n","RL_0 = np.array([8, 5])\n","data = approx_solution.y[1,:]\n","wrapped_dloss = lambda RL: dloss(data,RL[0],RL[1],1e-4)\n","\n","# Iteratively find the best parameters to match `data`\n","best_RL, descent_RLs = gradient_descent(RL_0, wrapped_dloss, 3e-3, 100)\n","plot_RL_descent(descent_RLs)"]},{"cell_type":"markdown","id":"3fbb19b8-c091-402c-8261-82ad4625e6c7","metadata":{"id":"3fbb19b8-c091-402c-8261-82ad4625e6c7"},"source":["This example uses finite differences to find the gradient with respect\n","to the parameters $R$ and $L$ and the `scipy.integrate.solve_ivp`\n","function to simulate [Equation 1](#eq-rlc-diffeq) for the different\n","values of $R$ and $L$. Given that we would like to now use our new\n","automatic differentiation with the `Dual` number class for $R$ and $L$,\n","we unfortunately cannot use `solve_ivp` as it sends most of the work to\n","`C` code where Python classes are all but forgotten. Thus, we will need\n","to implement our own simple numerical solver for differential equations.\n","\n","Comically, the simplest simulator for differential equations is the\n","Euler method, which uses finite differences. This method follows this\n","procedure:\n","\n","1.  Start with an initial condition $Q(t_0)$\n","2.  Approximate the derivative in time with a finite difference: $$\n","    \\begin{align*}\n","     \\frac{dQ}{dt}(t) \\approx \\frac{Q(t+h) - Q(t)}{h} &= f(t,R,L,C,E) \\\\\n","     Q(t+h) &= Q(t) + hf(t,R,L,C,E) \\\\\n","    \\end{align*}\n","    $$\n","3.  Take steps of size $h$ from starting time $t_0$ to end time $t_N$\n","    collecting the solution $Q(t_i)$ at all points along the way\n","\n","We can write a simple Python function to do this iteration (with dual\n","numbers) as follows:"]},{"cell_type":"code","execution_count":null,"id":"7e176448","metadata":{"id":"7e176448"},"outputs":[],"source":["def euler(f, y0, times, R, L, C, E):\n","    h = times[1]-times[0]\n","    ys = []\n","    ys.append(y0)\n","    for i in range(len(times)-1):\n","        ys.append(ys[i] + h*f(times[i], ys[i], R, L, C, E))\n","    return np.array(ys).T"]},{"cell_type":"markdown","id":"2fbac379-7f4e-4679-ae1c-2755b44f9f5c","metadata":{"id":"2fbac379-7f4e-4679-ae1c-2755b44f9f5c"},"source":["We can now make one of our parameters $R$ into a dual number and the\n","result of our Euler method simulation will include both the values of\n","$Q$ at all our time points and the derivative $\\frac{dQ}{dR}$ at all\n","those points:"]},{"cell_type":"code","execution_count":null,"id":"d988b26d","metadata":{"id":"d988b26d"},"outputs":[],"source":["R = Dual(1,1)\n","L = 10\n","y0 = np.array([Dual(0,1), Dual(0,1)])\n","\n","dual_solution = euler(f, y0, times, R, L, C, E)\n","\n","print(dual_solution[:,:10])"]},{"cell_type":"markdown","id":"f211ac67-a1e2-4c84-b8a0-b03af92f32ae","metadata":{"id":"f211ac67-a1e2-4c84-b8a0-b03af92f32ae"},"source":["This now allows us to rewrite our loss gradient function without needing\n","to do finite differences:\n","\n","----\n","#### Exercise 2\n","\n","Rewrite the `dloss` function to use the dual solutions instead of finite differences.\n","\n","Note that it might be easier to write a `dualQ` function (to replace the `Q` function we have been using) that returns both the solution `Q` and the derivatives `dQdR` and `dQdL`. Also note that in order to get the derivative with respect to `L`, you will have to use `euler` with only `L` as a `Dual` value. To get the derivative with respect to `R`, you will have to use only `R` as a `Dual` number.\n","\n","**Solution:**"]},{"cell_type":"code","execution_count":null,"id":"68d30587","metadata":{"id":"68d30587"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"26b4325b-c25a-467a-aa55-1cb3095b269e","metadata":{"id":"26b4325b-c25a-467a-aa55-1cb3095b269e"},"source":["*Note that unfortunately we need to run a simulation for each derivative\n","$\\frac{dQ}{dR}$ and $\\frac{dQ}{dL}$. This is computationally more\n","expensive that it needs to be and we could avoid it by instead having an\n","array for our dual values and derivatives, but that would require a bit\n","of rewriting for our `Dual` class that is beyond the scope of what we\n","are doing. However, if we were to make that improvement, we wouldn't even\n","need a `dloss` function. Instead we could use `loss` and directly pull out\n","the values $\\frac{d\\mathcal{L}}{dR}$ and $\\frac{d\\mathcal{L}}{dL}$ from\n","that.*\n","\n","----\n","\n","We can now use this new automatic differentiation loss to do gradient\n","descent:"]},{"cell_type":"code","execution_count":null,"id":"8df24a50","metadata":{"id":"8df24a50"},"outputs":[],"source":["RL_0 = np.array([Dual(8,1), Dual(5,1)])\n","data = approx_solution.y[1,:]\n","wrapped_dloss = lambda RL: dloss(data,RL[0],RL[1])\n","_, descent_RLs_dual = gradient_descent(RL_0, wrapped_dloss, 3e-3, 100)\n","\n","descent_RLs_values = np.array([np.array([RL[0].value,RL[1].value]) for RL in descent_RLs_dual])\n","\n","plot_RL_descent(descent_RLs_values)"]},{"cell_type":"markdown","id":"f2d7c79e-959b-4afb-abac-960d7ec41fed","metadata":{"id":"f2d7c79e-959b-4afb-abac-960d7ec41fed"},"source":["### Improving the estimation\n","\n","Note that the resulting parameter estimations do not arrive at the correct parameter values, though they are approaching.\n","How could this be given we now have more accurate derivatives?\n","\n","To answer this question, we need to remember that automatic differentiation gives exact derivatives *for the computational procedure*.\n","This means that it gives us exact derivatives of the Euler method.\n","But we know that the Euler method is a crude approximation with finite differences!\n","In comparison, the simulation method used by `solve_ivp` is far superior in accuracy and behavior.\n","\n","Thus, the key to improving our results is to improve the model that we use to match the data.\n","\n","With this in mind, let's see if we can improve our parameter estimation results by using a better simulation.\n","A commonly used, but much more accurate simulation method is called \"Runge-Kutta 4\".\n","The details of the method are beyond the scope of this module, but it can be written almost exactly like Euler method with just a few more operations for each time step as shown below:"]},{"cell_type":"code","execution_count":null,"id":"accfd681-87aa-4341-9623-3facb28028c1","metadata":{"id":"accfd681-87aa-4341-9623-3facb28028c1"},"outputs":[],"source":["def rk4(f, y0, times, R, L, C, E):\n","    h = times[1] - times[0]\n","    ys = []\n","    ys.append(y0)\n","    for i in range(len(times)-1):\n","        k1 = f(times[i], ys[i], R, L, C, E)\n","        k2 = f(times[i] + h/2, ys[i] + h*k1/2, R, L, C, E)\n","        k3 = f(times[i] + h/2, ys[i] + h*k2/2, R, L, C, E)\n","        k4 = f(times[i] + h, ys[i] + h*k3, R, L, C, E)\n","        ys.append(ys[i] + (h/6)*(k1 + 2*k2 + 2*k3 + k4))\n","    return np.array(ys).T"]},{"cell_type":"markdown","id":"c5bd5f76-9b29-4e31-bdf1-7cf671be1669","metadata":{"id":"c5bd5f76-9b29-4e31-bdf1-7cf671be1669"},"source":["Using this method, we can now redefine our `dualQ` computation and see much improved results:"]},{"cell_type":"code","execution_count":null,"id":"6160ed06-cc82-44ba-ab99-0e2cad2b74f2","metadata":{"id":"6160ed06-cc82-44ba-ab99-0e2cad2b74f2"},"outputs":[],"source":["def dualQ(R,L):\n","    dual_Rsolution = rk4(f, y0, times, R, L.value, C, E)\n","    dual_Lsolution = rk4(f, y0, times, R.value, L, C, E)\n","\n","    Q = np.array([Q.value for Q in dual_Rsolution[1]])\n","    dQdR = np.array([Q.derivative for Q in dual_Rsolution[1]])\n","    dQdL = np.array([Q.derivative for Q in dual_Lsolution[1]])\n","    return Q, np.array([dQdR, dQdL])\n","\n","_, descent_RLs_dual = gradient_descent(RL_0, wrapped_dloss, 30*1e-4, 100)\n","descent_RLs_values = np.array([np.array([RL[0].value,RL[1].value]) for RL in descent_RLs_dual])\n","plot_RL_descent(descent_RLs_values)"]},{"cell_type":"markdown","id":"d0d00555-1fd7-4bf9-85ce-876cde8b5ec2","metadata":{"id":"d0d00555-1fd7-4bf9-85ce-876cde8b5ec2"},"source":["We can now see that our parameter estimation lands right on top of the correct values. An excellent improvement!\n","\n","## Using Python libraries\n","\n","The demonstration we have gone through above has shown how easy it is to get started with forward mode automatic differentiation in Python. There really isn't any magic to it, it is just carefully defining functions to include the chain rule.\n","\n","However, there are other forms of automatic differentiation such as:\n","\n","- Reverse mode: instead of actually computing the derivatives at each step, just keep track of what operations should have been done (in a list or \"on a tape\"), then when you reach the end of the computation, go backwards calculating the derivatives until you reach the parameters of interest. This allows for some optimization of the operations because we have them all written out (so we may be able to do several at once). It's generally cheaper in computation time but more expensive in memory.\n","- Intermediate representation (IR): When a language like Python is run on a computer, it starts as the form you write, but is ultimately boiled down to 1s and 0s to send to your hardware. In between those two stages are usually a couple of \"intermediate representations\" of the code. The most cutting edge automatic differentiation looks at this IR stage (which has already been optimized) to determine the operations and therefore the gradient. It is more similar to reverse mode than forward mode.\n","\n","Below, we will explore some very common Python packages that implement automatic differentiation. These all generally implement reverse mode (which is cheaper for neural networks, their main focus). They are capable and well tested and can be used across many different applications.\n","\n","### PyTorch\n","In PyTorch, the key idea is to define all your objects as `torch` arrays instead of `Dual` numbers. To repeat an example from above:"]},{"cell_type":"code","execution_count":null,"id":"ae7a64a3-d176-43fd-b4fd-c7e2a016ecba","metadata":{"id":"ae7a64a3-d176-43fd-b4fd-c7e2a016ecba"},"outputs":[],"source":["import torch\n","\n","# f(x) = exp(sin(x^2 - (3-cos(x^7 + 10exp(x)))^2))\n","# df/dx = terribly messy\n","# f(1) = 0.50898, df/dx(1) = 8.62062\n","\n","# Use all pytorch objects and operations\n","x = torch.ones(1, requires_grad=True)\n","y = torch.exp(torch.sin(x**2 - (3 - torch.cos(x**7 + 10*torch.exp(x)))**2))\n","\n","# Print the actual computed value\n","print(y)\n","\n","# Do a \"backward pass\" of reverse mode autodiff and print the gradient\n","y.backward()\n","print(x.grad)"]},{"cell_type":"markdown","id":"41db1896-9397-48c5-8e98-23616ef48e52","metadata":{"id":"41db1896-9397-48c5-8e98-23616ef48e52"},"source":["Really not that far off from what we did before! Let's see if we can adjust our parameter estimation codes to make use of this. The basic idea here is to replace any `numpy` calls with `torch` calls."]},{"cell_type":"code","execution_count":null,"id":"6fa13029-e1b2-43ec-8f67-5cfa7cb5af2e","metadata":{"id":"6fa13029-e1b2-43ec-8f67-5cfa7cb5af2e"},"outputs":[],"source":["######## Just replacing `np.array` with `torch.stack` in most places ############\n","def torchf(t,y,R,L,C,E):\n","    return torch.stack([-R*y[0]/L - y[1]/(L*C) + E/L, y[0]])\n","\n","def torchrk4(f, y0, times, R, L, C, E):\n","    h = times[1] - times[0]\n","    ys = []\n","    ys.append(y0)\n","    for i in range(len(times)-1):\n","        k1 = f(times[i], ys[i], R, L, C, E)\n","        k2 = f(times[i] + h/2, ys[i] + h*k1/2, R, L, C, E)\n","        k3 = f(times[i] + h/2, ys[i] + h*k2/2, R, L, C, E)\n","        k4 = f(times[i] + h, ys[i] + h*k3, R, L, C, E)\n","        step = ys[i] + (h/6)*(k1 + 2*k2 + 2*k3 + k4)\n","        ys.append(step)\n","    return torch.stack(ys)\n","\n","def torch_descent(p0, df, alpha=0.1, max_iter=100):\n","    pstar = p0\n","    all_pstars = []\n","    for n in range(max_iter):\n","        pstar = pstar - alpha*df(pstar)\n","        all_pstars.append(pstar)\n","    return pstar, torch.stack(all_pstars)\n","\n","def torchloss(R,L,Qhat):\n","    Q_RL = torchQ(R,L)\n","    result = 0\n","    N = len(Q_RL)\n","    for i in range(len(Q_RL)):\n","        result += (Q_RL[i] - Qhat[i])**2\n","    return result / N\n","\n","############# More changes because of different gradient form  ###########\n","def torchQ(R,L):\n","    Q = torchrk4(torchf, y0, times, R, L, C, E)\n","    return Q[:,1]\n","\n","def torchdloss(Qhat,R,L):\n","    Q_loss = torchloss(R, L, Qhat)\n","    Q_loss.backward()\n","    Q_grad = tRL_0.grad\n","    # Make sure we get rid of the gradients for future runs!\n","    tRL_0.grad = torch.zeros(2)\n","    return Q_grad\n","\n","wrapped_torchdloss = lambda RL: torchdloss(data,RL[0],RL[1])"]},{"cell_type":"markdown","id":"212a360d-346d-4f25-b9ab-be8f1f010839","metadata":{"id":"212a360d-346d-4f25-b9ab-be8f1f010839"},"source":["We also need to redefine some of our initial conditions"]},{"cell_type":"code","execution_count":null,"id":"6f2e375c-41e0-433e-91d5-481a7046d65f","metadata":{"id":"6f2e375c-41e0-433e-91d5-481a7046d65f"},"outputs":[],"source":["t0 = 0\n","tf = 200\n","C = 2\n","E = 60\n","ps = (R,L,C,E)\n","y0 = torch.zeros(2)\n","times = torch.linspace(t0, tf, 200) # Times to collect simulation at\n","\n","data = torch.tensor(approx_solution.y[1,:])"]},{"cell_type":"code","execution_count":null,"id":"aefb60f0-06c4-4c03-9ffa-5bacf718cf2a","metadata":{"id":"aefb60f0-06c4-4c03-9ffa-5bacf718cf2a"},"outputs":[],"source":["tRL_0 = torch.tensor([8.0, 5.0], requires_grad=True)\n","_, descent_RL_values = torch_descent(tRL_0, wrapped_torchdloss, 30*1e-4, 100)\n","descent_RL_values = descent_RL_values.detach().numpy()\n","plot_RL_descent(descent_RL_values)"]},{"cell_type":"markdown","id":"242bfa86-27fe-423c-83ca-8a81138257c3","metadata":{"id":"242bfa86-27fe-423c-83ca-8a81138257c3"},"source":["### JAX\n","\n","JAX is a modern IR automatic differentiation library. This makes it a little different to work with than working with dual numbers or with the backward pass of PyTorch. Because it is analyzing the code itself, it actually makes it very easy to define exactly the derivative you are looking for. Instead of working with the variables themselves, you instead use functions and find their derivatives, then plug the values in.\n","\n","To really illustrate this, let's visit our previous example:"]},{"cell_type":"code","execution_count":null,"id":"5ce5a72f-3795-44bd-a974-63fa5165fa44","metadata":{"id":"5ce5a72f-3795-44bd-a974-63fa5165fa44"},"outputs":[],"source":["from jax import numpy as jnp\n","from jax import grad, jit\n","\n","# f(x) = exp(sin(x^2 - (3-cos(x^7 + 10exp(x)))^2))\n","# df/dx = terribly messy\n","# f(1) = 0.50898, df/dx(1) = 8.62062\n","\n","# Define a function and find it's gradient\n","x = 1.0\n","y = lambda x: jnp.exp(jnp.sin(x**2 - (3 - jnp.cos(x**7 + 10*jnp.exp(x)))**2))\n","dydx = grad(y)\n","\n","# Print the actual computed value\n","print(y(x))\n","\n","# Do a gradient value at dy/dx(1)\n","print(dydx(x))"]},{"cell_type":"markdown","id":"9100d4dd-f911-4410-ac80-e51b605dae77","metadata":{"id":"9100d4dd-f911-4410-ac80-e51b605dae77"},"source":["See how intuitive that was! Just like when we write out the math, we can write out the functions and find their derivatives without needing to put any values in.\n","\n","*Note that we replaced our `numpy` `np` variable with `jax.numpy`. This means that all the code we wrote before using numpy should just work with JAX!*\n","\n","Let's do it for our parameter estimation:"]},{"cell_type":"code","execution_count":null,"id":"982a1da3-8b9b-4363-aed0-1ad4695c0726","metadata":{"id":"982a1da3-8b9b-4363-aed0-1ad4695c0726"},"outputs":[],"source":["data = jnp.array(approx_solution.y[1,:])\n","y0 = jnp.zeros(2)\n","times = jnp.linspace(t0, tf, 200)\n","\n","RL_0 = jnp.array([8.0, 5.0])"]},{"cell_type":"code","execution_count":null,"id":"95cd179e-1f13-467d-b775-373c7b6b9d49","metadata":{"id":"95cd179e-1f13-467d-b775-373c7b6b9d49"},"outputs":[],"source":["######## Just replacing `np.array` with `jnp.array` in most places ############\n","def jaxf(t,y,R,L,C,E):\n","    return jnp.array([-R*y[0]/L - y[1]/(L*C) + E/L, y[0]])\n","\n","def jaxrk4(f, y0, times, R, L, C, E):\n","    h = times[1] - times[0]\n","    ys = []\n","    ys.append(y0)\n","    for i in range(len(times)-1):\n","        k1 = f(times[i], ys[i], R, L, C, E)\n","        k2 = f(times[i] + h/2, ys[i] + h*k1/2, R, L, C, E)\n","        k3 = f(times[i] + h/2, ys[i] + h*k2/2, R, L, C, E)\n","        k4 = f(times[i] + h, ys[i] + h*k3, R, L, C, E)\n","        step = ys[i] + (h/6)*(k1 + 2*k2 + 2*k3 + k4)\n","        ys.append(step)\n","    return jnp.array(ys)\n","\n","def jax_descent(p0, df, alpha=0.1, max_iter=100):\n","    pstar = p0\n","    all_pstars = []\n","    for n in range(max_iter):\n","        pstar = pstar - alpha*df(pstar)\n","        all_pstars.append(pstar)\n","    return pstar, jnp.array(all_pstars)\n","\n","@jit\n","def jaxloss(R,L,Qhat):\n","    Q_RL = jaxQ(R,L)\n","    result = 0\n","    N = len(Q_RL)\n","    for i in range(len(Q_RL)):\n","        result += (Q_RL[i] - Qhat[i])**2\n","    return result / N\n","\n","############# More changes because of different gradient form  ###########\n","def jaxQ(R,L):\n","    Q = jaxrk4(jaxf, y0, times, R, L, C, E)\n","    return Q[:,1]\n","\n","# Define our gradient function\n","wrapped_loss = lambda RL: jaxloss(RL[0],RL[1],data)\n","jaxdloss = grad(wrapped_loss)"]},{"cell_type":"markdown","id":"5d254aa4-cf46-466a-bfa0-1eab6a6197a6","metadata":{"id":"5d254aa4-cf46-466a-bfa0-1eab6a6197a6"},"source":["Now, the under the hood language analysis takes a significant amount of time up front. But we add the `jit` function call to just-in-time compile (do the analysis once for the code then just use it after). We should still run it just once to get things compiled:"]},{"cell_type":"code","execution_count":null,"id":"181fb04b-edfa-4189-b5f7-20a25b7d8841","metadata":{"id":"181fb04b-edfa-4189-b5f7-20a25b7d8841"},"outputs":[],"source":["jaxdloss(RL_0)"]},{"cell_type":"code","execution_count":null,"id":"fbed64f9-8b47-4567-8819-d97caa6ac525","metadata":{"id":"fbed64f9-8b47-4567-8819-d97caa6ac525"},"outputs":[],"source":["_, descent_RLs_jax = jax_descent(RL_0, jaxdloss, 30*1e-4, 100)\n","plot_RL_descent(descent_RLs_jax)"]},{"cell_type":"code","execution_count":null,"id":"4e8de485-00fe-4274-8d2b-9edbac0b120f","metadata":{"id":"4e8de485-00fe-4274-8d2b-9edbac0b120f"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":5}