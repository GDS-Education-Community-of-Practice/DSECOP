{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e280f71d",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/Automatic_Differentiation/01_parameter_estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889fbdd-5099-48c9-8f59-a6742c3d0240",
   "metadata": {
    "id": "c889fbdd-5099-48c9-8f59-a6742c3d0240"
   },
   "source": [
    "# Estimating parameters of a differential equation\n",
    "\n",
    "## RLC Circuit\n",
    "\n",
    "The RLC circuit is a simple model of a resistor, inductance, and\n",
    "capacitor connected in series as seen below:\n",
    "\n",
    "![Example of an RLC circuit](https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Automatic_Differentiation/images/rlc_circuit.drawio.svg)\n",
    "\n",
    "When the switch is closed, the battery/generator provides an electrical\n",
    "current through the entirety of the circuit. The voltage added by this\n",
    "battery/generator can be written as $E(t)$. At any time $t$, the current\n",
    "at any point in the circuit is given by $I(t)$ and if $I(t) > 0$ the\n",
    "flow is clockwise around the circuit and if $I(t) < 0$ the flow is\n",
    "counter-clockwise.\n",
    "\n",
    "The first component in the circuit is the “resistor” which reduces DC or\n",
    "AC current and releases the excess energy as heat. It causes a voltage\n",
    "potential change from one end to the other. This voltage change caused\n",
    "by the resistor can be written as:\n",
    "\n",
    "<span id=\"eq-resistance\">$$ V_R = RI(t)  \\qquad(1)$$</span>\n",
    "\n",
    "where $R>0$ is the resistance for the given resistor.\n",
    "\n",
    "The next component is the “inductor” which also reduces AC current but\n",
    "stores the excess energy in the form of a magnetic field. It also causes\n",
    "a voltage potential change from one end to the other. This voltage\n",
    "change can be written as:\n",
    "\n",
    "<span id=\"eq-inductance\">$$ V_L = L\\frac{d}{dt}I(t)  \\qquad(2)$$</span>\n",
    "\n",
    "where $L>0$ is the inductance for the given inductor.\n",
    "\n",
    "The final component is the “capacitor” which stores up electrical charge\n",
    "$Q(t)$. This charge is related to the current $I(t)$ by the following\n",
    "integral:\n",
    "\n",
    "$$ Q(t) = Q(t_0) + \\int_0^t I(s) ds. $$\n",
    "\n",
    "This integral expresses that the charge at time $t$ is equal to the\n",
    "charge at starting time $t_0$ plus the sum of the current from the start\n",
    "time until the present. Since the current is a continuous quantity, the\n",
    "integral is needed rather to sum the current. This relationship can be\n",
    "more simply written as $\\frac{d}{dt}Q(t) = I(t)$. Given this, the\n",
    "voltage change caused by the capacitor can be written as:\n",
    "\n",
    "<span id=\"eq-capacitance\">$$ V_C = \\frac{Q(t)}{C}  \\qquad(3)$$</span>\n",
    "\n",
    "where $C>0$ is the capacitance for the given capacitor.\n",
    "\n",
    "[Kirchoff’s\n",
    "law](https://en.wikipedia.org/wiki/Kirchhoff%27s_circuit_laws) states\n",
    "that the directed sum of voltages across a closed loop is 0 or equal to\n",
    "the “impressed voltage” or the voltage added from an energy source. In\n",
    "this RLC case, this means that:\n",
    "\n",
    "$$ V_R + V_L + V_C = E(t). $$\n",
    "\n",
    "By substituting in our voltages from [Equation 1](#eq-resistance),\n",
    "[Equation 2](#eq-inductance), [Equation 3](#eq-capacitance), we can\n",
    "expand this equation to:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V_R + V_L + V_C &= E(t) \\\\\n",
    "RI(t) + L\\frac{d}{dt}I(t) + \\frac{Q(t)}{C} &= E(t) \\\\\n",
    "R\\frac{d}{dt}Q(t) + L\\frac{d^2}{dt^2}Q(t) + \\frac{Q(t)}{C} &= E(t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "if we substitute $Q'(t)$ for $\\frac{d}{dt}Q(t)$ and rearrange:\n",
    "\n",
    "<span id=\"eq-rlc-diffeq\">$$ L Q''(t) + RQ'(t) + \\frac{Q(t)}{C} = E(t)  \\qquad(4)$$</span>\n",
    "\n",
    "[Reference](https://math.libretexts.org/Bookshelves/Differential_Equations/Elementary_Differential_Equations_with_Boundary_Value_Problems_(Trench)/06%3A_Applications_of_Linear_Second_Order_Equations/6.03%3A_The_RLC_Circuit)\n",
    "\n",
    "### Simulating the RLC circuit\n",
    "\n",
    "[Equation 4](#eq-rlc-diffeq) is actually simple enough to be solved with\n",
    "pen and paper. But in order to practice techniques that can be applied\n",
    "to more complicated equations that can’t be solved directly, let’s\n",
    "consider simulating the equation.\n",
    "\n",
    "We can use the `scipy.integrate` python library to simulate this\n",
    "equation. First, let’s import the Python libraries we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72048f32",
   "metadata": {
    "id": "72048f32"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as si\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c572180b-0a1c-498f-a9f3-bdf62e14c9e1",
   "metadata": {
    "id": "c572180b-0a1c-498f-a9f3-bdf62e14c9e1"
   },
   "source": [
    "Now, we can setup the simulation parameters such as the starting time\n",
    "and the equation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33b7d3",
   "metadata": {
    "id": "8d33b7d3"
   },
   "outputs": [],
   "source": [
    "t0 = 0\n",
    "R = 1\n",
    "L = 10\n",
    "C = 2\n",
    "E = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4276c15-91cc-4af1-b507-620ddf23274d",
   "metadata": {
    "id": "c4276c15-91cc-4af1-b507-620ddf23274d"
   },
   "source": [
    "In `scipy`, to solve for a quantity $y(t)$ it is assumed that the\n",
    "equation is of the form $y'(t) = f(t,y)$ where $f$ is just a function of\n",
    "time $t$ the quantity of interest $y(t)$. In our case, we need to\n",
    "introduce a “dummy variable” $D(t) = Q'(t)$ to be able to write our\n",
    "equation in that form:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L Q''(t) + RQ'(t) + \\frac{Q(t)}{C} &= E(t) \\\\\n",
    "L D'(t) + RD(t) + \\frac{Q(t)}{C} &= E(t) \\\\\n",
    "D'(t) + \\frac{R}{L}D(t) + \\frac{Q(t)}{LC} &= \\frac{E(t)}{L}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This may seem like a small change but it allows us to write our equation\n",
    "in the form:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dt}\n",
    "\\begin{bmatrix}\n",
    "D(t) \\\\ Q(t)\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "-\\frac{R}{L}D(t) - \\frac{Q(t)}{LC} + \\frac{E(t)}{L} \\\\ D(t)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now we have a form written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y(t) &= \\begin{bmatrix}D(t) \\\\ Q(t)\\end{bmatrix} \\\\\n",
    "f(t,y) &= \\begin{bmatrix}-\\frac{R}{L}D(t) - \\frac{Q(t)}{LC} + \\frac{E(t)}{L} \\\\ Q(t)\\end{bmatrix}\n",
    ".\\end{align}\n",
    "$$\n",
    "\n",
    "This can be written in Python (in `scipy` form) as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352c65a",
   "metadata": {
    "id": "d352c65a"
   },
   "outputs": [],
   "source": [
    "# Assuming y[0] = D(t) and y[1] = Q(t)\n",
    "def f(t,y,R,L,C,E):\n",
    "    return np.array([-R*y[0]/L - y[1]/(L*C) + E/L, y[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1083e04e-3752-4889-80c7-67de12118be8",
   "metadata": {
    "id": "1083e04e-3752-4889-80c7-67de12118be8"
   },
   "source": [
    "We can now find the amount of current in the circuit over time by\n",
    "simulating the equation (i.e. numerically approximating the solution at\n",
    "specific points in time) using `scipy` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae980063",
   "metadata": {
    "id": "ae980063"
   },
   "outputs": [],
   "source": [
    "tf = 200                         # Final time\n",
    "y0 = np.zeros(2)                 # Starting conditions, Q(t0) = D(t0) = 0\n",
    "ps = (R,L,C,E)                   # Parameters: R, L, C, E\n",
    "times = np.linspace(t0, tf, 200) # Times to collect simulation at\n",
    "approx_solution = si.solve_ivp(f, (t0, tf), y0, args=ps, t_eval=times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07408293-17cb-4bee-bce2-dcd17ee5e752",
   "metadata": {
    "id": "07408293-17cb-4bee-bce2-dcd17ee5e752"
   },
   "source": [
    "We can then plot the approximate solution with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34b280",
   "metadata": {
    "id": "ea34b280"
   },
   "outputs": [],
   "source": [
    "plt.plot(approx_solution.t, approx_solution.y[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cfe26-7dc2-4873-a741-38c6bd42e590",
   "metadata": {
    "id": "4b2cfe26-7dc2-4873-a741-38c6bd42e590"
   },
   "source": [
    "This demonstrates that after connecting the battery/generator, the\n",
    "circuit increases in current, oscillates for some time, then ultimately\n",
    "settles into a constant current.\n",
    "\n",
    "*Note: `t_eval=times` means that we collect information at `times`\n",
    "(which we will use later). Plotting only `approx_solution.y[1]` means we\n",
    "just take the values for $Q$ and not for the dummy variable $D$.*\n",
    "\n",
    "## Estimating parameters\n",
    "\n",
    "[Equation 4](#eq-rlc-diffeq) has a few key parameters that influence how\n",
    "quickly the current stabilizes. Specifically, the resistance $R$, the\n",
    "inductance $L$, and the capacitance $C$. These are often assumed to be\n",
    "some values for the sake of simulation and analysis. However, in many\n",
    "real world systems, these parameters may not be known exactly. For\n",
    "example, if an RLC circuit comes out of a factory, there may be a range\n",
    "of possible values for $R$, $L$, and $C$ due to manufacturing tolerances\n",
    "and material properties.\n",
    "\n",
    "We thus arrive at a key data science question: **“can we choose\n",
    "parameters for a given model such that the output matches what we\n",
    "observe in the real world”?** This problem is called “parameter\n",
    "estimation” and is extremely important in a wide variety of real world\n",
    "situations.\n",
    "\n",
    "**Contextual problem:** Let’s consider that we have some data from an\n",
    "RLC circuit with a capacitance that is well known to be $C=2$ but for\n",
    "which $R$ and $L$ are uncertain (having only two unknown parameters\n",
    "makes things easier to visualize). We’d like to know estimate these two\n",
    "uncertain parameters. The previously simulated data can act as “fake\n",
    "data” of the measured current in the circuit.\n",
    "\n",
    "Let’s first define our “objective” (also often called a “loss” or\n",
    "“error”). We’d like to choose $R$ and $L$ such that the simulated\n",
    "current $Q(t)$ matches the observed current $\\hat{Q}(t)$. We can use a\n",
    "common metric called the “least-squares” or “mean squared-error”\n",
    "comparison for our objective:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(R,L) = \\frac{1}{N}\\sum_{t=t_0}^{t_N} \\left( Q(t,R,L) - \\hat{Q}(t) \\right)^2\n",
    "$$\n",
    "\n",
    "Note that because our data is only observed at $N$ specific points, the\n",
    "objective function is a sum rather than an integral. We use the letter\n",
    "$\\mathcal{L}$ for the objective because it is commonly called the\n",
    "“loss”. We’d like the loss to be as small as possible because that would\n",
    "represent the best possible choice of $R$ and $L$. We’d thus like to\n",
    "“minimize the loss.” We can write this simple loss as a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e77f4",
   "metadata": {
    "id": "419e77f4"
   },
   "outputs": [],
   "source": [
    "# First define how we calculate Q over time with simulation\n",
    "def Q(R,L):\n",
    "    return si.solve_ivp(f, (t0, tf), y0, args=(R,L,C,E), t_eval=times).y[1,:]\n",
    "\n",
    "def loss(R,L,Qhat):\n",
    "    Q_RL = Q(R,L)\n",
    "    result = 0\n",
    "    N = len(Q_RL)\n",
    "    for i in range(len(Q_RL)):\n",
    "        result += (Q_RL[i] - Qhat[i])**2\n",
    "    return result / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eb4151-2901-49fd-a514-8e6cbac7ead1",
   "metadata": {
    "id": "e4eb4151-2901-49fd-a514-8e6cbac7ead1"
   },
   "source": [
    "*Note: the calculation of `Q(R,L)` is using variables\n",
    "`t0`,`t`,`y0`,`C`,`E` from the previous simulation.*\n",
    "\n",
    "Given this objective, we can try to find the optimal value of $R$ using\n",
    "simple calculus. i.e. the function $\\mathcal{L}$ is minimized with\n",
    "respect to $R$ at $R=R^*$ if $\\frac{d}{dR}\\mathcal{L}(R^*) = 0$. We can\n",
    "write this out as:\n",
    "\n",
    "<span id=\"eq-grad-loss\">$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dR}\\mathcal{L}(R^*,L) &= \\frac{d}{dR}\\frac{1}{N}\\sum_{t=t_0}^{t_N} \\left( Q(t, R^*, L) - \\hat{Q}(t) \\right)^2 \\\\\n",
    " &= \\frac{1}{N}\\sum_{t=t_0}^{t_N} 2\\left( Q(t, R^*, L) - \\hat{Q}(t) \\right)\\frac{d}{dR}Q(t,R^*,L)\n",
    "\\end{align}\n",
    " \\qquad(5)$$</span>\n",
    "\n",
    "The exact same formula would be valid if we were looking to find the\n",
    "optimal value of $L$, so we end up with a system of equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}0\\\\0\\end{bmatrix} &=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{N}\\sum_{t=t_0}^{t_N} 2\\left( Q(t, R^*, L) - \\hat{Q}(t) \\right)\\frac{d}{dR}Q(t,R^*,L) \\\\\n",
    "\\frac{1}{N}\\sum_{t=t_0}^{t_N} 2\\left( Q(t, R, L^*) - \\hat{Q}(t) \\right)\\frac{d}{dL}Q(t,R,L^*)\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Well, that’s a nice simple formula to find our optimal values! But there\n",
    "are two outstanding questions that remain:\n",
    "\n",
    "1.  How can we find the derivative of our simulation values $Q(t,R,L)$\n",
    "    with respect to $R$ or $L$?\n",
    "2.  Once we have those derivatives, how can we actually solve the\n",
    "    equation above?\n",
    "\n",
    "There are several answers to both questions, but the following two\n",
    "sections will cover a simple option for each.\n",
    "\n",
    "### Finite differences\n",
    "\n",
    "The definition of the derivative $\\frac{dQ}{dR}$ is:\n",
    "\n",
    "$$\n",
    "\\lim_{h\\to 0} \\frac{Q(t,R+h,L) - Q(t,R,L)}{h}.\n",
    "$$\n",
    "\n",
    "On a computer, we can approximate this by choosing a small $h$ and then\n",
    "approximating the integral with.\n",
    "\n",
    "<span id=\"eq-finite-diff\">$$\n",
    "\\frac{dQ}{dR} \\approx \\frac{Q(t,R,L) - Q(t,R+h,L)}{h}\n",
    " \\qquad(6)$$</span>\n",
    "\n",
    "This is called a “finite difference” approximation. Note that this\n",
    "approximation is only valid for fixed values of $t$ and $L$.\n",
    "\n",
    "We can write a simple function in Python to compute this value for our\n",
    "simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e91f6e",
   "metadata": {
    "id": "e2e91f6e"
   },
   "outputs": [],
   "source": [
    "# Compute the pieces we need for derivatives:\n",
    "# dQ/dR = [Q(R+h,L) - Q(R,L)] / h\n",
    "def dQR(R,h):\n",
    "    Q_RL = Q(R,L)\n",
    "    dQdR = (Q(R+h,L) - Q_RL) / h\n",
    "    return np.array([dQdR])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1fe673-5de6-4c82-b724-e8860f315a03",
   "metadata": {
    "id": "5e1fe673-5de6-4c82-b724-e8860f315a03"
   },
   "source": [
    "----\n",
    "#### Exercise 1\n",
    "The above function `dQR` only computes the derivative $\\frac{dQ}{dR}$. Write a function `dQ(R,L,h)` that computes both $\\frac{dQ}{dR}$ and $\\frac{dQ}{dL}$ and returns them in a Numpy array `np.array([...])`.\n",
    "\n",
    "*Solution:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1c328-e29d-4600-acec-c4fc1901c181",
   "metadata": {
    "id": "7ab1c328-e29d-4600-acec-c4fc1901c181"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96c284cc-cc8b-49d2-a950-3e9d2b435e47",
   "metadata": {
    "id": "96c284cc-cc8b-49d2-a950-3e9d2b435e47"
   },
   "source": [
    "----\n",
    "We can then write a function to compute the derivative of our gradient\n",
    "from [Equation 5](#eq-grad-loss) as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1f326",
   "metadata": {
    "id": "40c1f326"
   },
   "outputs": [],
   "source": [
    "def dloss(Qhat,R,L,h):\n",
    "    Q_RL = Q(R,L)\n",
    "    dQ_RL = dQ(R,L,h)\n",
    "    N = len(Q_RL)\n",
    "    return 2*np.sum((Q_RL - Qhat) * dQ_RL, axis=1) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12da3c7-057c-4c6a-bbbf-8967a426cbda",
   "metadata": {
    "id": "d12da3c7-057c-4c6a-bbbf-8967a426cbda"
   },
   "source": [
    "### Gradient descent\n",
    "\n",
    "Given the derivative information, how can we solve the system of\n",
    "equations\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}0\\\\0\\end{bmatrix} &=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{N}\\sum_{t=t_0}^{t_N} 2\\left( Q(t, R^*, L) - \\hat{Q}(t) \\right)\\frac{d}{dR}Q(t,R^*,L) \\\\\n",
    "\\frac{1}{N}\\sum_{t=t_0}^{t_N} 2\\left( Q(t, R, L^*) - \\hat{Q}(t) \\right)\\frac{d}{dL}Q(t,R,L^*)\n",
    "\\end{bmatrix}\n",
    "?\\end{align}\n",
    "$$\n",
    "\n",
    "There is a simple technique called “gradient descent” that can be used\n",
    "to solve this problem. The premise is that if we can estimate what the\n",
    "optimal values $R^*,L^*$ are, then we can follow the gradient\n",
    "(derivatives) $\\frac{d\\mathcal{L}}{dR},\\frac{d\\mathcal{L}}{dL}$ down to\n",
    "the optimal values step by step. This procedure can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "R_{n+1} \\\\ L_{n+1}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "R_{n} - \\alpha \\frac{d\\mathcal{L}}{dR}(R_n) \\\\\n",
    "L_{n} - \\alpha \\frac{d\\mathcal{L}}{dL}(L_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the size of the step to take at each iteration\n",
    "(commonly called the “learning rate”). We can start at some guess of\n",
    "parameters $[R_0, L_0]$ and then iteratively improve them to better\n",
    "match our observed data.\n",
    "\n",
    "We can write a simple version of this in Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de5311",
   "metadata": {
    "id": "93de5311"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(p0, df, alpha=0.1, max_iter=100):\n",
    "    pstar = p0\n",
    "    all_pstars = []\n",
    "    for n in range(max_iter):\n",
    "        pstar = pstar - alpha*df(pstar)\n",
    "        all_pstars.append(pstar)\n",
    "    return pstar, np.array(all_pstars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489e3a5-5b72-49bb-8dc3-5e469e6f2a14",
   "metadata": {
    "id": "a489e3a5-5b72-49bb-8dc3-5e469e6f2a14"
   },
   "source": [
    "To visualize the idea behind this, consider finding an $x^*$ such that\n",
    "$\\frac{dy}{dx}(x^*) = 0$ for the function $y(x) = \\sin(x^3 + 0.1x)$ on\n",
    "the interval $\\frac{-\\pi}{2} \\leq x \\leq \\frac{\\pi}{3}$. Taking 100\n",
    "steps of gradient descent on this function can be visualized as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3181faa5",
   "metadata": {
    "id": "3181faa5"
   },
   "outputs": [],
   "source": [
    "# Define our function and its derivative\n",
    "y = lambda x: np.sin(x**3 + 0.1*x)\n",
    "dydx = lambda x: np.cos(x**3 + 0.1*x)*(3*x**2 + 0.1)\n",
    "\n",
    "# Make a guess at where the zero is\n",
    "x_0 = np.pi/3\n",
    "\n",
    "# Use gradient descent to iteratively improve our guess\n",
    "best_xs, descent_xs = gradient_descent(x_0, dydx, 0.1, 100)\n",
    "\n",
    "# Make a video showing how the iterations improved\n",
    "fig = plt.figure()\n",
    "xs = np.linspace(-np.pi/2,np.pi/3)\n",
    "plots = [\n",
    "    plt.plot(xs,y(xs),label=\"$y(x)$\")[0],\n",
    "    plt.scatter(descent_xs[0],y(descent_xs[0]),c='r',s=100,zorder=2,label=\"$x^*$\"),\n",
    "]\n",
    "plt.legend()\n",
    "def anim_func(i):\n",
    "    plots[1].set_offsets([descent_xs[i],y(descent_xs[i])])\n",
    "    return plots\n",
    "\n",
    "anim = FuncAnimation(fig, anim_func, frames=range(len(descent_xs)), interval=100, blit=True)\n",
    "plt.close()\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071ea05-eb22-478c-9124-ce654df64289",
   "metadata": {
    "id": "b071ea05-eb22-478c-9124-ce654df64289"
   },
   "source": [
    "In this animation, the red dot represents the estimated value of $x^*$.\n",
    "You can see that step by step, it “rolls” down the landscape toward the\n",
    "smallest point. The direction it rolls is given by the derivative\n",
    "$df/dx$ which points it towards smaller regions.\n",
    "\n",
    "We would like to do exactly this for the RLC circuit using a function that is two\n",
    "dimensional (because it takes in parameters $R$ and $L$).\n",
    "\n",
    "## Example of parameter estimation\n",
    "\n",
    "We have some data from our previous simulation and we know that the true\n",
    "values are $R=1$ and $L=10$. If we take $R=8$ and $L=5$ as an initial\n",
    "guess, we can plug our previous functions into the `gradient_descent`\n",
    "function to get an estimate of the true values of $R$ and $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9148a0",
   "metadata": {
    "id": "4d9148a0"
   },
   "outputs": [],
   "source": [
    "# Some initial guesses\n",
    "RL_0 = np.array([8, 5])\n",
    "\n",
    "# The data we collected in our previous simulation\n",
    "data = approx_solution.y[1,:]\n",
    "\n",
    "# We want to have a function that only takes in R and L as inputs\n",
    "# so, we fix Qhat and h\n",
    "wrapped_dloss = lambda RL: dloss(data,RL[0],RL[1],1e-4)\n",
    "\n",
    "# Find the gradient descent values\n",
    "best_RL, descent_RLs = gradient_descent(RL_0, wrapped_dloss, 3e-3, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734bb65-47b0-412e-816d-d1f6ba8a36ef",
   "metadata": {
    "id": "4734bb65-47b0-412e-816d-d1f6ba8a36ef"
   },
   "source": [
    "The answer can be visualized similar to the example above. However, this\n",
    "time we are varying parameters $R$ and $L$ so we have a two dimensional\n",
    "plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a920ead-585c-4456-95ee-58cf38fb5bae",
   "metadata": {
    "id": "5a920ead-585c-4456-95ee-58cf38fb5bae"
   },
   "outputs": [],
   "source": [
    "def plot_RL_descent(descent_RLs):\n",
    "    # Map out the landscape\n",
    "    max_R = max(abs(descent_RLs[:,0].max()+.5),5); min_R = min(abs(descent_RLs[:,0].min()-.5), .5)\n",
    "    max_L = max(abs(descent_RLs[:,1].max()+.5),10.5); min_L = min(abs(descent_RLs[:,1].min()-.5), .5)\n",
    "    Rs = np.linspace(min_R, max_R, 30)\n",
    "    Ls = np.linspace(min_L, max_L, 30)\n",
    "    Qs = np.zeros((30,30))\n",
    "    for i in range(30):\n",
    "        for j in range(30):\n",
    "            Qs[j,i] = loss(Rs[i],Ls[j],data)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.contourf(Rs,Ls,Qs,levels=30),\n",
    "    plots = [\n",
    "        plt.scatter(1,10,c='orange',s=100,zorder=2,label=\"Correct\"),\n",
    "        plt.scatter(descent_RLs[0][0],descent_RLs[0][1],c='r',s=100,zorder=3,label=\"Gradient descent\"),\n",
    "    ]\n",
    "    plt.legend()\n",
    "    plt.colorbar(plots[0], label=\"Loss\")\n",
    "    plt.xlabel(\"$R$\"); plt.ylabel(\"$L$\")\n",
    "    def anim_func(i):\n",
    "        plots[1].set_offsets([descent_RLs[i][0],descent_RLs[i][1]])\n",
    "        return plots\n",
    "\n",
    "    anim = FuncAnimation(fig, anim_func, frames=range(len(descent_RLs)), interval=100, blit=True)\n",
    "    plt.close()\n",
    "    return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f1fd1-00f6-48d6-bf09-94c5b88f54dc",
   "metadata": {
    "id": "a24f1fd1-00f6-48d6-bf09-94c5b88f54dc"
   },
   "outputs": [],
   "source": [
    "plot_RL_descent(descent_RLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e87a2-236f-4335-92dd-9b1b81d25f2d",
   "metadata": {
    "id": "811e87a2-236f-4335-92dd-9b1b81d25f2d"
   },
   "source": [
    "As we can see, the gradient is nice and smooth and everything works\n",
    "great! Our approximation starts far away but converges quickly to the\n",
    "correct values of $R$ and $L$.\n",
    "\n",
    "### Things to watch out for\n",
    "\n",
    "The above parameter estimation works swimmingly. However, there are\n",
    "several places where numbers were selected without any justification. In\n",
    "the following sections, we will go through each of these dangers\n",
    "individually to explore how they can make parameter estimation\n",
    "difficult.\n",
    "\n",
    "#### Number of comparison points\n",
    "\n",
    "In the beginnning of this notebook, the variable `times` was arbitrarily\n",
    "set to be of a certain length. This defines the number of time points at\n",
    "which we compare the “true” data with the simulation data for different\n",
    "values of $R$ and $L$. Though this seems like a minor detail, it can\n",
    "have a large impact on the success of a gradient descent. In particular,\n",
    "if we use too few points, that might not be enough information to get a\n",
    "good gradient with.\n",
    "\n",
    "Let’s explore this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dbb4d7",
   "metadata": {
    "id": "f8dbb4d7"
   },
   "outputs": [],
   "source": [
    "# Using only 5 points for comparison\n",
    "times = np.linspace(t0, tf, 5)\n",
    "data = si.solve_ivp(f, (t0, tf), y0, args=ps, t_eval=times).y[1,:]\n",
    "wrapped_dloss = lambda RL: dloss(data,RL[0],RL[1],1e-4)\n",
    "_, descent_RLs_5_point = gradient_descent(RL_0, wrapped_dloss, 3e-3, 100)\n",
    "\n",
    "plot_RL_descent(descent_RLs_5_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290508b-afa2-4381-b2a5-f2b7a16127ee",
   "metadata": {
    "id": "0290508b-afa2-4381-b2a5-f2b7a16127ee"
   },
   "source": [
    "As you can see, with only a couple of points at which to compare the\n",
    "simulation with the real data, we struggle to converge, indicating a\n",
    "weak gradient. You can observe in the loss landscape that the gradient\n",
    "is tiny almost everywhere! This makes our iterations very very slow to\n",
    "approach the correct value.\n",
    "\n",
    "If we increase that number, we can get improved results:\n",
    "\n",
    "----\n",
    "#### Exercise 2\n",
    "Explore using more comparison points. Determine how many comparison points are needed to converge to the correct answer in 100 iterations. Then determine how many iterations it takes to converge to the correct point when using all the comparison points.\n",
    "\n",
    "*Solution:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4082822",
   "metadata": {
    "id": "f4082822"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d3e65f1-a7a9-440c-a645-272bc1d4607f",
   "metadata": {
    "id": "3d3e65f1-a7a9-440c-a645-272bc1d4607f"
   },
   "source": [
    "This is somewhat of a trick question. The student should observe that it requires an outrageous number of iterations due to the other errors described below.\n",
    "\n",
    "----\n",
    "\n",
    "#### Initial guess\n",
    "\n",
    "Another number that was arbitrarily chosen for the earlier parameter\n",
    "estimation is the inital guess `RL_0`. After having seen the loss\n",
    "landscapes in the previous animations, we can note that the speed at\n",
    "which our gradient iteration approaches the correct value depends\n",
    "significantly on the starting point. Furthermore, there are regions in\n",
    "which it doesn’t make physical sense to start (such as for $R<0$).\n",
    "\n",
    "Let’s explore how this starting point can impact our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b206abd",
   "metadata": {
    "id": "9b206abd"
   },
   "outputs": [],
   "source": [
    "# Reset our number of points\n",
    "times = np.linspace(t0, tf, 200)\n",
    "data = si.solve_ivp(f, (t0, tf), y0, args=ps, t_eval=times).y[1,:]\n",
    "\n",
    "# Set an initial condition very far away\n",
    "RL_0 = np.array([50, 80])\n",
    "_, descent_RLs_RL0far = gradient_descent(RL_0, wrapped_dloss, 3e-3, 100)\n",
    "\n",
    "plot_RL_descent(descent_RLs_RL0far)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356715b3-b56c-4346-89cc-30e59193a038",
   "metadata": {
    "id": "356715b3-b56c-4346-89cc-30e59193a038"
   },
   "source": [
    "Note that in this case, the descent seems like it is on a successful\n",
    "track, but is moving slowly and from very far away. It is hard to say\n",
    "how many iterations it will take for it to arrive at the correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306dc71",
   "metadata": {
    "id": "7306dc71"
   },
   "outputs": [],
   "source": [
    "# Set an initial condition in a \"bad\" location\n",
    "RL_0 = np.array([0.1, 5])\n",
    "_, descent_RLs_RL0bad = gradient_descent(RL_0, wrapped_dloss, 3e-3, 100)\n",
    "\n",
    "plot_RL_descent(descent_RLs_RL0bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad744303-1768-44c8-a057-0e92b840eb2f",
   "metadata": {
    "id": "ad744303-1768-44c8-a057-0e92b840eb2f"
   },
   "source": [
    "As you can see, beginning too close to a region which is physically\n",
    "impossible for the system results in enormous gradients that push our\n",
    "iteration far away from the true values. This ultimately hampers our\n",
    "ability to get accurate estimates of the parameters. To avoid this, it\n",
    "is important to take some time to consider reasonable ranges for the\n",
    "parameters to help try one or more inital guesses.\n",
    "\n",
    "#### Choosing step sizes\n",
    "\n",
    "A core component to succesful parameter estimation are the size of the\n",
    "derivative step size `h` and the gradient descent step size `alpha`.\n",
    "Each of these helps control the size of the gradient and are vital for a\n",
    "successful approximation. Given that they both have similar effects on\n",
    "the results, we will limit our exploration to the iteration step size\n",
    "`alpha`. If this step size is too small, the iteration will converge too\n",
    "slowly and possibly never reach the true values. On the other hand, if\n",
    "this step is too large, our iterations may step right past or over the\n",
    "true values which could result in our approximations approaching\n",
    "infinity or yielding physically impossible parameter values.\n",
    "\n",
    "This can be seen in the following two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62008911",
   "metadata": {
    "id": "62008911"
   },
   "outputs": [],
   "source": [
    "# Reset the initial condition\n",
    "RL_0 = np.array([8, 5])\n",
    "\n",
    "# Use a small step size for our gradient descent\n",
    "_, descent_RLs_small_step = gradient_descent(RL_0, wrapped_dloss, 30*1e-6, 100)\n",
    "\n",
    "plot_RL_descent(descent_RLs_small_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da853b-f6a9-477b-be8e-2f967b7f5541",
   "metadata": {
    "id": "20da853b-f6a9-477b-be8e-2f967b7f5541"
   },
   "source": [
    "Notice how slowly the iteration progresses with this small step size. As\n",
    "observed previously, this ultimately may mean that the procedure only\n",
    "arrives at the true parameter values after a lot of computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06e869",
   "metadata": {
    "id": "1b06e869"
   },
   "outputs": [],
   "source": [
    "# Use a large step size for our gradient descent\n",
    "_, descent_RLs_large_step = gradient_descent(RL_0, wrapped_dloss, 1e-2, 100)\n",
    "\n",
    "plot_RL_descent(descent_RLs_large_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7c1d7-382d-49ff-820a-b4477eab5a03",
   "metadata": {
    "id": "73e7c1d7-382d-49ff-820a-b4477eab5a03"
   },
   "source": [
    "Alternatively, this large step size results in erratic and incorrect\n",
    "iterations which never arrive at the correct parameter values.\n",
    "\n",
    "#### Dealing with noisy data\n",
    "\n",
    "A final consideration to take into account when looking at parameter\n",
    "estimation is the quality of the data used for comparison. In the\n",
    "examples above, the “measured” data was actually just a simulation with\n",
    "some known parameter values. This means that the data is smooth, well\n",
    "behaved, and is guaranteed to have a solution with our model. In real\n",
    "world scenarios, the data is often polluted by some sort of measurement\n",
    "noise, with some missing or completely incorrect values, and is actually\n",
    "represented by a model more complicated than our simple estimate from\n",
    "[Equation 4](#eq-rlc-diffeq).\n",
    "\n",
    "We can get a simple example of this by adding some Gaussian noise to our\n",
    "measurement data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81402720",
   "metadata": {
    "id": "81402720"
   },
   "outputs": [],
   "source": [
    "# Add 10% of the maximum of our data in Gaussian noise\n",
    "noisy_data = data + 0.1 * data.max() * np.random.normal(size=data.shape)\n",
    "\n",
    "# Show the new data\n",
    "plt.plot(times, noisy_data); plt.title(\"Data with 10% noise\"); plt.show()\n",
    "\n",
    "# Gradient descent and plot\n",
    "wrapped_dloss = lambda RL: dloss(noisy_data,RL[0],RL[1],1e-4)\n",
    "_, descent_RLs_small_noise = gradient_descent(RL_0, wrapped_dloss, 3e-3, 100)\n",
    "\n",
    "plot_RL_descent(descent_RLs_small_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb7adf-c193-4f2b-8f74-ec3047824048",
   "metadata": {
    "id": "7cbb7adf-c193-4f2b-8f74-ec3047824048"
   },
   "source": [
    "Note that this is a significant amount of noise and the data looks\n",
    "fairly different. Yet still, on average the data shares the same general\n",
    "characteristics and so the mean squared error difference is still a good\n",
    "measure. However, the descent ultimately converges not quite to the\n",
    "correct parameter values but nearby.\n",
    "\n",
    "Let’s try with even more noise:\n",
    "\n",
    "----\n",
    "\n",
    "#### Exercise 3\n",
    "What is the behavior of the iteration as more and more noise is added? Make a plot of the data with a larger amount of noise that demonstrates the described behavior. Include an animation like those above of the iteration.\n",
    "\n",
    "*Solution:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239a2f8",
   "metadata": {
    "id": "0239a2f8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a989702-4d6b-4103-b63a-9b08b2410069",
   "metadata": {
    "id": "4a989702-4d6b-4103-b63a-9b08b2410069"
   },
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1c68e-6de5-48f7-8f5e-96dc7a3694eb",
   "metadata": {
    "id": "51e1c68e-6de5-48f7-8f5e-96dc7a3694eb"
   },
   "source": [
    "## Improving the gradient descent\n",
    "\n",
    "In each of the previous subsections, we observed slow, erratic, and incorrect convergence of the parameters as they navigate the loss space during the gradient descent. Although we explored several ways to carefully select descent parameters to ensure reasonable convergence, we can also consider an improved descent procedure. There are a wide variety of popular choices for gradient descent methods, but they all generally revolve around two core ideas:\n",
    "\n",
    "1. Momentum\n",
    "2. Adaptive step sizes `alpha`\n",
    "\n",
    "Each of these tries to overcome issues such as slow convergence or settling into incorrect locations.\n",
    "\n",
    "### With momentum\n",
    "The first approach is to incorporate momentum. This is most commonly applied if there are several minimums but we are trying to arrive at the lowest of them all. Momentum would allow the iteration to enter a minimum but have its momentum carry it out and on to another, possibly lower minimum.\n",
    "\n",
    "Though our loss function is guaranteed to have only one minimum, it could help with our convergence.\n",
    "\n",
    "To incorporate momentum, we can keep a running measure of the previous gradients and combine them with our current gradient.\n",
    "Thus, if the previous gradients were big, even if our gradient is small, we should keep moving.\n",
    "\n",
    "This can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "R_{n+1} \\\\ L_{n+1}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "R_{n} - \\alpha \\gamma_{n} \\\\\n",
    "L_{n} - \\alpha \\delta_{n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\gamma_{n+1} = \\beta\\gamma_{n} + \\frac{d\\mathcal{L}}{dR}(R_n) \\\\\n",
    "\\delta_{n+1} = \\beta\\delta_{n} + \\frac{d\\mathcal{L}}{dL}(L_n)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Notice that this does require a new parameter $\\beta$ to determine how much importance to give to previous gradients.\n",
    "\n",
    "We can adjust the previous `gradient_descent` function to easily incorporate this as a new function `gradient_descent_with_momentum` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621641de-d2fb-44ee-a33c-101e84246040",
   "metadata": {
    "id": "621641de-d2fb-44ee-a33c-101e84246040"
   },
   "outputs": [],
   "source": [
    "def gradient_descent_with_momentum(p0, df, alpha=0.1, beta=0.3, max_iter=100):\n",
    "    pstar = p0\n",
    "    all_pstars = []\n",
    "    sum_grad = 0\n",
    "    for n in range(max_iter):\n",
    "        sum_grad = beta*sum_grad + df(pstar)\n",
    "        pstar = pstar - alpha*sum_grad\n",
    "        all_pstars.append(pstar)\n",
    "    return pstar, np.array(all_pstars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3468dfe4-1702-4d60-846a-1cba9ebaf55f",
   "metadata": {
    "id": "3468dfe4-1702-4d60-846a-1cba9ebaf55f"
   },
   "source": [
    "We can use it just like we used the `gradient_descent` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d096fda-7f16-43af-89c1-ef655ece6fb7",
   "metadata": {
    "id": "8d096fda-7f16-43af-89c1-ef655ece6fb7"
   },
   "outputs": [],
   "source": [
    "RL_0 = np.array([8, 5])\n",
    "data = approx_solution.y[1,:]\n",
    "wrapped_dloss = lambda RL: dloss(data,RL[0],RL[1],1e-4)\n",
    "best_RL, descent_RLs = gradient_descent_with_momentum(RL_0, wrapped_dloss, 3e-3, .7, 100)\n",
    "\n",
    "plot_RL_descent(descent_RLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba9318-9365-4919-b7e3-bd4df716bfaa",
   "metadata": {
    "id": "8aba9318-9365-4919-b7e3-bd4df716bfaa"
   },
   "source": [
    "Notice how the momentum propels the iteration to rock back and forth like a ball falling down a slope. The momentum also helps carry the values through the flat spots of the loss landscape to land right on the correct value!\n",
    "\n",
    "### Adaptive step size\n",
    "The second common alteration to gradient descent is to adjust the step size `alpha` during the iteration.\n",
    "One such method is called the **adagrad** method (which is short for *adaptive gradient*).\n",
    "This simple adjustment to gradient descent keeps a running measure of the size of the gradients $\\frac{d\\mathcal{L}}{dR}$ and $\\frac{d\\mathcal{L}}{dL}$ and slowly decays the size of `alpha` using it.\n",
    "It can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "R_{n+1} \\\\ L_{n+1}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "R_{n} - \\frac{\\alpha}{\\sqrt{\\gamma_n}} \\frac{d\\mathcal{L}}{dR}(R_n) \\\\\n",
    "L_{n} - \\frac{\\alpha}{\\sqrt{\\delta_n}} \\frac{d\\mathcal{L}}{dL}(L_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\gamma_{n+1} = \\gamma_{n} + \\frac{d\\mathcal{L}}{dR}(R_n)^2 \\\\\n",
    "\\delta_{n+1} = \\delta_{n} + \\frac{d\\mathcal{L}}{dL}(L_n)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can adjust the previous `gradient_descent` function to easily incorporate this as a new function `adagrad` as follows:\n",
    "\n",
    "----\n",
    "#### Exercise 4\n",
    "Write a function `adagrad(p0, df, alpha=0.1, max_iter=100)` that implements the adagrad method. Use it to find the optimal $R$ and $L$ values as is done above and make an animation to demonstrate the solution. How large can `alpha` be in this new method?\n",
    "\n",
    "*Solution:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0702c-d117-445b-ab35-7eb8d9f8a683",
   "metadata": {
    "id": "15e0702c-d117-445b-ab35-7eb8d9f8a683"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4602280d-0656-4e89-86e5-0929e46a4f84",
   "metadata": {
    "id": "4602280d-0656-4e89-86e5-0929e46a4f84"
   },
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XPLahNd5jwav",
   "metadata": {
    "id": "XPLahNd5jwav"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
