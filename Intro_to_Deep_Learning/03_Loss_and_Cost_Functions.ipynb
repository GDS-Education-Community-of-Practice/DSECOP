{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68b637f3",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/Intro_to_Deep_Learning/03_Loss_and_Cost_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Lecture III: Loss and Cost Functions\n",
    "\n",
    "As it was said in the previous lecture, in a standard neural network, we use a training set to train our model. For example, in our case, we know the probability of being hot Jupiter is 1 for some exoplanets and is 0 for some of them. Hence, we will use this knowledge to measure how good our model is in predicting the probability of being a hot-Jupiter; this is the definition of **Loss Function** or **Error Function**. In other words, the loss function indicates how well those parameters accomplish the network's task. We define a loss function so that the best parameters of our model correspond to the minimum of the loss function. In other words, we convert the learning problem into an optimization problem, define a loss function, and then optimize the algorithm to minimize the loss function.\n",
    "\n",
    "There are several ways to define a loss function mathematically; the most common loss functions are **Mean Squared error**, **Mean Absolute Error**, **Log-Likelihood Loss**, **Hinge Loss**, and **Huber Loss**. \n",
    "Here we use **Log-Likelihood Loss**, which is relatively simple and commonly used in classification problems. The log-likelihood loss function is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "L(a, y^i) = - y^i log(a) - (1 - y^i) log(1 - a) \\tag{1}\n",
    "\\label{eq:loss}\n",
    "\\end{equation}\n",
    "\n",
    "where $a$ is the **predicted output** of the network and $y^i$ is the **known output** of the $i^{th}$-element in the training set. Let's say we know that the $i^{th}$-element in the training set is the hot Jupiter, hence $y^i = 1$. In this case, for eq.\\ref{eq:loss} we have,\n",
    "\n",
    "\\begin{equation}\n",
    "L(a, 1) = - log(a) \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "We want to minimize the loss function, which means that $y$ should be large. But we know that the sigmoid function's maximum value of $z$ is 1. So, if we want to minimize the Loss function, $z$ should approach 1. \n",
    "\n",
    "On the other hand, let's say the $j^{th}$-element in the training set is NOT the hot Jupiter, hence $y^i = 0$. In this case, for eq.\\ref{eq:loss} we have,\n",
    "\n",
    "\\begin{equation}\n",
    "L(a, 0) = - (1 - 0) log(1 - a) \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "In this case, minimizing the loss functions leads to $z$ should approach 0. So, it seems it works!\n",
    "\n",
    "Based on the loss function, we can define another function which is called **Cost Function**. By definition, a cost function is the average of the loss-value over the whole elements in the training set, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "J( \\omega, b) = \\frac{1}{m} \\Sigma_{i = 1}^{m} L(a, y^i) \\tag{4}\n",
    "\\label{eq:Gcost}\n",
    "\\end{equation}\n",
    "\n",
    "Using eq.\\ref{eq:loss} into eq.\\ref{eq:Gcost}, we have\n",
    "\n",
    "\\begin{equation}\n",
    "J( \\omega, b) = \\frac{1}{m} \\Sigma_{i = 1}^{m} [- y^i log(a) - (1 - y^i) log(1 - a)] \\tag{5}\n",
    "\\label{eq:cost}\n",
    "\\end{equation}\n",
    "\n",
    "We minimize the cost function (i.e., the average loss-values over the training set) to train our network to find the best model parameter values ($\\omega$ and $b$). But how we do that, we use the method of **Gradient Descent**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278498f7",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "A necessary and sufficient condition for a function $f(x)$ to be convex on a interval is that the second derivative $\\frac{d^2f}{dx^2} >= 0$ for all x in the interval. We can show a local minimum of a convex function is also a global minimum. In a neural network we try to minimize the cost function. Does the cost function has to be convex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477e42c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
