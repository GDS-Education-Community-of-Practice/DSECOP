{"cells":[{"cell_type":"markdown","id":"612e34f8","metadata":{"id":"612e34f8"},"source":["<a href=\"https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/Intro_to_Deep_Learning/04_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","\n","# Lecture IV: Gradient Descent\n","\n","As we discussed in the previous lecture, we convert the learning problem into an optimization problem: Try to minimize the loss/cost function.\n","\n","Finding the minimum of a function frequently requires the knowledge of its derivative, which we will discuss in this lecture.\n","\n","**Gradient descent**, one of the most well-known minimization algorithms, is an iterative first-order optimization algorithm used to find a local minimum/maximum of a given function.\n","\n","In our problem, we are interested in inferring the best values of the model parameters: $\\omega$ and $b$, corresponding to the cost function minimum. Mathematically speaking, the gradient descent method takes the following steps to find the minimum:\n","Initialize $\\omega$ and $b$ (by guessing or randomly),\n","Find the slope of the cost function at that point,\n","Change the model parameters ($\\omega$ and $b$) along the path of the steepest decent at the current location in the function to find the next set of parameter values.\n","Repeat steps 1-3 until the parameter values do not change (significantly), implying that we have reached the minimum of the cost function where the slope is effectively zero.\n","By definition, we change the parameter values at each iteration of the algorithm such that\n","\n","\\begin{equation}\n","\\omega := \\omega - \\alpha \\frac{dJ(\\omega, b)}{d\\omega}, \\tag{1}\n","\\label{eq:changingOmega}\n","\\end{equation}\n","\\begin{equation}\n","b := b - \\alpha \\frac{dJ(\\omega, b)}{db}, \\tag{2}\n","\\label{eq:changingb}\n","\\end{equation}\n","\n","where $\\alpha$ is **learning scale**.\n","Let's see this method in action. Suppose we have two inputs $x_1$ and $x_2$. Using the logistic regression method, we need to define three parameters, $\\omega_1$, $\\omega_2$, and $b$ such that\n","\n","\\begin{equation}\n","z = \\omega_1 x_1 + \\omega_2 x_2 + b, \\tag{3}\n","\\end{equation}\n","\n","and then the output is going to be\n","\n","\\begin{equation}\n","a = \\sigma(z) . \\tag{4}\n","\\end{equation}\n","\n","Now let's define the loss function for the $i_{th}$-element in our training set:\n","\n","\\begin{equation}\n","L(a, y^i) = - y^i log(a) - (1 - y^i) log(1 - a) . \\tag{5}\n","\\label{eq:loss_1node}\n","\\end{equation}\n","\n","We want to find the slope of the loss function at the point ($\\omega_1$, $\\omega_2$, and $b$), and then based on the value of the slope at that point, modify them ($\\omega_1$, $\\omega_2$, and $b$) by using eq.\\ref{eq:changingOmega} and eq.\\ref{eq:changingb}, to reduce the loss function and rest at its minimum.\n","\n","How can we find the derivative of the loss function with respect to $\\omega_1$, $\\omega_2$, and $b$? The most popular answer is the **backpropagation method**:\n","find the derivative of the loss function with respect to the $z$, then\n","with respect to $y$, and at the last step,\n","with respect to $\\omega_1$, $\\omega_2$, and $b$.\n","![GD1-2.jpg](attachment:GD1-2.jpg)\n","\n","After calculating the derivative of the loss function with respect to $\\omega_1$, $\\omega_2$, and b, we then modify those using eq.\\ref{eq:changingOmega} and eq.\\ref{eq:changingb}. Then, we repeat the procedure: calculating $z$, $a$, and the loss function using modified $\\omega_1$, $\\omega_2$, and b. Then again, we find the derivative and modify them again based on the slope of the loss function."]},{"cell_type":"markdown","id":"ff9cb931","metadata":{"id":"ff9cb931"},"source":["# Homework\n","In programing, the term $\\frac{dL}{dq}$ is also denoted by $dq$, where $q$ is a paramater, such as $z$ or $\\omega_i$. Calculate $da$, $dz$, $d\\omega_1$, $d\\omega_2$, and $db$ using eq. \\ref{eq:loss_1node} for loss function and considering sigmoid function."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}