{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce4650c",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/Intro_to_Deep_Learning/11_Homework_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Lecture XI: Homework Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa3f0d",
   "metadata": {},
   "source": [
    "## Lecture I:\n",
    "\n",
    "Perceptron is a neural network with a single neuron. For more information of history of neural network see https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55177bd1",
   "metadata": {},
   "source": [
    "## Lecture II:\n",
    "\n",
    "rank ($X$) $ = (n, m)$\n",
    "\n",
    "rank ($X^{i}$) $ = (n, 1)$\n",
    "\n",
    "rank ($\\omega$) $ = (n, 1)$\n",
    "\n",
    "rank ($b$) $ = (1, 1)$\n",
    "\n",
    "rank ($z$) $ = (1, m)$\n",
    "\n",
    "rank ($a$) $ = (1, m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff89769",
   "metadata": {},
   "source": [
    "## Lecture III:\n",
    "\n",
    "No, the cost function doesn't need to be convex. Most of the cost functions used in the neural network are not convex. But it is not necessary because we want to find *a minimum* of the cost function; it could be a *global* or a *local* minimum.  In the case of the convex-cost function, the minimum that you find is guaranteed to be a global minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba540f82",
   "metadata": {},
   "source": [
    "## Lecture IV:\n",
    "\n",
    "\\begin{equation}\n",
    "da := \\frac{dL}{da} = - \\frac{y^{i}}{a} + \\frac{1- y^{i}}{1 - a}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "dz := \\frac{dL}{dz} = \\frac{dL}{da} * \\frac{da}{dz} = [- \\frac{y^{i}}{a} + \\frac{1- y^{i}}{1 - 1}][a (1 -a )]= a - y^{i}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "d\\omega_1 := \\frac{dL}{d\\omega_1} = \\frac{dL}{da} * \\frac{da}{dz}* \\frac{dz}{d\\omega_1} = (a - y^{i}) x_1^{i}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "d\\omega_2 := \\frac{dL}{d\\omega_2} = \\frac{dL}{da} * \\frac{da}{dz}* \\frac{dz}{d\\omega_2} = (a - y^{i}) x_2^{i}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "db := \\frac{dL}{db} = \\frac{dL}{da} * \\frac{da}{dz}* \\frac{dz}{db} = (a - y^{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086417f",
   "metadata": {},
   "source": [
    "## Lecture V:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e7d3a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "  \n",
    "    return(1 / (1 + math.exp(-x))) \n",
    "\n",
    "\n",
    "def gradient_decent_one_node_one_layer(X_t, Y, n_iteration, learnig_rate):\n",
    "    \n",
    "    alpha = learning_rate                  \n",
    "    \n",
    "    for k in range(n_iteration):\n",
    "        \n",
    "        z = 0; db = 0 \n",
    "        d_omega = np.zeros((1, n)) \n",
    "        \n",
    "                                         \n",
    "        n = X_t.shape[0] \n",
    "          \n",
    "        \n",
    "        omega = np.random.rand((1, n)) * np.sqrt(1/n)        \n",
    "        b = np.random.rand((1, 1))\n",
    "\n",
    "                                 \n",
    "        \n",
    "        Z = np.dot(omega , X_t) + b\n",
    "        A = sigmoid(Z)\n",
    "        dZ = A - Y\n",
    "        \n",
    "        J = -  Y * np.log(A) - (1 - Y) * np.log(1 - A) \n",
    "        cost = np.sum(J, axis=1)/m\n",
    "        d_omega = (1/m) * np.dot(dZ, X_t.T)\n",
    "        db = np.sum(dZ, axis=1)/m\n",
    "        \n",
    "        omega -= alpha * d_omega\n",
    "        b -= alpha * db\n",
    "        \n",
    "    \n",
    "    return(omega, b)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c06b7c1",
   "metadata": {},
   "source": [
    "## Lecture VI:\n",
    "\n",
    "rank ($\\omega^{[l]}_j$) $= (1, n_{l-1})$\n",
    "\n",
    "rank ($b^{[l]}_j$) $= (1, 1)$\n",
    "\n",
    "rank ($z^{[l]}_j$) $= (1, 1)$\n",
    "\n",
    "rank ($a^{[l]}_j$) $= (1, 1)$\n",
    "\n",
    "\n",
    "rank ($\\omega^{[l]}$) $= (n_{l}, n_{l-1})$\n",
    "\n",
    "rank ($b^{[l]}$) $= (n_{l}, 1)$\n",
    "\n",
    "rank ($Z^{[l]}$) $= (n_{l}, 1)$\n",
    "\n",
    "rank ($A^{[l]}$) $= (n_{l}, 1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280b2c3",
   "metadata": {},
   "source": [
    "## Lecture VII:\n",
    "\n",
    "1. learning rate ($\\alpha$)\n",
    "2. number of iterations\n",
    "3. activation function\n",
    "4. number of hidden layers\n",
    "5. number of nodes in each hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d121707",
   "metadata": {},
   "source": [
    "## Lecture VIII: \n",
    "\n",
    "Another way to regularize the cost function is to use both **'$L_1$-Regularization'** and **'$L_2$-Regularization'** simultaneously. This method is called **Elastic net**. The extra term in cost function in this method is \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\lambda_1}{2m} |\\omega|^2 + \\frac{\\lambda_2}{2m} |\\omega| \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f4c198",
   "metadata": {},
   "source": [
    "## Lecture IX: \n",
    "\n",
    "An example of a linear activation function is $A(Z) = Z$, which effectively means there is no activation function! So, in this case, we have\n",
    "\n",
    "\\begin{equation}\n",
    "A^{[l = 1]} = Z^{[l = 1]} = \\omega^{[l = 1]}X_0 + b^{[l = 1]}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "A^{[l = 2]} = Z^{[l = 2]} = \\omega^{[l = 2]}[\\omega^{[l = 1]}X_0 + b^{[l = 1]}] + b^{[l = 2]} \n",
    "\\end{equation}\n",
    "\n",
    "and so on. Therefore, for the last layer, we can write,\n",
    "\n",
    "\\begin{equation}\n",
    "A^{[L]} = Z^{[L]} = \\omega^{[L]}\\omega^{[L-1]}\\omega^{[L-2]}...\\omega^{[1]}X_0 + F(b^{[l]}, \\omega^{[l]})\n",
    "\\end{equation}\n",
    "\n",
    "Technically, we can define a *new* $\\omega$ and a *new* $b$ as \n",
    "\n",
    "\\begin{equation}\n",
    "\\omega_{new} = \\omega^{[L]}\\omega^{[L-1]}\\omega^{[L-2]}...\\omega^{[1]}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "b_{new} = F(b^{[l]}, \\omega^{[l]})\n",
    "\\end{equation}\n",
    "\n",
    "Using these new parameters, we can rewrite the output as \n",
    "\n",
    "\\begin{equation}\n",
    "A^{[L]} = Z^{[L]} = \\omega_{new}X_0 + b_{new}\n",
    "\\end{equation}\n",
    "\n",
    "Hence, effectively it is like a neural network with just one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f160ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
