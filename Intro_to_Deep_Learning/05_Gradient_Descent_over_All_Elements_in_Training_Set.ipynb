{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad32c53",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/Intro_to_Deep_Learning/05_Gradient_Descent_over_All_Elements_in_Training_Set.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Lecture V: Gradient Descent over All Elements in Training Set\n",
    "\n",
    "So far we have just considered one element in the training set, $(X^i, y^i)$, where $X^i = (x^{i}_{1}, x^{i}_{2})$. Instead of the loss function, we use the cost function to go over all $m$ elements in the training set. For example, to modify the parameter $\\omega_1$, we take a derivative of the loss function with respect to $\\omega_1^i$ and take an average of all its values over the training set, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\omega_1} J(\\omega, b) = \\frac{1}{m} \\Sigma_{i = 1} ^{m} \\frac{\\partial}{\\partial \\omega^i_1} L(a^i, y^i) \\tag{1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341c233",
   "metadata": {},
   "source": [
    "# miniProject\n",
    "\n",
    "One of the crucial rules in programing for Neural networks is to avoid **for-loops**. Because of the obvious reason, that makes the program runtime much longer compared to the runtime of a **vectorized code**. The following code snippet implements the gradient descent method for the training data set. Vectorize this code snippet as much as possible (by converting the for-loops to array-like operations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c51734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "  \n",
    "    return(1 / (1 + math.exp(-x))) \n",
    "\n",
    "def sigmoid_prim(x):\n",
    "    a = (np.exp(-z))/(1 + np.exp(-z))**2\n",
    "    return a\n",
    "\n",
    "\n",
    "def gradient_decent_one_node_one_layer(X_t, Y, n_iteration, learnig_rate):\n",
    "    \n",
    "    alpha = learning_rate                  # learning scale\n",
    "    \n",
    "    for k in range(n_iteration):\n",
    "        \n",
    "        z = 0; db = 0 \n",
    "        d_omega = np.zeros((1, n)) \n",
    "        \n",
    "                                         \n",
    "        n = X_t.shape[0]   # number of dimension of the input X_t^(1) = [x_1^(1), x_2^(1)], {X_t.shape = = (n, m)}                        #X = [x_1, x_2]\n",
    "        \n",
    "        omega = np.random.rand((1, n)) * np.sqrt(1/n)        # initial values for [omega_1, omega_2]\n",
    "        b = np.random.rand()\n",
    "\n",
    "        m = len(Y)                           # number of data in the tarining set, or m = X_t.shape[1] {length of [X_t^(1), ..., X_t^(m)]} or [y_1, ..., y_m]\n",
    "\n",
    "\n",
    "        for i in range(1, m, +1):\n",
    "            z = np.dot(omega , X_t[i]) + b                 # z = \\omega^T X + b\n",
    "            a = sigmoid(z)\n",
    "\n",
    "            J += - Y[i] * np.log(a) - (1 - Y[i]) * np.log(1 - a)\n",
    "            dz = a - Y[i]\n",
    "\n",
    "            for j in range(n):\n",
    "                d_omega[j] += X_t[i][j] * dz\n",
    "\n",
    "            db += dz \n",
    "\n",
    "        J = J / m\n",
    "        for j in range(n):\n",
    "            d_omeg[j] = d_omeg_[j] / m\n",
    "            omega[j] = omega[j] - alpha * d_omega[j]\n",
    "\n",
    "        db =  db / m\n",
    "        b = b - alpha * db\n",
    "\n",
    "    return(omega, b)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1925fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
