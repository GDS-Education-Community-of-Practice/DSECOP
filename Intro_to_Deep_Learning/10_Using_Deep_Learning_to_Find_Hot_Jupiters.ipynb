{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2463c680",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/Intro_to_Deep_Learning/10_Using_Deep_Learning_to_Find_Hot_Jupiters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Lecture X: Using Deep Learning to Find Hot-Jupiters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff36b82",
   "metadata": {},
   "source": [
    "Now, we have already learned the basic concepts of deep learning. So, it's time to use our knowledge in a real problem. Let's think about the question in the second lecture when **we want to know which detected exoplanet is hot-Jupiter**. \n",
    "\n",
    "**Step I**: The first step is to find a *training set*, i.e., a set of data that we know if they are hot-Jupiter or not. I make that training set for you; you can download the data of more than 3,000 exoplanets from this module. This data collected from [here](https://exoplanetarchive.ipac.caltech.edu/docs/data.html). We also need a test set to examine our neural network and find its efficiency. A rule of thumb here is to divide your data as $98%$ for the training set and $2%$ for the *dev/test* set if you have a huge number of data. Here, the first step of your project is to divide the data into two sets with a ratio of $6 :4$ (training/test).\n",
    "\n",
    "**Step II**: Now, let's think about how many parameters we should consider in our neural network. In the data given to you, some parameters may not help identify the type of exoplanets. The best thing is to consider all the parameters and let the neural network decide which ones are more important. But for simplicity, we use our prior knowledge to rule out some of the parameters. Thus, the second step in your project is to read the data and collect these parameters:\n",
    "\n",
    "1. Orbital Period, \n",
    "2. Transit Duration, \n",
    "3. Planetary Radius, \n",
    "4. Eqillibruim Temperature, \n",
    "5. Effective Stellar Temperature, \n",
    "6. Stellar Surface Gravity, \n",
    "7. Stellar Radius.\n",
    "\n",
    "You also need to collect the data from\n",
    "\n",
    "8. Type of Exoplanet (1 for hot-Jupiter, 0 for the other types).\n",
    "\n",
    "**Step III**: We want to use the logistic regression model in our neural network. Then we will use the Gradient descent method to let the neural network learn the weight ($\\omega$ and $b$) of each input. To do so, we have to set the hyperparameters such as *learning rate*, *activation function*, and *number of hidden layers*. Therefore, set these hyperparameters as:\n",
    "1. rating rate: $\\alpha = 0.03$, \n",
    "2. activation function: $g(z) = tanh(z)$ and for the last layer: $g(z) = \\sigma(z)$, \n",
    "3. number of hidden layers: 3, \n",
    "4. the number of nodes in hidden layers [4, 3, 1],\n",
    "5. the number of iterations: 5000.\n",
    "\n",
    "Plot the diagram of your neural network.\n",
    "\n",
    "**Step IV**: Define the loss and cost function using [eq.1]( Lecture 3 Loss and Cost Functions.ipynb ) and [eq.5](Lecture 3 Loss and Cost Functions.ipynb) . Grab a cup of coffee and generalize your code for the `miniProject` in Lecture V to implement the gradient descent method for this neural network. (This is a difficult one, so take your time and write your code neatly.)\n",
    "\n",
    "\n",
    "**Step V**:  Run your code over all nodes and layers. Then, the result of your code should be the best $\\omega$ and $b$ for each node.\n",
    "\n",
    "**Step VI**: Knowing the best $\\omega$ and $b$ for each node, you can use the *learned model parameters* to test your neural network on your *test set*.\n",
    "\n",
    "**Step VII**: For what percentage of the data in the test set does your neural network predict the type of exoplanet correctly? Rerun it to see if it converges or not (use a for-loop to repeat the procedure, and see over a hundred times how many times you get $> 80\\%$.)\n",
    "\n",
    "**Step VIII**: If the efficiency is higher than $90\\%$, congratulation! You did an excellent job! Celebrate it by pouring yourself *a mega pint of wine (or coffee)!*\n",
    "\n",
    "**Step IX**: If the efficiency is sometimes less than $80\\%$, the neural network does not predict well. Thus, we should modify it. You can modify it by adding more layers/nodes, or changing the other hyperparameters such as learning rate/number of iterations.\n",
    "\n",
    "**Step X**: You already know how deep learning works and how to write code for building a neural network at this step. However, there are a lot of deep learning packages in python. You can shorten and speed up your code using them. So, this time try to use any of the software packages you may know and rewrite your code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
