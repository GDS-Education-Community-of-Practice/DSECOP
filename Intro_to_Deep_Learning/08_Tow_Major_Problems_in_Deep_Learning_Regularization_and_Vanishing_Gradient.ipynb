{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03fe9c7b",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/Intro_to_Deep_Learning/08_Tow_Major_Problems_in_Deep_Learning_Regularization_and_Vanishing_Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Lecture VIII: Two Major Problems in Deep Learning-Regularization and Vanishing Gradient  \n",
    "\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Sometimes our devised model is too complex for the simple dataset at hand. This can be highly misleading in practice because the model tends to yield extremely acurrate results on the training set, while performs poorly on any dataset not used in training. In such cases, the neural network is **overfitting** the training data. \n",
    "\n",
    "A common measure of model complexity is the number of parameters it has relative to the size of data. Overfitting frequently occurs when the model has too many parameters.\n",
    "\n",
    "One way to address the overfitting (or high variance) problem is to suppress (or **regularize**) some or all of the parameters of the model, such that they can only take certain values or ranges of values.\n",
    "\n",
    "This is done by adding one additional term in the definition of the cost function. The additional term controls the excessively fluctuating function such that the coefficients (parameters) cannot take extreme values. For example, consider the cost function in the logistic regression method in the neural network with one hidden layer:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\omega, b) = \\frac{1}{m} \\Sigma_{i=1}^{m} L(a, y^i) \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "To avoid high variance, we re-define the cost function by adding a new term such that\n",
    "\\begin{equation}\n",
    "J(\\omega, b) = \\frac{1}{m} \\Sigma_{i=1}^{m} L(a, y^i) + \\frac{\\lambda}{2m} |\\omega|^2 . \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "This extra term called **'$L_2$-Regularization'**, and $\\lambda$ called **Regularization parameter** which is a hyperparameter and should be given to the neural network. In the $L_2$-Regularization, the term $|\\omega|^2$ is simply the *Euclidean norm*:\n",
    "\\begin{equation}\n",
    "|\\omega|^2 = \\Sigma_{j = 1}^{n_x} \\omega_j^2 \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "where $n_x$ is the number of nodes in the hidden layer. In general, if you have more than one hidden layer, the newly-defined  cost function will be\n",
    "\\begin{equation}\n",
    "J(\\omega^{[1]}, b^{[1]}, ..., \\omega^{[L]}, b^{[L]}) = \\frac{1}{m}\\Sigma_{i=1}^{m} L(a, y^i) + \\frac{\\lambda}{2m}\\Sigma_{l = 1}^{L} |\\omega^{[l]}|^2 , \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "where $L$ is the number of layers and $|\\omega^{[l]}|^2$ is called **Frobenius norm**:\n",
    "\\begin{equation}\n",
    "|\\omega^{[l]}|^2 = \\Sigma_{i=1}^{n^{[l-1]}} \\Sigma_{j=1}^{n^{[l]}} (\\omega_{ij}^{[l]})^2 . \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "There are also other ways to regularize the cost fucntion. For example, another less-common way is by using **'$L_1$-Regularization'** where the extra term added to the cost function is $\\frac{\\lambda}{2m} |\\omega|$, where $|\\omega| = \\Sigma_{j = 1}^{n_x} \\omega_j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e37c3f9",
   "metadata": {},
   "source": [
    "## Vanishing Gradient\n",
    "\n",
    "In a very deep neural network, sometimes the gradient is so tiny (or extremely large) that it makes network hard to train. This problem is related to multiple activation functions used in deep neural networks; activation functions map a large input space into a small input space (for example, $tanh(z)$ maps to $[-1, 1]$). Therefore, a significant change in the input of the activation function will cause a slight change in the output. Hence, the derivative becomes small.\n",
    "\n",
    "This is not a significant problem if the neural network has only a few layers. However, adding more layers can be a significant barrier to training the neural network.\n",
    "\n",
    "One way to avoid the vanishing gradient problem is initializing model parameters by multiplying them with the square root of their variance. Let's define the variance of $\\omega$ is\n",
    "\n",
    "\\begin{equation}\n",
    "var(\\omega^{[l]}) = \\frac{1}{n^{[l-1]}} . \\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, the random initial value for $\\omega$ would be defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\omega^{[l]} = random(\\omega^{[l]}) * \\sqrt{\\frac{1}{n^{[l - 1]}}} . \\tag{7}\n",
    "\\end{equation}\n",
    "\n",
    "If you use the ReLU activation function, it is shown that the variance should be defiened as\n",
    "\\begin{equation}\n",
    "var(\\omega^{[l]}) = \\frac{2}{n^{[l-1]}} . \\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore for ReLU actiovation function the intitial value of $\\omega$ is better to be defined as \n",
    "\\begin{equation}\n",
    "\\omega^{[l]} = random(\\omega^{[l]}) * \\sqrt{\\frac{2}{n^{[l - 1]}}} . \\tag{9}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a54a2",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Describe other popular regularization terms commonly used in Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296443ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
