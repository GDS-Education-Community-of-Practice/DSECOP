{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d181f0a8",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/Intro_to_Deep_Learning/12_Using_Deep_Learning_to_Find_Hot_Jupiters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Lecture XII: Using Deep Learning to Find Hot-Jupiters\n",
    "## Lecture X-Solution\n",
    "\n",
    "The first step is to read and clean data. Here, I use pandas to read data from the file 'NASAExoplanetsData.csv' . However, we do not need all the columns. As mentioned in Lecture X, we only want 7 columns of data corresponded to 7 quantities. But before that, it is essential to normalize data before using a neural network, so in Lines 25-28 I normalized all the inputs except the type of the exoplanet, which has to be 0 or 1 ('label' column) and no need to be normalized. In the next step, I have to divide data into two sets: a training set and a test set with a ratio of 6:4 (Line 32-33). Then, make two matrices (X, Y) for each set as inputs of our neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90287033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def readData():\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    df = pd.read_csv('NASAExoplanetsData.csv')#, encoding = 'utf-8')\n",
    "    \n",
    "    # read file: NASAExoplanet_header.txt to understand the header of the data\n",
    "    f = open('NASAExoplanet_header.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "\n",
    "    header = {}\n",
    "    for i, l in enumerate(lines):\n",
    "        parts = l.split('\\t')\n",
    "        header[parts[2].rstrip('\\n')] = parts[1]\n",
    "\n",
    "    # define the paramters that we need, use the file: NASAExoplanet_header to find which columns of data you need\n",
    "    parameters = ['Orbital Period [days]', 'Planetary Radius [Earth radii]', \n",
    "                  'Equilibrium Temperature [K]', \n",
    "                  'Stellar Surface Gravity [log10(cm/s**2)]', \n",
    "                  'Stellar Effective Temperature [K]','label', \n",
    "                  'Transit Duration [hrs]', 'Stellar Radius [Solar radii]']\n",
    "\n",
    "    # store needed data\n",
    "    data = pd.DataFrame()\n",
    "    for p in parameters:\n",
    "        \n",
    "        if p != 'label':\n",
    "       \n",
    "            data[p]  = (df[header[p]] - df[header[p]].mean()) / (df[header[p]].max() - df[header[p]].min()) #normalizing data\n",
    "        else:\n",
    "            data[p] = df[header[p]]\n",
    "        \n",
    "    # randomly split data in two sets with ratio 6:4  \n",
    "    trainingSet = data.sample(frac = 0.6) \n",
    "    testSet = data.drop(trainingSet.index)\n",
    "    \n",
    "    # make the matrices X and Y for the trainig set\n",
    "    dataX = trainingSet.loc[:, trainingSet.columns != 'label'] \n",
    "    dataY = trainingSet['label']\n",
    "    \n",
    "    X = np.atleast_2d(dataX.to_numpy()).T\n",
    "    Y = np.atleast_2d(dataY.to_numpy())\n",
    "    \n",
    "    # make the matirces X and Y for the test set\n",
    "    dataX_test = testSet.loc[:, testSet.columns != 'label']\n",
    "    dataY_test = testSet['label']\n",
    "    \n",
    "    X_test = np.atleast_2d(dataX_test.to_numpy()).T\n",
    "    Y_test = np.atleast_2d(dataY_test.to_numpy())\n",
    "    \n",
    "    \n",
    "    return X, Y, X_test, Y_test\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = readData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e50bc",
   "metadata": {},
   "source": [
    "Now, it's time to build our neural network. As mentioned in Lecture X, we want a neural network with 2 hidden layers with three nodes in each hidden layer. Let's make our neural network in general form, using $L$ hidden layers and a list of the number of nodes in each layer like \\[n_0, n_1, ..., n_L\\]. Note that n_0 is the number of parameters which in our case is 7 and n_L has to be 1 (it's a binary classification problem). We also want to use $tanh(z)$ as an activation function for all hidden layers except the last layer in which the sigmoid function has to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c49886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def activation_sigmoid(z):\n",
    "    return 1 /(1 + np.exp(-z))\n",
    "\n",
    "def activation_sigmoid_der(z): # derivative of the sigmoid function which is going to be used in backpropagation (in gradient descent method)\n",
    "    a = (np.exp(-z))/(1 + np.exp(-z))**2\n",
    "    return a\n",
    "\n",
    "def activation_tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def activation_tanh_der(z):   # derivative of the tanh(z) which is going to be used in backpropagation (in gradient descent method)\n",
    "    a = 1 - (np.tanh(z))**2\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101afc14",
   "metadata": {},
   "source": [
    "We use the logestic regression method in which we have\n",
    "\\begin{equation}\n",
    "Z^{[l]} = (\\omega^{[l]}) . A^{[l-1]} + b^{[l]}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "A^{[l]} = activation(Z^{[l]})\n",
    "\\end{equation}\n",
    "\n",
    "where $l$ is the number of layers. In each layer, we also need $\\frac{dA}{dZ}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377f62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logestic_regression(l, X, omega, b): \n",
    "    \n",
    "    Z = np.dot(omega, X) + b\n",
    "    if l == L: # last layer is different because of the different activation function\n",
    "        \n",
    "        A = activation_sigmoid(Z)\n",
    "        A_prim = activation_sigmoid_der(Z)\n",
    "    \n",
    "    else:\n",
    "        A = activation_tanh(Z)\n",
    "        A_prim = activation_tanh_der(Z)\n",
    "    \n",
    "    return A, A_prim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270dc83",
   "metadata": {},
   "source": [
    "Back propagation is the most challenging step in building a neural network. Based on Lectures IV and V, in each layer, we need to compute the derivative of the cost function with respect to $\\omega^{[l]}$ and $b^{[l]}$ to modify them by using eq.1 and eq.2 in Lecture IV.\n",
    "\n",
    "Starting with the last layer ($l = L$), what we want is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial \\omega^{[L]}} = \\frac{\\partial J}{\\partial A^{[L]}} \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}} \\frac{\\partial Z^{[L]}}{\\partial \\omega^{[L]}} \\tag{3}\n",
    "\\label{eq:lastlayer1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial b^{[L]}} = \\frac{\\partial J}{\\partial A^{[L]}} \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}} \\frac{\\partial Z^{[L]}}{\\partial b^{[L]}} \\tag{4}\n",
    "\\label{eq:lastlayer2}\n",
    "\\end{equation}\n",
    "\n",
    "Let's define $\\delta^{[L]}$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^{[L]} = \\frac{\\partial J}{\\partial A^{[L]}}  \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}} .\\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "Rewirtting eq.\\ref{eq:lastlayer1} and eq. \\ref{eq:lastlayer2} using $\\delta^{[L]}$ we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial \\omega^{[L]}} = \\delta^{[L]} \\frac{\\partial Z^{[L]}}{\\partial \\omega^{[L]}} \\tag{6}\n",
    "\\label{eq:lastlayer3}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial b^{[L]}} = \\delta^{[L]} \\frac{\\partial Z^{[L]}}{\\partial b^{[L]}} \\tag{7}\n",
    "\\label{eq:lastlayer4}\n",
    "\\end{equation}\n",
    "\n",
    "Using eq.5 in Lecture IV, $\\delta^{[L]}  = (A^{[L]} - Y)$.\n",
    "\n",
    "For the one layer before the last layer, i.e., $l = L - 1$, we want to calculate:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial \\omega^{[L - 1]}} = \\frac{\\partial J}{\\partial A^{[L]}} \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}} \\frac{\\partial Z^{[L]}}{\\partial A^{[L - 1]}} \\frac{\\partial A^{[L - 1]}}{\\partial Z^{[L - 1]}} \\frac{\\partial Z^{[L - 1]}}{\\partial \\omega^{[L - 1]}} \\tag{8} \n",
    "\\label{eq:l_1layer1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial b^{[L]}} = \\frac{\\partial J}{\\partial A^{[L]}} \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}} \\frac{\\partial Z^{[L]}}{\\partial A^{[L - 1]}} \\frac{\\partial A^{[L - 1]}}{\\partial Z^{[L - 1]}} \\frac{\\partial Z^{[L - 1]}}{\\partial b^{[L - 1]}}  \\tag{9}\n",
    "\\label{eq:l_1layer2}\n",
    "\\end{equation}\n",
    "\n",
    "Based on the logestic regression method we know that $\\frac{\\partial Z^{[L]}}{A^{[L-1]}} = (\\omega^{[L]})^{T}$ and $\\frac{\\partial Z^{[L - 1]}}{\\omega^{[L-1]}} = (A^{[L - 2]})^{T}$. Thus, using these equation and $\\delta^{[L]}$ in eq. \\ref{eq:l_1layer1}, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial \\omega^{[L - 1]}} = \\{[(\\omega^{[L]})^{T} . \\delta^{[L]}] \\odot \\frac{d A^{[L -1]}}{d Z^{[L - 1]}}\\} . (A^{[L - 2]})^{T} \\tag{10}\n",
    "\\label{eq:l_1layer3}\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\odot$ operator is a member-wise multiplication or **Hadamard multiplication**. \n",
    "\n",
    "Now let's define\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^{[L - 1]} = ((\\omega^{[L]})^{T} . \\delta^{[L]}) \\odot \\frac{d A^{[L -1]}}{d Z^{[L - 1]}}\\tag{11}\n",
    "\\end{equation}\n",
    "\n",
    "So for eq. \\ref{eq:l_1layer3} we have\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial \\omega^{[L - 1]}} = \\delta^{[L - 1]} . (A^{[L - 2]})^{T} \\tag{12}\n",
    "\\label{eq:l_1layer4}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Generaizing this result, for the layer $l < L$ , we have\n",
    "\\begin{equation}\n",
    "\\delta^{[l]} = [(\\omega^{[l + 1]})^{T} . \\delta^{[l + 1]}] \\odot \\frac{d A^{[l]}}{d Z^{[l]}}  \\tag{13}\n",
    "\\label{eq:delta}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial \\omega^{[l]}} = \\delta^{[l]} . (A^{[l - 1]})^{T} \\tag{14}\n",
    "\\label{eq:cost_der}\n",
    "\\end{equation}\n",
    "\n",
    "With similar approch we can find $\\frac{\\partial J}{\\partial b^{[l]}}$.\n",
    "\n",
    "\n",
    "Therefore, in the back propagation algorithm, we take these steps:\n",
    "\n",
    "1. for the last layer $l = L$, use the eq.\\ref{eq:lastlayer2} to compute $\\delta^{[L]}$,\n",
    "2. then, for all the other hidden layers, we calcualte $\\delta^{[l]}$ using eq. \\ref{eq:delta};\n",
    "3. having $\\delta^{[l]}$, we can now calcualte $\\frac{\\partial J}{\\partial \\omega^{[l]}}$ and $\\frac{\\partial J}{\\partial b^{[l]}}$;\n",
    "4. using eq. 1 and eq. 2 in Lecture IV, at the last step of this part, we compute the new *weight* and *bias* ($\\omega^{[l]}$ and $b^{[l]}$).\n",
    "\n",
    "Following the rank of matrices in this calculation can be helpful to understand better the equation presented here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed467e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(l, A_prim, w, D): \n",
    "    \n",
    "    delta = np.atleast_2d(np.dot(np.transpose(w), D) * A_prim)\n",
    "    \n",
    "    return delta\n",
    "    \n",
    "def update(omega, learning_rate, A, D, b, m):\n",
    "    a = np.atleast_2d(A)\n",
    "    d = np.atleast_2d(D)\n",
    "    ad = np.dot(d, a.T)\n",
    "    omega -= (learning_rate/m) * ad\n",
    "    \n",
    "    sigma = (np.atleast_2d(np.sum(d, axis=1))).T\n",
    "    b -= (learning_rate/m) * sigma\n",
    "    \n",
    "    return omega, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f90796",
   "metadata": {},
   "source": [
    "Having all the needed functions and data, now we can build our neural network. The first step is to initialize variables such as $\\omega^{[l]}$ and $b^{[l]}$ (Lines: 24-28). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d58f36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def NN(L, N_L, n_iteration, learning_rate, X_t, Y):\n",
    "    \n",
    "    \n",
    "    m = X_t.shape[1]\n",
    "    \n",
    "    \n",
    "    # DEFINE VARIABLES:\n",
    "    #--------------------\n",
    "    A = np.zeros((L+1), dtype=list)\n",
    "    A_prim = np.zeros((L+1),dtype=list)\n",
    "    omega = np.zeros((L+1),dtype=list)\n",
    "    b = np.zeros((L+1),dtype=list)\n",
    "    delta = np.zeros((L+1),dtype=list)\n",
    "    \n",
    "    \n",
    "    # INITIALIZIATION:\n",
    "    #--------------------\n",
    "    for l in range(1, L+1, 1):\n",
    "        \n",
    "        n_o = N_L[l]       # number of nodes\n",
    "        n_i = N_L[l- 1]    # number of inputs (or nodes of the previous layer)\n",
    "        \n",
    "        omega[l] = np.random.rand(n_o, n_i) * np.sqrt(1/n_i)\n",
    "        b[l] = np.random.rand(n_o, 1)\n",
    "        A[l] = np.zeros((n_o, m), dtype=float)\n",
    "        A_prim[l] = np.zeros((n_o, m), dtype=float)\n",
    "        delta[l] = np.zeros((n_o, m), dtype=float)\n",
    "            \n",
    "\n",
    "    for k in range(n_iteration):\n",
    "        \n",
    "        # FORWARD:\n",
    "        #--------------------\n",
    "        A[0] = X_t\n",
    "        \n",
    "        for l in range(1, L+1, +1):\n",
    "            \n",
    "            n_o = N_L[l]         # number of nodes\n",
    "            #n_i = N_L[l - 1]    # number of inputs (or nodes of the previous layer)\n",
    "            \n",
    "           \n",
    "            A[l], A_prim[l] = logestic_regression(l, A[l-1], omega[l], b[l])\n",
    "            \n",
    "        \n",
    "        \n",
    "        # BACKWARD:\n",
    "        #--------------------\n",
    "        for l in range(L, 0, -1):\n",
    "            \n",
    "            if l == L:        \n",
    "                \n",
    "                error = A[l] - Y\n",
    "                delta[l] = np.atleast_2d(error)  # Note that if you define the Loss function defirently, this term would be different\n",
    "                                                # for example, if you define Loss as \\Sigma (A - Y)^2, delta[L] would be error * A_prim[L])\n",
    "                \n",
    "            else:\n",
    "                delta[l] = back_propagation(L, A_prim[l], omega[l + 1], delta[l + 1])\n",
    "            \n",
    "            \n",
    "            \n",
    "        # UPDATE:\n",
    "        #--------------------\n",
    "        for l in range(1, L+1, 1):\n",
    "            omega[l], b[l] = update(omega[l], learning_rate, A[l-1], delta[l], b[l], m)\n",
    "            \n",
    "            \n",
    "             \n",
    "    return omega, b\n",
    "#----------------------------------------------------------------------------------------------------------------    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed008c",
   "metadata": {},
   "source": [
    "Now it's time to test the *learned* model parameters, $\\omega^{[l]}$ and $b^{[l]}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4d1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(L, omega, b, X, Y):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    A = np.zeros((L+1), dtype=list)\n",
    "    \n",
    "    \n",
    "    A[0] = X\n",
    "    for l in range(1, L+1, 1):\n",
    "\n",
    "        n_o = N_L[l]     # number of nodes\n",
    "        A[l] = np.zeros((n_o, m), dtype=float)\n",
    "\n",
    "        A[l] = logestic_regression(l, A[l-1], omega[l], b[l])[0]\n",
    "    \n",
    "    counter = 0.\n",
    "    for a, y in zip(A[L], Y):\n",
    "        counter = np.count_nonzero((np.abs(a-y)< 0.5) == True) # i.e. if A[L][i] > 0.5 consider it as 1\n",
    "                                                                #     if A[L][i] < 0.5 consider it as 0\n",
    "        \n",
    "    correctness = (counter/m)*100 # percentage of correct results\n",
    "    \n",
    "    return A[L], correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8199ef6",
   "metadata": {},
   "source": [
    "Let's run the code with these hyperparameters:\n",
    "\n",
    "- learning_rate = 0.03\n",
    "- n_iteration = 5000\n",
    "- 3 hidden layers\n",
    "- number of nodes in hidden layers: [7, 4, 3, 1] (note that 7 corresponds to the input layer (l=0))\n",
    "\n",
    "Run the program 100 times to see if the code converges with these hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872157ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "n_iteration = 5000\n",
    "L = 3\n",
    "N_L = [7, 4, 3, 1]\n",
    "\n",
    "number_of_efficieny_less_than_80 = 0.\n",
    "\n",
    "for i in range(100):\n",
    "    omega, b = NN(L, N_L, n_iteration, learning_rate, X_train, Y_train)\n",
    "    output, efficieny = predict(L, omega, b, X_test, Y_test)\n",
    "    \n",
    "    if efficieny < 80.:\n",
    "        number_of_efficieny_less_than_80 += 1\n",
    "print(number_of_efficieny_less_than_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4b6499",
   "metadata": {},
   "source": [
    "Try with\n",
    "\n",
    "- learning_rate = 0.03\n",
    "- n_iteration = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "41672546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "learnig_rate = 0.03\n",
    "n_iteration = 10000\n",
    "L = 3\n",
    "N_L = [7, 4, 3, 1]\n",
    "\n",
    "number_of_efficieny_less_than_80 = 0.\n",
    "\n",
    "for i in range(100):\n",
    "    omega, b = NN(L, N_L, n_iteration, learning_rate, X_train, Y_train)\n",
    "    output, efficieny = predict(L, omega, b, X_test, Y_test)\n",
    "    \n",
    "    if efficieny < 80.:\n",
    "        number_of_efficieny_less_than_80 += 1\n",
    "print(number_of_efficieny_less_than_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec0470",
   "metadata": {},
   "source": [
    "Now let's repeat the procidure with the help of `keras`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(3, input_shape=(7,), activation='tanh'))\n",
    "model.add(Dense(3, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, Y_train, epochs=150, batch_size=10)\n",
    "\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_test, Y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
