{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J5diGm3ihktC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef87f42-ba07-4b1b-f1fb-387247c66b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/My Drive/DSECOP/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "# Here are some necessary packages that we need to import to run this notebook\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/DSECOP/Colab Notebooks/\")\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Curve fitting data to understand underlying trends"
      ],
      "metadata": {
        "id": "BuUft5QGh1Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll learn how to *curve fit* a function to a histogram of a dataset. Finding a function that closely resembles a given histogram will provide information on the underlying model that governs the data."
      ],
      "metadata": {
        "id": "E28Hd0tWPWxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why is curve fitting a histogram useful?\n",
        "\n"
      ],
      "metadata": {
        "id": "nXpUSGEtiCrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By binning a dataset into a histogram, we effectively reduce the dimension of the dataset from $N$ (the number of observables taken) to the number of bins that we choose for the histogram $I = \\sum i$ (recall that in the last notebook, we introduced $i$ as an index over bins). Now, instead of trying to understand each observation made in isolation, we only have to understand $I$ more meaningful numbers that represent the probability distribution of the observations.\n",
        "\n",
        "However, maybe we want to reduce the dimension of the dataset even further. Perhaps we only care about the mean value of the observables, or their standard deviation. Or, perhaps we're deciding between two theories that predict different analytic models that describe the distribution of observables, and we want to determine which analytic model is a better fit to the data. \n",
        "\n",
        "To do this, we'll need to *curve fit* our data.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "l-xn80yIPcRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's continue to analyze the Uranium-241 sample you were given. \n",
        "\n",
        "You know that radioactive decay processes can be modeled by Poisson processes. \n",
        "\n",
        "$$p_{\\lambda t}(n) = \\frac{(\\lambda t)^n e^{-(\\lambda t)}}{n!} $$\n",
        "Here, $\\lambda$ represent the decay rate of the process, and has units of counts (or Geiger counter clickers) per minute. In our experiment, we're logging the number of clicks per minute, so we can just set $t$ = 1 minute."
         "\n",
      ],
      "metadata": {
        "id": "Djb22z_kjI3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Activity:** Write a function that plots a Poisson distribution as a function of the number of counts $n$, given a parameter $\\lambda$. (Remember that you can set $t$ = 1 minute.) Normalize the distribution to have an area of 1000. This means that if you record the number of clicks per minute 1000 times, each value  $p_{\\lambda t}(n)$ represents the number of times you would expect to observe $n$ counts."
      ],
      "metadata": {
        "id": "TtdqmyTMtA3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "JgFRMOJJhru3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curve fitting data determines model parameters\n",
        "\n"
      ],
      "metadata": {
        "id": "wVout96Vjruz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we fit a function to a distribution of data, we need to first choosen an analytic model that we think could represent the underlying data. The curve fitting procedure then determines the exact values for the parameters of the model that best align with the data. \n",
        "\n",
        "Choosing the right analytic model can be a problem in its own right. The choice should be motivated by our theoretical understanding of the system. In the example of Uranium-241, we know that the counts per minute represents a decay process and can therefore be described by a Poisson distribution with a decay rate $\\lambda$. So there is one free parameter $\\lambda$ that curve fitting will help determine. More generally, we might have to curve fit for a number of parameters $\\Theta$. (For example, a simple gaussian curve already has 2 parameters, mean $\\mu$ and standard deviation $\\sigma$.)\n",
        "\n",
        "In essense, these are the steps we want to execute:\n",
        "\n",
        "1. Create a histogram for given dataset with a bin choice $b_i$. The histogram produces a list of counts in each bin, $n_i$.\n",
        "\n",
        "2. Define a function $f(b_i, \\Theta)$ to fit to the histogram. This function will be calculated at the center of each bin to produce a list of the same length of $n_i$.\n",
        "\n",
        "3. Select a trial value for the fit parameter(s) $\\Theta_{\\textrm{trial}}$. Calculate the numbers $f(b_i, \\Theta_{\\textrm{trial}})$ at each bin center.\n",
        "\n",
        "4. Calculate the *loss* between the fit function and the data, i.e. between $f(b_i, \\Theta_{\\textrm{trial}})$ and $n_i$. In some settings, this is also known as the *error* of the fit function.\n",
        "\n",
        "4. Repeat steps 3 and 4 for a range of $\\Theta_{\\textrm{trial}}$ values. \n",
        "\n",
        "5. Select the $\\Theta_{\\textrm{trial}}$ that minimizes the loss. This will produce the best-fitting function to the data."
      ],
      "metadata": {
        "id": "lLEhUeL1Pi44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do we choose a loss function?\n",
        "\n"
      ],
      "metadata": {
        "id": "8L-adeoNlz3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We haven't been very specific as to what the actual loss function is. In fact, many loss functions exist! The only requirements on loss functions are that they are small when the fit function closely matches the distribution of observables, and large otherwise.\n",
        "\n",
        "Probably the most well-known loss function is the *mean-squared error*. \n",
        "\n",
        "$$\\mathcal{L}_{MSE}(\\Theta) = \\frac{\\sum_i (n_i - f(b_i, \\Theta))^2}{\\textrm{num. bins}}.$$\n",
        "\n",
        "Remember that $n_i$ is the list of counts in each histogram bin, so it represents the data. $f(b_i, \\Theta)$ is the model fit function at a given bin $b_i$. So if the fit function isn't a close match to the data, then we expect the error to be very large.\n",
        "\n",
        "Another relatively common loss function is the *mean-absolute error*\n",
        "\n",
        "$$\\mathcal{L}_{MAE}(\\Theta) = \\frac{\\sum_i |n_i - f(b_i, \\Theta)|}{\\textrm{num. bins}}.$$\n",
        "\n",
        "This function might be preferred over the mean-squared error when you want to deemphasize the importance of outliers in the dataset."
      ],
      "metadata": {
        "id": "0mSyNlVxQa5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activity**: Write a function to calculate the least square loss given a set of histogrammed counts ```obs_vals``` and a set of fit values ```fit_vals```."
      ],
      "metadata": {
        "id": "VlQCGMqvtzc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def least_sq_loss(obs_vals, fit_vals):\n",
        "\n",
        "  # Your code here\n",
        "  return "
      ],
      "metadata": {
        "id": "Tl7tiIKYjqO2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activity**: Read in the dataset ```geiger_counts_A.txt``` and histogram it. Calculate the loss between the histogram and a Poisson distribution for a range of trial $\\lambda$'s, and find the lambda that minimizes the loss. Also provide a plot of the loss as a function of the parameter $\\lambda$."
      ],
      "metadata": {
        "id": "ay7HeeVfmrKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_location = \"datasets/geiger_counts_A.txt\"\n",
        "\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "4R16Eld6jWyn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activity:** Now plot the best-fit Poisson function on top of a histogram of the data. Does the function provide a good fit to the data?"
      ],
      "metadata": {
        "id": "9IfkDv4injiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "EWoDpNpFmwqk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "id": "bGCt_1r1DjI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using built-in functions to curve fit\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3vgm0XCKntT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As is the case with most common tasks, there are a number of curve fitting packages that are more efficient than anything we could write. One such package is ```scipy.optimize.curve_fit()```.\n",
        "\n",
        "The function takes three main arguments: \n",
        "\n",
        "1. ```f```: the analytic function that you want to curve fit to. The function itself must be of the form ```f(data, params)``` (i.e. the first argument must be the observable)\n",
        "2. ```xdata```: a ```np.array``` (or ```list```) containing the $x$-axis values that you want to include in the fit. This should be equal to the bin centers of your histogrammed data.\n",
        "3. ```ydata```: a ```np.array``` (or ```list```) of the historam counts $n_i$\n",
        "\n",
        "You can read more about ```scipy.optimize.curve_fit()``` [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "9nmo9tFlQedZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activity:** Use ```scipy.optimize.curve_fit()``` to fit a Poisson distribution to the dataset. Then make a plot containing (1) a histogram of the dataset, (2) the best-fit Poisson function that you found from your manual scan over values of $\\lambda$, and (3) the best-fit Poisson function returned by ```scipy.optimize()```."
      ],
      "metadata": {
        "id": "ERx32FCmujUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import curve_fit\n",
        "\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "819lp_i3nl0B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stating fit model parameters with confidence"
      ],
      "metadata": {
        "id": "FrqKp0SA-ZML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have found a value of $\\lambda$ that minimizes the loss of our fit function, we are in a position to state our results. However, the results will be of limited use if we don't provide some sort of error on our estimate. \n",
        "\n",
        "Let's say that you ask a friend to independently measure the decay rate of your Uranium-241 sample so that you can compare estimates. The friend tells you that they've measured a decay rate of 12.6 counts / minute. This seems plausible, but it likely isn't exactly the decay rate that you measured. So there seems to be a conflict between the two measurements...\n",
        "\n",
        "But wait! Your friend then tells you that the uncertainty on their measurement was 0.4 counts / minute. Does this fix the discrepancy?\n",
        "\n"
      ],
      "metadata": {
        "id": "LnjqeQmq-agI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Where do uncertainties on a measurement come from?"
      ],
      "metadata": {
        "id": "XAweQUQJTJg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In any physical experiment, there are many reasons why your measurement of a given quantity might not match the true quantity. \n",
        "\n",
        "There will certainly be **statistical errors**, which come from random fluctuations in nature. Just because we know that the decays per minute of a Uranium-241 sample are modeled by a Poisson distribution doesn't mean that we expect our measurements of the decays per minute to be *perfectly* Poissonian, just roughly so. Another random source of statistical errors can be the experimental setup: maybe when you set up the Uranium-241 sample every day, you use a ruler to put the sample exactly 10 cm from the Geiger counter. But perhaps on Monday, you actually put the sample 10.01 cm away, and on Tuesday you put the sample 9.95 cm away, and so on.\n",
        "\n",
        "There might also be **systematic errors**, which come from the experimental setup. Maybe your Geiger counter is old and always beeps an extra two times a minute. Or perhaps your ruler isn't calibrated correctly, and what you read as 10 cm is in reality 10.1 cm.\n",
        "\n",
        "As a result, when we make a measurement $Y$ for a given experiment, the distribution of all measurements $Y$ you could make, when accounting for all of the statistical and systematic errors of an experiment, follows a normal distribution.\n",
        "\n",
        "<p><a href=\"https://sisu.ut.ee/measurement/31-normal-distribution\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1x5sxTgujQTaCPP4WlELtRJ8jiNxyGnNo\" alt=\"drawing\" width=\"400\"/>\n",
        "</a></p>\n",
        "\n",
        "*Note*: that this distribution is not necessarily centered on the true value of the measurement $Y^*$! Imagine a scenario where your systematic errors all cause you to underestimate the measurement of $Y$. Systematic errors affect the *standard deviation* of the distribution. In contrast, statistical errors affect the *mean* of the distribution*.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hpbCPcdXTLa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting measurement uncertainty to a stated measurement\n",
        "\n"
      ],
      "metadata": {
        "id": "k9MFhMqdZIIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The standard way to quote measurement uncertainty is to give the actual measured value $\\pm$ 1 standard deviation of the uncertainty from all sources. So if a measurement is cited as 12.6 $\\pm$ 0.4 counts / minute, this means that, given the observed evidence, there is a 68.3% chance that the true value of the decay rate lies between 12.2 and 13 counts per minute.\n",
        "\n",
        "---\n",
        "\n",
        "Where would these uncertainties come from? As a few examples: the uncertainty for experimental apparatuses would likely be given in the equiptment manuals. The uncertainty due to random fluctuations given bin in a histogram of counts is equal to the square root of the number of counts in that bin.\n",
        "\n",
        "If you wanted to quote the measurement uncertainty using a model fitting tool like ```scipy.optimize.curve_fit()```, there is an easy way to do so. The function returns both ```popt``` (the best fit parameter values) and ```pcov```, which is a matrix quantifying the measurement uncertainties on these values. The square root of the diagonal entry in row $i$ corresponds to the fit uncertainty on parameter $i$."
      ],
      "metadata": {
        "id": "I2wp6Rn5ZNzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activity**: State your recovered value of the decay rate $\\lambda$, giving 80% and 95% uncertainty limits. Is your value for $\\lambda$ consistent with your friend's?"
      ],
      "metadata": {
        "id": "5YsKG7tcBXnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KGxhWc4-a16",
        "outputId": "0e817a17-a624-4116-f2bc-98433e13650f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recovered value of lambda is 11.96689750165973 +/- 0.03540218199809753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "id": "Vw6QW10sFDUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Move on to notebook 03*"
      ],
      "metadata": {
        "id": "SUxlY-Q9uw5F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a5Dho7h2uyHN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
