{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/Symbolic_Regression/02_NeuralNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# Neural Network\n"],"metadata":{"id":"pp_7lPDuOsDo"}},{"cell_type":"markdown","source":["The network is a feed-forward, fully connected neural network with six hidden layers with soft plus activation functions, the first three having 128 neurons and the last three having 64 neurons. For each formula, we use 80% as the training set and the remainder as the validation set, training for 100 epochs with learning rate 0.005 and batch size 2048. We use the rms error loss function and the Adam optimizer with a weight decay of 10^−2. The learning rate and momentum schedules were implemented as described in ***L. N. Smith, N. Topin, Super-convergence: Very fast training of residual networks using large learning rates (2018); https://openreview.net/forum?id=H1A5ztj3b*** and ***L. N. Smith, A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay. arXiv:1803.09820 (2018).*** using the FastAI package ***J. Howard et al., Fastai, https://github.com/fastai/fastai (2018).***, with a ratio of 20 between the maximum and minimum learning rates, and using 10% of the iterations for the last part of the training cycle. For the momentum, the maximum β1 value was 0.95 and the minimum 0.85, while β2 = 0.99.\n"],"metadata":{"id":"rqKzT1D_SKxo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGh7m7XROpGI"},"outputs":[],"source":["from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils import data\n","import pickle\n","from matplotlib import pyplot as plt\n","import torch.utils.data as utils\n","import time\n","import os\n","\n","bs = 2048\n","wd = 1e-2\n","\n","is_cuda = torch.cuda.is_available()"]},{"cell_type":"code","source":["class MultDataset(data.Dataset):\n","    def __init__(self, factors, product):\n","        'Initialization'\n","        self.factors = factors\n","        self.product = product\n","\n","    def __len__(self):\n","        'Denotes the total number of samples'\n","        return len(self.product)\n","\n","    def __getitem__(self, index):\n","        # Load data and get label\n","        x = self.factors[index]\n","        y = self.product[index]\n","\n","        return x, y\n","\n","#This network uses Root Mean Square Error Loss\n","def rmse_loss(pred, targ):\n","    denom = targ**2\n","    denom = torch.sqrt(denom.sum()/len(denom))\n","    return torch.sqrt(F.mse_loss(pred, targ))/denom\n","\n","#In the notebook we train the Neural Network with default hyperparameters.\n","#Try changing the number of epochs, etc.\n","def NN_train(data, epochs=1000, lrs=1e-2, N_red_lr=4, pretrained_path=\"\"):\n","    try:\n","        os.mkdir(\"results/NN_trained_models/\")\n","    except:\n","        pass\n","\n","    try:\n","        n_variables = len(data[0])-1\n","        variables=data[:,:-1]\n","        f_dependent = data[:,-1]\n","        f_dependent = np.reshape(f_dependent,(len(f_dependent),1))\n","\n","        epochs = 200*n_variables\n","        if len(data)<5000:\n","            epochs = epochs*3\n","\n","        if n_variables==0 or n_variables==1:\n","            return 0\n","\n","\n","\n","\n","        factors = torch.from_numpy(variables)\n","        if is_cuda:\n","            factors = factors.cuda()\n","        else:\n","            factors = factors\n","        factors = factors.float()\n","\n","        product = torch.from_numpy(f_dependent)\n","        if is_cuda:\n","            product = product.cuda()\n","        else:\n","            product = product\n","        product = product.float()\n","\n","        class SimpleNet(nn.Module):\n","            def __init__(self, ni):\n","                super().__init__()\n","                self.linear1 = nn.Linear(ni, 128)\n","                self.linear2 = nn.Linear(128, 128)\n","                self.linear3 = nn.Linear(128, 64)\n","                self.linear4 = nn.Linear(64,64)\n","                self.linear5 = nn.Linear(64,1)\n","\n","            def forward(self, x):\n","                x = F.tanh(self.linear1(x))\n","                x = F.tanh(self.linear2(x))\n","                x = F.tanh(self.linear3(x))\n","                x = F.tanh(self.linear4(x))\n","                x = self.linear5(x)\n","                return x\n","\n","        my_dataset = utils.TensorDataset(factors,product) # create your datset\n","        my_dataloader = utils.DataLoader(my_dataset, batch_size=bs, shuffle=True) # create your dataloader\n","\n","        if is_cuda:\n","            model_feynman = SimpleNet(n_variables).cuda()\n","        else:\n","            model_feynman = SimpleNet(n_variables)\n","\n","        if pretrained_path!=\"\":\n","            model_feynman.load_state_dict(torch.load(pretrained_path))\n","\n","        check_es_loss = 10000\n","\n","        for i_i in range(N_red_lr):\n","            optimizer_feynman = optim.Adam(model_feynman.parameters(), lr = lrs)\n","            for epoch in range(epochs):\n","                model_feynman.train()\n","                for i, data in enumerate(my_dataloader):\n","                    optimizer_feynman.zero_grad()\n","\n","                    if is_cuda:\n","                        fct = data[0].float().cuda()\n","                        prd = data[1].float().cuda()\n","                    else:\n","                        fct = data[0].float()\n","                        prd = data[1].float()\n","\n","                    loss = rmse_loss(model_feynman(fct),prd)\n","                    loss.backward()\n","                    optimizer_feynman.step()\n","\n","                '''\n","                # Early stopping\n","                if epoch%20==0 and epoch>0:\n","                    if check_es_loss < loss:\n","                        break\n","                    else:\n","                        torch.save(model_feynman.state_dict(), \"results/NN_trained_models/models/\" + filename + \".h5\")\n","                        check_es_loss = loss\n","                if epoch==0:\n","                    if check_es_loss < loss:\n","                        torch.save(model_feynman.state_dict(), \"results/NN_trained_models/models/\" + filename + \".h5\")\n","                        check_es_loss = loss\n","                '''\n","                torch.save(model_feynman.state_dict(), \"weights.h5\")\n","            lrs = lrs/10\n","\n","        return model_feynman\n","\n","    except NameError:\n","        print(\"Error in file\")\n","        raise"],"metadata":{"id":"fgOGWFAEPKMp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_derivatives(pathdir,filename,model):\n","    try:\n","        data = np.loadtxt(pathdir+filename)[:,0:-1]\n","        pts = np.loadtxt(pathdir+filename)[:,0:-1]\n","        pts = torch.tensor(pts)\n","        pts = pts.clone().detach()\n","        is_cuda = torch.cuda.is_available()\n","        grad_weights = torch.ones(pts.shape[0], 1)\n","        if is_cuda:\n","            pts = pts.float().cuda()\n","            model = model.cuda()\n","            grad_weights = grad_weights.cuda()\n","\n","        pts.requires_grad_(True)\n","        outs = model(pts)\n","        grad = torch.autograd.grad(outs, pts, grad_outputs=grad_weights, create_graph=True)[0]\n","        save_grads = grad.detach().data.cpu().numpy()\n","        save_data = np.column_stack((data,save_grads))\n","        np.savetxt(\"results/gradients_comp_%s.txt\" %filename,save_data)\n","        return 1\n","    except:\n","        return 0"],"metadata":{"id":"ATsQFKMdOu1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultDataset(data.Dataset):\n","    def __init__(self, factors, product):\n","        'Initialization'\n","        self.factors = factors\n","        self.product = product\n","\n","    def __len__(self):\n","        'Denotes the total number of samples'\n","        return len(self.product)\n","\n","    def __getitem__(self, index):\n","        # Load data and get label\n","        x = self.factors[index]\n","        y = self.product[index]\n","\n","        return x, y\n","\n","def rmse_loss(pred, targ):\n","    denom = targ**2\n","    denom = torch.sqrt(denom.sum()/len(denom))\n","\n","    return torch.sqrt(F.mse_loss(pred, targ))/denom\n","\n","\n","def NN_eval(data):\n","    try:\n","        n_variables = len(data[0])-1\n","        variables=data[:,:-1]\n","        f_dependent = data[:,-1]\n","        f_dependent = np.reshape(f_dependent,(len(f_dependent),1))\n","\n","\n","        if n_variables==0:\n","            return 0\n","        elif n_variables==1:\n","            variables = np.reshape(variables,(len(variables),1))\n","        else:\n","          variables=variables\n","\n","\n","\n","        factors = torch.from_numpy(variables[0:int(5*len(variables)/6)])\n","        if is_cuda:\n","            factors = factors.cuda()\n","        else:\n","            factors = factors\n","        factors = factors.float()\n","        product = torch.from_numpy(f_dependent[0:int(5*len(f_dependent)/6)])\n","        if is_cuda:\n","            product = product.cuda()\n","        else:\n","            product = product\n","        product = product.float()\n","\n","        factors_val = torch.from_numpy(variables[int(5*len(variables)/6):int(len(variables))])\n","        if is_cuda:\n","            factors_val = factors_val.cuda()\n","        else:\n","            factors_val = factors_val\n","        factors_val = factors_val.float()\n","        product_val = torch.from_numpy(f_dependent[int(5*len(variables)/6):int(len(variables))])\n","        if is_cuda:\n","            product_val = product_val.cuda()\n","        else:\n","            product_val = product_val\n","        product_val = product_val.float()\n","\n","        class SimpleNet(nn.Module):\n","            def __init__(self, ni):\n","                super().__init__()\n","                self.linear1 = nn.Linear(ni, 128)\n","                self.linear2 = nn.Linear(128, 128)\n","                self.linear3 = nn.Linear(128, 64)\n","                self.linear4 = nn.Linear(64,64)\n","                self.linear5 = nn.Linear(64,1)\n","\n","            def forward(self, x):\n","                x = F.tanh(self.linear1(x))\n","                x = F.tanh(self.linear2(x))\n","                x = F.tanh(self.linear3(x))\n","                x = F.tanh(self.linear4(x))\n","                x = self.linear5(x)\n","                return x\n","\n","        if is_cuda:\n","            model = SimpleNet(n_variables).cuda()\n","        else:\n","            model = SimpleNet(n_variables)\n","\n","        model.load_state_dict(torch.load(\"weights.h5\"))\n","        model.eval()\n","        return(rmse_loss(model(factors_val),product_val),model)\n","\n","    except Exception as e:\n","        print(e)\n","        return (100,0)"],"metadata":{"id":"61GxnaBJO4yw"},"execution_count":null,"outputs":[]}]}