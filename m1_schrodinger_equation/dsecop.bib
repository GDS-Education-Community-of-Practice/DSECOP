
@article{cranmerDiscoveringSymbolicModels2020c,
  title = {Discovering {{Symbolic Models}} from {{Deep Learning}} with {{Inductive Biases}}},
  author = {Cranmer, Miles and {Sanchez-Gonzalez}, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
  year = {2020},
  month = nov,
  journal = {arXiv:2006.11287 [astro-ph, physics:physics, stat]},
  eprint = {2006.11287},
  eprinttype = {arxiv},
  primaryclass = {astro-ph, physics:physics, stat},
  abstract = {We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning,Physics - Computational Physics,Statistics - Machine Learning},
  file = {/Users/karan-casus/Zotero/storage/H6KJW36N/Cranmer et al. - 2020 - Discovering Symbolic Models from Deep Learning wit.pdf;/Users/karan-casus/Zotero/storage/MI2P2QB6/2006.html}
}

@article{cranmerLagrangianNeuralNetworks2020,
  title = {Lagrangian {{Neural Networks}}},
  author = {Cranmer, Miles and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter and Spergel, David and Ho, Shirley},
  year = {2020},
  month = jul,
  journal = {arXiv:2003.04630 [physics, stat]},
  eprint = {2003.04630},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  abstract = {Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Physics - Computational Physics,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning},
  file = {/Users/karan-casus/Zotero/storage/I9MTMXBS/Cranmer et al. - 2020 - Lagrangian Neural Networks.pdf;/Users/karan-casus/Zotero/storage/57D8SHLJ/2003.html}
}

@article{lemosRediscoveringOrbitalMechanics2022,
  title = {Rediscovering Orbital Mechanics with Machine Learning},
  author = {Lemos, Pablo and Jeffrey, Niall and Cranmer, Miles and Ho, Shirley and Battaglia, Peter},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.02306 [astro-ph]},
  eprint = {2202.02306},
  eprinttype = {arxiv},
  primaryclass = {astro-ph},
  abstract = {We present an approach for using machine learning to automatically discover the governing equations and hidden properties of real physical systems from observations. We train a "graph neural network" to simulate the dynamics of our solar system's Sun, planets, and large moons from 30 years of trajectory data. We then use symbolic regression to discover an analytical expression for the force law implicitly learned by the neural network, which our results showed is equivalent to Newton's law of gravitation. The key assumptions that were required were translational and rotational equivariance, and Newton's second and third laws of motion. Our approach correctly discovered the form of the symbolic force law. Furthermore, our approach did not require any assumptions about the masses of planets and moons or physical constants. They, too, were accurately inferred through our methods. Though, of course, the classical law of gravitation has been known since Isaac Newton, our result serves as a validation that our method can discover unknown laws and hidden properties from observed data. More broadly this work represents a key step toward realizing the potential of machine learning for accelerating scientific discovery.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Earth and Planetary Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning},
  file = {/Users/karan-casus/Zotero/storage/4EEXCZV4/Lemos et al. - 2022 - Rediscovering orbital mechanics with machine learn.pdf;/Users/karan-casus/Zotero/storage/TID2LLBP/2202.html}
}

@article{pathakModelFreePredictionLarge2018,
  title = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}: {{A Reservoir Computing Approach}}},
  shorttitle = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}},
  author = {Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
  year = {2018},
  month = jan,
  journal = {Physical Review Letters},
  volume = {120},
  number = {2},
  pages = {024102},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.120.024102},
  abstract = {We demonstrate the effectiveness of using machine learning for model-free prediction of spatiotemporally chaotic systems of arbitrarily large spatial extent and attractor dimension purely from observations of the system's past evolution. We present a parallel scheme with an example implementation based on the reservoir computing paradigm and demonstrate the scalability of our scheme using the Kuramoto-Sivashinsky equation as an example of a spatiotemporally chaotic system.},
  file = {/Users/karan-casus/Zotero/storage/SLQSKDFE/Pathak et al. - 2018 - Model-Free Prediction of Large Spatiotemporally Ch.pdf;/Users/karan-casus/Zotero/storage/XTXAYGXC/PhysRevLett.120.html}
}

@article{pathakUsingMachineLearning2017,
  title = {Using {{Machine Learning}} to {{Replicate Chaotic Attractors}} and {{Calculate Lyapunov Exponents}} from {{Data}}},
  author = {Pathak, Jaideep and Lu, Zhixin and Hunt, Brian R. and Girvan, Michelle and Ott, Edward},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.07313 [nlin]},
  eprint = {1710.07313},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  doi = {10.1063/1.5010300},
  abstract = {We use recent advances in the machine learning area known as 'reservoir computing' to formulate a method for model-free estimation from data of the Lyapunov exponents of a chaotic process. The technique uses a limited time series of measurements as input to a high-dimensional dynamical system called a 'reservoir'. After the reservoir's response to the data is recorded, linear regression is used to learn a large set of parameters, called the 'output weights'. The learned output weights are then used to form a modified autonomous reservoir designed to be capable of producing arbitrarily long time series whose ergodic properties approximate those of the input signal. When successful, we say that the autonomous reservoir reproduces the attractor's 'climate'. Since the reservoir equations and output weights are known, we can compute derivatives needed to determine the Lyapunov exponents of the autonomous reservoir, which we then use as estimates of the Lyapunov exponents for the original input generating system. We illustrate the effectiveness of our technique with two examples, the Lorenz system, and the Kuramoto-Sivashinsky (KS) equation. In particular, we use the Lorenz system to show that achieving climate reproduction may require tuning of the reservoir parameters. For the case of the KS equation, we note that as the system's spatial size is increased, the number of Lyapunov exponents increases, thus yielding a challenging test of our method, which we find the method successfully passes.},
  archiveprefix = {arXiv},
  keywords = {Nonlinear Sciences - Chaotic Dynamics},
  file = {/Users/karan-casus/Zotero/storage/TUYAHC2L/Pathak et al. - 2017 - Using Machine Learning to Replicate Chaotic Attrac.pdf;/Users/karan-casus/Zotero/storage/NFV5UZR7/1710.html}
}



@article{karniadakisPhysicsinformedMachineLearning2021a,
  title = {Physics-Informed Machine Learning},
  author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  year = {2021},
  month = jun,
  journal = {Nature Reviews Physics},
  volume = {3},
  number = {6},
  pages = {422--440},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5820},
  doi = {10.1038/s42254-021-00314-5},
  abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Applied mathematics,Computational science},
  file = {/Users/karan-casus/Zotero/storage/7QWJGV3K/Karniadakis et al. - 2021 - Physics-informed machine learning.pdf;/Users/karan-casus/Zotero/storage/Q5HZ9TCW/s42254-021-00314-5.html}
}

@article{raissiPhysicsinformedNeuralNetworks2019c,
  title = {Physics-Informed Neural Networks: {{A}} Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations},
  shorttitle = {Physics-Informed Neural Networks},
  author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
  year = {2019},
  month = feb,
  journal = {Journal of Computational Physics},
  volume = {378},
  pages = {686--707},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2018.10.045},
  abstract = {We introduce physics-informed neural networks \textendash{} neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\textendash Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\textendash diffusion systems, and the propagation of nonlinear shallow-water waves.},
  langid = {english},
  keywords = {Data-driven scientific computing,Machine learning,Nonlinear dynamics,Predictive modeling,Runge–Kutta methods},
  file = {/Users/karan-casus/Zotero/storage/GHHG8FPY/S0021999118307125.html}
}


