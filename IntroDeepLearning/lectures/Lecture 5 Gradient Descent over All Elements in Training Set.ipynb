{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530b3c11",
   "metadata": {},
   "source": [
    "# Lecture V: Gradient Descent over All Elements in Training Set\n",
    "\n",
    "So far we have just considered one element in the training set, $(X^i, y^i)$, where $X^i = (x^{i}_{1}, x^{i}_{2})$. To go over the all $m$ elements in the training set, instead of loss function, we use cost function. For example, to modify the parameter $\\omega_1$, we take a derivative of the loss function respect to $\\omega_1^i$ and take a average of all its value over the tarning set, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\omega_1} J(\\omega, b) = \\frac{1}{m} \\Sigma_{i = 1} ^{m} \\frac{\\partial}{\\partial \\omega^i_1} L(a^i, y^i) \\tag{1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f941c4e",
   "metadata": {},
   "source": [
    "# miniProject\n",
    "\n",
    "One of the cricial rules in programing for Nueral networks is to avoid **for-loops**. Because of obvios reason that makes your running time much longer compared to use of **vectorization**. In this probelm, a block of code correpsonded to calculating the gradient desent over the training set is preseneted. Try to rewrite this with the least number of for-loops. If you want to see the changes in the modele paramerters, run your code using this [file](Lecture5-HW-vectorization.txt)  as the inputs and define the number of iteration and learning rate as $n_{iteration} = 10$ and $\\alpha = 0.05$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0414f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "  \n",
    "    return(1 / (1 + math.exp(-x))) \n",
    "\n",
    "\n",
    "def gradient_decent(X_t, Y, n_iteration, learnig_rate):\n",
    "    \n",
    "    alpha = learning_rate                  # learning scale\n",
    "    \n",
    "    for k in range(n_iteration):\n",
    "        \n",
    "        z = 0; d_omega_1 = 0; d_omega_2 = 0; db = 0 \n",
    "        d_omega = np.zeros((n),)
    "        \n",
    "                                         \n",
    "        n = X_t.shape[0]   # number of dimension of the input X_t^(1) = [x_1^(1), x_2^(1)], {X_t.shape = = (n, m)}                        #X = [x_1, x_2]\n",
    "        \n",
    "        omega = np.random.rand((n,1)) * np.sqrt(1/n)        # initial values for [omega_1, omega_2]\n",
    "        \n",
    "\n",
    "        m = len(Y)                           # number of data in the tarining set, or m = X_t.shape[1] {length of [X_t^(1), ..., X_t^(m)]} or [y_1, ..., y_m]\n",
    "\n",
    "\n",
    "        for i in range(1, m, +1):\n",
    "            z = np.transpose(omega) * X_t[i] + b                 # z = \\omega^T X + b\n",
    "            a = sigma(z)\n",
    "\n",
    "            J += - Y[i] * np.log(a) - (1 - Y[i]) * np.log(1 - a)\n",
    "            dz = a - Y[i]\n",
    "\n",
    "            for j in range(n):\n",
    "                d_omega[j] += X_t[i][j] * dz\n",
    "\n",
    "            db += dz \n",
    "\n",
    "        J = J / m\n",
    "        for j in range(n):\n",
    "            d_omega[j] = d_omega_[j] / m\n",
    "            omega[j] = omega[j] - alpha * d_omega[j]\n",
    "\n",
    "        db =  db / m\n",
    "        b = b - alpha * db\n",
    "\n",
    "    return(omega, b)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a0d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
