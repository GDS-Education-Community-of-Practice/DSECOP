{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4754a4",
   "metadata": {},
   "source": [
    "# Lecture X: Using Deep Learning to Find Hot-Jupiters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8872e52",
   "metadata": {},
   "source": [
    "Now, we have already learned the basic concepts of deep learning. So, it's time to use our knowledge in a real problem. Let's think about the question in the second lecture when **we want to know which detected exoplanet is hot-Jupiter**. \n",
    "\n",
    "**Step I**: The first step to do is to find a *training set*, i.e. a set of data that we know if they are hot-Jupiter or not. I make that training set for you, you can download the data of more than 5,000 exoplanets from here (??). This data collected from [here](https://exoplanetarchive.ipac.caltech.edu/docs/data.html). We also need a test set to examine our neural network and find its efficiency. A rule of thumb here is to divide your data as $98%$ for the training set and $2%$ for the *dev/test* set if you have a huge number of data. Here, the first step of your project is to divide the data into two sets with the ratio of $90/10$ (training/test) in 100.\n",
    "\n",
    "**Step II**: Now, let's think about how many parameters we should consider in our neural network. In the data given to you, some parameters may not be useful to identify the type of exoplanets. The best thing is to consider all the parameters and let the neural network decide which ones are more important. But just for simplicity here we use a little bit of our prior knowledge to roll out some of the parameters. Thus, the second step in your project is to read the data, and collect these parameters:\n",
    "1. Orbital Period, \n",
    "2. Transit Duration, \n",
    "3. Planetary Radius, \n",
    "4. Eqillibruim Temperature, \n",
    "5. Stellar Effective Temperature, \n",
    "6. Stellar Surface Gravity, \n",
    "7. Stellar Radius.\n",
    "\n",
    "You also need to collect the data from\n",
    "\n",
    "8. Type of Exoplanet, (1 for hot-Jupiter, 0 for the other types).\n",
    "\n",
    "**Step III**: We want to use the logistic regression model in our neural network. Then we are going to use the method of Gradient descent to let the neural network learn the weight ($\\omega$ and $b$) of each input. To do so, we have to set the hyperparameters such as *learning rate*, *activation function*, and *number of hidden layers*. Therefore, set these hyperparameters as:\n",
    "1. rating rate: $\\alpha = 0.03$, \n",
    "2. activation function: $g(z) = tanh(z)$ and for the last layer: $g(z) = \\sigma(z)$, \n",
    "3. number of hidden layers: 2, \n",
    "4. the number of nodes in each hidden layer 3.\n",
    "\n",
    "Plot the diagram of your neural network.\n",
    "\n",
    "**Step IV**: Define the loss and cost function by using [eq.1]( Lecture 3 Loss and Cost Functions.ipynb ) and [eq.5](Lecture 3 Loss and Cost Functions.ipynb) . Grab a cup of coffee and generalize your code for miniProject in Lecture V, to implement the gradient descent method for this neural network. (This is a difficult one, so take your time and write your code in a very clean and neat way.)\n",
    "\n",
    "\n",
    "**Step V**:  Run your code over all nodes and layers. Then, the result of your code should be the best $\\omega$ and $b$ for each node.\n",
    "\n",
    "**Step VI**: Knowing the best $\\omega$ and $b$ for each node, you can use the *learned model parameters* to test your neural network on your *test set*.\n",
    "\n",
    "**Step VII**: For what percentage of the data in your test set, your neural network predicts the type of exoplanet correctly?\n",
    "\n",
    "**Step VIII**: If the efficiency is higher than $90\\%$ congratulation! You did a very good job! Celebrate it by pouring yourself *a mega pint of wine (or coffee)!*\n",
    "\n",
    "**Step XI**: If the efficiency is less than $70\\%$, the neural network does not predict well. Thus, we should modify it. You can modify it by adding more layers/nodes. Or changing the other hyperparameters such as learning rate.\n",
    "\n",
    "**Step X**: At this step, you already know how deep learning works and how to write code for building a neural network. However, there are a lot of deep learning packages in python. You can summarize your code by using them. So, this time try to use any of the packages that you may know and rewrite your code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb801f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c27ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
